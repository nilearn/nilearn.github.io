<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/copybutton.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/clipboard.min.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.2.24. nilearn.datasets.fetch_localizer_calculation_task"href=nilearn.datasets.fetch_localizer_calculation_task.html rel=next><link title="8.2.22. nilearn.datasets.fetch_localizer_button_task"href=nilearn.datasets.fetch_localizer_button_task.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.2.24. nilearn.datasets.fetch_localizer_calculation_task"accesskey=N href=nilearn.datasets.fetch_localizer_calculation_task.html>next</a> |</li><li class=right><a title="8.2.22. nilearn.datasets.fetch_localizer_button_task"accesskey=P href=nilearn.datasets.fetch_localizer_button_task.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li><a href=../../glossary.html>Glossary</a>| </li><li><a href=../../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-datasets-fetch-localizer-contrasts><h1><span class=section-number>8.2.23. </span>nilearn.datasets.fetch_localizer_contrasts<a title="Permalink to this headline"class=headerlink href=#nilearn-datasets-fetch-localizer-contrasts>¶</a></h1><dl class="py function"><dt class="sig sig-object py"id=nilearn.datasets.fetch_localizer_contrasts><span class="sig-prename descclassname"><span class=pre>nilearn.datasets.</span></span><span class="sig-name descname"><span class=pre>fetch_localizer_contrasts</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>contrasts</span></span></em>, <em class=sig-param><span class=n><span class=pre>n_subjects</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>get_tmaps</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>get_masks</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>get_anats</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>data_dir</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>url</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>resume</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>verbose</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>1</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/2fd66656/nilearn/datasets/func.py#L409><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.datasets.fetch_localizer_contrasts>¶</a></dt><dd><p>Download and load Brainomics/Localizer dataset (94 subjects).</p> <p>“The Functional Localizer is a simple and fast acquisition procedure based on a 5-minute functional magnetic resonance imaging (fMRI) sequence that can be run as easily and as systematically as an anatomical scan. This protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level. Individual functional maps are reliable and quite precise. The procedure is described in more detail on the Functional Localizer page.” (see <a class="reference external"href=https://osf.io/vhtf6/>https://osf.io/vhtf6/</a>)</p> <p>You may cite <a class="footnote-reference brackets"href=#papadopoulosorfanos2017309 id=id1>1</a> when using this dataset.</p> <p>Scientific results obtained using this dataset are described in <a class="footnote-reference brackets"href=#pinel2007fast id=id2>2</a>.</p> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>contrasts</strong><span class=classifier>list of str</span></dt><dd><p>The contrasts to be fetched (for all 94 subjects available). Allowed values are:</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=o>-</span> <span class=s2>"checkerboard"</span>
<span class=o>-</span> <span class=s2>"horizontal checkerboard"</span>
<span class=o>-</span> <span class=s2>"vertical checkerboard"</span>
<span class=o>-</span> <span class=s2>"horizontal vs vertical checkerboard"</span>
<span class=o>-</span> <span class=s2>"vertical vs horizontal checkerboard"</span>
<span class=o>-</span> <span class=s2>"sentence listening"</span>
<span class=o>-</span> <span class=s2>"sentence reading"</span>
<span class=o>-</span> <span class=s2>"sentence listening and reading"</span>
<span class=o>-</span> <span class=s2>"sentence reading vs checkerboard"</span>
<span class=o>-</span> <span class=s2>"calculation (auditory cue)"</span>
<span class=o>-</span> <span class=s2>"calculation (visual cue)"</span>
<span class=o>-</span> <span class=s2>"calculation (auditory and visual cue)"</span>
<span class=o>-</span> <span class=s2>"calculation (auditory cue) vs sentence listening"</span>
<span class=o>-</span> <span class=s2>"calculation (visual cue) vs sentence reading"</span>
<span class=o>-</span> <span class=s2>"calculation vs sentences"</span>
<span class=o>-</span> <span class=s2>"calculation (auditory cue) and sentence listening"</span>
<span class=o>-</span> <span class=s2>"calculation (visual cue) and sentence reading"</span>
<span class=o>-</span> <span class=s2>"calculation and sentence listening/reading"</span>
<span class=o>-</span> <span class=s2>"calculation (auditory cue) and sentence listening vs "</span>
<span class=o>-</span> <span class=s2>"calculation (visual cue) and sentence reading"</span>
<span class=o>-</span> <span class=s2>"calculation (visual cue) and sentence reading vs checkerboard"</span>
<span class=o>-</span> <span class=s2>"calculation and sentence listening/reading vs button press"</span>
<span class=o>-</span> <span class=s2>"left button press (auditory cue)"</span>
<span class=o>-</span> <span class=s2>"left button press (visual cue)"</span>
<span class=o>-</span> <span class=s2>"left button press"</span>
<span class=o>-</span> <span class=s2>"left vs right button press"</span>
<span class=o>-</span> <span class=s2>"right button press (auditory cue)"</span>
<span class=o>-</span> <span class=s2>"right button press (visual cue)"</span>
<span class=o>-</span> <span class=s2>"right button press"</span>
<span class=o>-</span> <span class=s2>"right vs left button press"</span>
<span class=o>-</span> <span class=s2>"button press (auditory cue) vs sentence listening"</span>
<span class=o>-</span> <span class=s2>"button press (visual cue) vs sentence reading"</span>
<span class=o>-</span> <span class=s2>"button press vs calculation and sentence listening/reading"</span>
</pre></div></div> <p>or equivalently on can use the original names:</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=o>-</span> <span class=s2>"checkerboard"</span>
<span class=o>-</span> <span class=s2>"horizontal checkerboard"</span>
<span class=o>-</span> <span class=s2>"vertical checkerboard"</span>
<span class=o>-</span> <span class=s2>"horizontal vs vertical checkerboard"</span>
<span class=o>-</span> <span class=s2>"vertical vs horizontal checkerboard"</span>
<span class=o>-</span> <span class=s2>"auditory sentences"</span>
<span class=o>-</span> <span class=s2>"visual sentences"</span>
<span class=o>-</span> <span class=s2>"auditory&visual sentences"</span>
<span class=o>-</span> <span class=s2>"visual sentences vs checkerboard"</span>
<span class=o>-</span> <span class=s2>"auditory calculation"</span>
<span class=o>-</span> <span class=s2>"visual calculation"</span>
<span class=o>-</span> <span class=s2>"auditory&visual calculation"</span>
<span class=o>-</span> <span class=s2>"auditory calculation vs auditory sentences"</span>
<span class=o>-</span> <span class=s2>"visual calculation vs sentences"</span>
<span class=o>-</span> <span class=s2>"auditory&visual calculation vs sentences"</span>
<span class=o>-</span> <span class=s2>"auditory processing"</span>
<span class=o>-</span> <span class=s2>"visual processing"</span>
<span class=o>-</span> <span class=s2>"visual processing vs auditory processing"</span>
<span class=o>-</span> <span class=s2>"auditory processing vs visual processing"</span>
<span class=o>-</span> <span class=s2>"visual processing vs checkerboard"</span>
<span class=o>-</span> <span class=s2>"cognitive processing vs motor"</span>
<span class=o>-</span> <span class=s2>"left auditory click"</span>
<span class=o>-</span> <span class=s2>"left visual click"</span>
<span class=o>-</span> <span class=s2>"left auditory&visual click"</span>
<span class=o>-</span> <span class=s2>"left auditory & visual click vs right auditory&visual click"</span>
<span class=o>-</span> <span class=s2>"right auditory click"</span>
<span class=o>-</span> <span class=s2>"right visual click"</span>
<span class=o>-</span> <span class=s2>"right auditory&visual click"</span>
<span class=o>-</span> <span class=s2>"right auditory & visual click vs left auditory&visual click"</span>
<span class=o>-</span> <span class=s2>"auditory click vs auditory sentences"</span>
<span class=o>-</span> <span class=s2>"visual click vs visual sentences"</span>
<span class=o>-</span> <span class=s2>"auditory&visual motor vs cognitive processing"</span>
</pre></div></div></dd><dt><strong>n_subjects</strong><span class=classifier>int or list, optional</span></dt><dd><p>The number or list of subjects to load. If None is given, all 94 subjects are used.</p></dd><dt><strong>get_tmaps</strong><span class=classifier>boolean, optional</span></dt><dd><p>Whether t maps should be fetched or not. Default=False.</p></dd><dt><strong>get_masks</strong><span class=classifier>boolean, optional</span></dt><dd><p>Whether individual masks should be fetched or not. Default=False.</p></dd><dt><strong>get_anats</strong><span class=classifier>boolean, optional</span></dt><dd><p>Whether individual structural images should be fetched or not. Default=False.</p></dd><dt><strong>data_dir</strong><span class=classifier><a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/pathlib.html#pathlib.Path><code class="xref py py-obj docutils literal notranslate"><span class=pre>pathlib.Path</span></code></a> or <a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/stdtypes.html#str><code class="xref py py-obj docutils literal notranslate"><span class=pre>str</span></code></a>, optional</span></dt><dd><p>Path where data should be downloaded. By default, files are downloaded in home directory.</p></dd><dt><strong>url</strong><span class=classifier><a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/stdtypes.html#str><code class="xref py py-obj docutils literal notranslate"><span class=pre>str</span></code></a>, optional</span></dt><dd><p>URL of file to download. Override download URL. Used for test only (or if you setup a mirror of the data). Default=None.</p></dd><dt><strong>resume</strong><span class=classifier><a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, optional</span></dt><dd><p>Whether to resume download of a partly-downloaded file. Default=True.</p></dd><dt><strong>verbose</strong><span class=classifier><a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>, optional</span></dt><dd><p>Verbosity level (0 means no message). Default=1.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>data</strong><span class=classifier>Bunch</span></dt><dd><p>Dictionary-like object, the interest attributes are :</p> <ul class=simple><li><dl class=simple><dt>‘cmaps’: string list</dt><dd><p>Paths to nifti contrast maps</p></dd></dl></li><li><dl class=simple><dt>‘tmaps’ string list (if ‘get_tmaps’ set to True)</dt><dd><p>Paths to nifti t maps</p></dd></dl></li><li><dl class=simple><dt>‘masks’: string list</dt><dd><p>Paths to nifti files corresponding to the subjects individual masks</p></dd></dl></li><li><dl class=simple><dt>‘anats’: string</dt><dd><p>Path to nifti files corresponding to the subjects structural images</p></dd></dl></li></ul></dd></dl></dd></dl> <div class="admonition seealso"><p class=admonition-title>See also</p><dl class=simple><dt><a class="reference internal"href=nilearn.datasets.fetch_localizer_calculation_task.html#nilearn.datasets.fetch_localizer_calculation_task title=nilearn.datasets.fetch_localizer_calculation_task><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_calculation_task</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.datasets.fetch_localizer_button_task.html#nilearn.datasets.fetch_localizer_button_task title=nilearn.datasets.fetch_localizer_button_task><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_button_task</span></code></a></dt><dd></dd></dl></div> <p class=rubric>References</p> <p><dl class="footnote brackets"><dt class=label id=papadopoulosorfanos2017309><span class=brackets><a class=fn-backref href=#id1>1</a></span></dt><dd><p>Dimitri Papadopoulos Orfanos, Vincent Michel, Yannick Schwartz, Philippe Pinel, Antonio Moreno, Denis Le Bihan, and Vincent Frouin. The brainomics/localizer database. <em>NeuroImage</em>, 144:309–314, 2017. Data Sharing Part II. URL: <a class="reference external"href=https://www.sciencedirect.com/science/article/pii/S1053811915008745>https://www.sciencedirect.com/science/article/pii/S1053811915008745</a>, <a class="reference external"href=https://doi.org/https://doi.org/10.1016/j.neuroimage.2015.09.052>doi:https://doi.org/10.1016/j.neuroimage.2015.09.052</a>.</p></dd><dt class=label id=pinel2007fast><span class=brackets><a class=fn-backref href=#id2>2</a></span></dt><dd><p>Philippe Pinel, Bertrand Thirion, Sébastien Meriaux, Antoinette Jobert, Julien Serres, Denis Le Bihan, Jean-Baptiste Poline, and Stanislas Dehaene. Fast reproducible identification and large-scale databasing of individual functional cognitive networks. <em>BMC Neuroscience</em>, 2007.</p></dd></dl></dd></dl><div class=section id=examples-using-nilearn-datasets-fetch-localizer-contrasts><h2><span class=section-number>8.2.23.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_contrasts</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-datasets-fetch-localizer-contrasts>¶</a></h2><div tooltip='This script showcases the so-called "All resolution inference" procedure, in which the proporti...'class=sphx-glr-thumbcontainer><div class="figure align-default"id=id3><img alt="Second-level fMRI model: true positive proportion in clusters"src=../../_images/sphx_glr_plot_proportion_activated_voxels_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_proportion_activated_voxels.html#sphx-glr-auto-examples-05-glm-second-level-plot-proportion-activated-voxels-py><span class="std std-ref">Second-level fMRI model: true positive proportion in clusters</span></a></span><a title="Permalink to this image"class=headerlink href=#id3>¶</a></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a second level analysis in experimental d..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id4><img alt="Second-level fMRI model: two-sample test, unpaired and paired"src=../../_images/sphx_glr_plot_second_level_two_sample_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_two_sample_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-two-sample-test-py><span class="std std-ref">Second-level fMRI model: two-sample test, unpaired and paired</span></a></span><a title="Permalink to this image"class=headerlink href=#id4>¶</a></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a second-level analysis (one-sample test)..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id5><img alt="Second-level fMRI model: one sample test"src=../../_images/sphx_glr_plot_second_level_one_sample_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_one_sample_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-one-sample-test-py><span class="std std-ref">Second-level fMRI model: one sample test</span></a></span><a title="Permalink to this image"class=headerlink href=#id5>¶</a></p></div></div><div tooltip="This example shows the results obtained in a group analysis using a more complex contrast than ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id6><img alt="Example of generic design in second-level models"src=../../_images/sphx_glr_plot_second_level_association_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_association_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-association-test-py><span class="std std-ref">Example of generic design in second-level models</span></a></span><a title="Permalink to this image"class=headerlink href=#id6>¶</a></p></div></div><div tooltip="This example shows the results obtained in a massively univariate analysis performed at the int..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id7><img alt="Massively univariate analysis of a motor task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_mass_univariate_methods_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html#sphx-glr-auto-examples-07-advanced-plot-localizer-mass-univariate-methods-py><span class="std std-ref">Massively univariate analysis of a motor task from the Localizer dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id7>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.2.23. nilearn.datasets.fetch_localizer_contrasts</a><ul><li><a class="reference internal"href=#examples-using-nilearn-datasets-fetch-localizer-contrasts>8.2.23.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_contrasts</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.datasets.fetch_localizer_button_task.html><span class=section-number>8.2.22. </span>nilearn.datasets.fetch_localizer_button_task</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.datasets.fetch_localizer_calculation_task.html><span class=section-number>8.2.24. </span>nilearn.datasets.fetch_localizer_calculation_task</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2021. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.datasets.fetch_localizer_contrasts.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>