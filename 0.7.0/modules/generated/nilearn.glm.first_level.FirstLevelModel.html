<!doctypehtml><html lang=en xmlns=http://www.w3.org/1999/xhtml><meta content=IE=Edge http-equiv=X-UA-Compatible><meta content="text/html; charset=utf-8"http-equiv=Content-Type><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/language_data.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.12.15.2. nilearn.glm.first_level.check_design_matrix"href=nilearn.glm.first_level.check_design_matrix.html rel=next><link title="8.12.14. nilearn.glm.threshold_stats_img"href=nilearn.glm.threshold_stats_img.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script></head><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.12.15.2. nilearn.glm.first_level.check_design_matrix"accesskey=N href=nilearn.glm.first_level.check_design_matrix.html>next</a> |</li><li class=right><a title="8.12.14. nilearn.glm.threshold_stats_img"accesskey=P href=nilearn.glm.threshold_stats_img.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html>8. Reference documentation: all nilearn functions</a> »</li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class="first admonition-title">Note</p><p class=last>This page is a reference documentation. It only explains the class signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-glm-first-level-firstlevelmodel><h1>8.12.15.1. nilearn.glm.first_level.FirstLevelModel<a title="Permalink to this headline"class=headerlink href=#nilearn-glm-first-level-firstlevelmodel>¶</a></h1><dl class=class><dt id=nilearn.glm.first_level.FirstLevelModel><em class=property>class </em><code class=descclassname>nilearn.glm.first_level.</code><code class=descname>FirstLevelModel</code><span class=sig-paren>(</span><em>t_r=None, slice_time_ref=0.0, hrf_model='glover', drift_model='cosine', high_pass=0.01, drift_order=1, fir_delays=[0], min_onset=-24, mask_img=None, target_affine=None, target_shape=None, smoothing_fwhm=None, memory=Memory(location=None), memory_level=1, standardize=False, signal_scaling=0, noise_model='ar1', verbose=0, n_jobs=1, minimize_memory=True, subject_label=None</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel>¶</a></dt><dd><p>Implementation of the General Linear Model for single session fMRI data.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>t_r</strong> : float</p> <blockquote><div><p>This parameter indicates repetition times of the experimental runs. In seconds. It is necessary to correctly consider times in the design matrix. This parameter is also passed to nilearn.signal.clean. Please see the related documentation for details.</p></div></blockquote> <p><strong>slice_time_ref</strong> : float, optional (default 0.)</p> <blockquote><div><p>This parameter indicates the time of the reference slice used in the slice timing preprocessing step of the experimental runs. It is expressed as a percentage of the t_r (time repetition), so it can have values between 0. and 1.</p></div></blockquote> <p><strong>hrf_model</strong> : {‘spm’, ‘spm + derivative’, ‘spm + derivative + dispersion’,</p> <blockquote><div><p>‘glover’, ‘glover + derivative’, ‘glover + derivative + dispersion’, ‘fir’, None} String that specifies the hemodynamic response function. Defaults to ‘glover’.</p></div></blockquote> <p><strong>drift_model</strong> : string, optional</p> <blockquote><div><p>This parameter specifies the desired drift model for the design matrices. It can be ‘polynomial’, ‘cosine’ or None.</p></div></blockquote> <p><strong>high_pass</strong> : float, optional</p> <blockquote><div><p>This parameter specifies the cut frequency of the high-pass filter in Hz for the design matrices. Used only if drift_model is ‘cosine’.</p></div></blockquote> <p><strong>drift_order</strong> : int, optional</p> <blockquote><div><p>This parameter specifices the order of the drift model (in case it is polynomial) for the design matrices.</p></div></blockquote> <p><strong>fir_delays</strong> : array of shape(n_onsets) or list, optional</p> <blockquote><div><p>In case of FIR design, yields the array of delays used in the FIR model, in scans.</p></div></blockquote> <p><strong>min_onset</strong> : float, optional</p> <blockquote><div><p>This parameter specifies the minimal onset relative to the design (in seconds). Events that start before (slice_time_ref * t_r + min_onset) are not considered.</p></div></blockquote> <p><strong>mask_img</strong> : Niimg-like, NiftiMasker object or False, optional</p> <blockquote><div><p>Mask to be used on data. If an instance of masker is passed, then its mask will be used. If no mask is given, it will be computed automatically by a NiftiMasker with default parameters. If False is given then the data will not be masked.</p></div></blockquote> <p><strong>target_affine</strong> : 3x3 or 4x4 matrix, optional</p> <blockquote><div><p>This parameter is passed to nilearn.image.resample_img. Please see the related documentation for details.</p></div></blockquote> <p><strong>target_shape</strong> : 3-tuple of integers, optional</p> <blockquote><div><p>This parameter is passed to nilearn.image.resample_img. Please see the related documentation for details.</p></div></blockquote> <p><strong>smoothing_fwhm</strong> : float, optional</p> <blockquote><div><p>If smoothing_fwhm is not None, it gives the size in millimeters of the spatial smoothing to apply to the signal.</p></div></blockquote> <p><strong>memory</strong> : string, optional</p> <blockquote><div><p>Path to the directory used to cache the masking process and the glm fit. By default, no caching is done. Creates instance of joblib.Memory.</p></div></blockquote> <p><strong>memory_level</strong> : integer, optional</p> <blockquote><div><p>Rough estimator of the amount of memory used by caching. Higher value means more memory for caching.</p></div></blockquote> <p><strong>standardize</strong> : boolean, optional</p> <blockquote><div><p>If standardize is True, the time-series are centered and normed: their variance is put to 1 in the time dimension.</p></div></blockquote> <p><strong>signal_scaling</strong> : False, int or (int, int), optional,</p> <blockquote><div><p>If not False, fMRI signals are scaled to the mean value of scaling_axis given, which can be 0, 1 or (0, 1). 0 refers to mean scaling each voxel with respect to time, 1 refers to mean scaling each time point with respect to all voxels & (0, 1) refers to scaling with respect to voxels and time, which is known as grand mean scaling. Incompatible with standardize (standardize=False is enforced when signal_scaling is not False).</p></div></blockquote> <p><strong>noise_model</strong> : {‘ar1’, ‘ols’}, optional</p> <blockquote><div><p>The temporal variance model. Defaults to ‘ar1’</p></div></blockquote> <p><strong>verbose</strong> : integer, optional</p> <blockquote><div><p>Indicate the level of verbosity. By default, nothing is printed. If 0 prints nothing. If 1 prints progress by computation of each run. If 2 prints timing details of masker and GLM. If 3 prints masker computation details.</p></div></blockquote> <p><strong>n_jobs</strong> : integer, optional</p> <blockquote><div><p>The number of CPUs to use to do the computation. -1 means ‘all CPUs’, -2 ‘all CPUs but one’, and so on.</p></div></blockquote> <p><strong>minimize_memory</strong> : boolean, optional</p> <blockquote><div><p>Gets rid of some variables on the model fit results that are not necessary for contrast computation and would only be useful for further inspection of model details. This has an important impact on memory consumption. True by default.</p></div></blockquote> <p><strong>subject_label</strong> : string, optional</p> <blockquote class=last><div><p>This id will be used to identify a <cite>FirstLevelModel</cite> when passed to a <cite>SecondLevelModel</cite> object.</p></div></blockquote></td></tr></tbody></table> <p class=rubric>Attributes</p> <table border=1 class=docutils><colgroup><col width=6%><col width=94%></colgroup><tbody valign=top><tr class=row-odd><td><strong>labels_</strong></td><td>(array of shape (n_voxels,),) a map of values on voxels used to identify the corresponding model</td></tr><tr class=row-even><td><strong>results_</strong></td><td>(dict,) with keys corresponding to the different labels values. Values are SimpleRegressionResults corresponding to the voxels, if minimize_memory is True, RegressionResults if minimize_memory is False</td></tr></tbody></table> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.__init__><code class=descname>__init__</code><span class=sig-paren>(</span><em>t_r=None, slice_time_ref=0.0, hrf_model='glover', drift_model='cosine', high_pass=0.01, drift_order=1, fir_delays=[0], min_onset=-24, mask_img=None, target_affine=None, target_shape=None, smoothing_fwhm=None, memory=Memory(location=None), memory_level=1, standardize=False, signal_scaling=0, noise_model='ar1', verbose=0, n_jobs=1, minimize_memory=True, subject_label=None</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.__init__>¶</a></dt><dd><p>Initialize self. See help(type(self)) for accurate signature.</p></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.compute_contrast><code class=descname>compute_contrast</code><span class=sig-paren>(</span><em>contrast_def</em>, <em>stat_type=None</em>, <em>output_type='z_score'</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.compute_contrast>¶</a></dt><dd><p>Generate different outputs corresponding to the contrasts provided e.g. z_map, t_map, effects and variance. In multi-session case, outputs the fixed effects map.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>contrast_def</strong> : str or array of shape (n_col) or list of (string or</p> <blockquote><div><blockquote><div><p>array of shape (n_col))</p></div></blockquote><p>where <code class="docutils literal notranslate"><span class=pre>n_col</span></code> is the number of columns of the design matrix, (one array per run). If only one array is provided when there are several runs, it will be assumed that the same contrast is desired for all runs. The string can be a formula compatible with <cite>pandas.DataFrame.eval</cite>. Basically one can use the name of the conditions as they appear in the design matrix of the fitted model combined with operators +- and combined with numbers with operators +-<cite>*</cite>/.</p></div></blockquote> <p><strong>stat_type</strong> : {‘t’, ‘F’}, optional</p> <blockquote><div><p>type of the contrast</p></div></blockquote> <p><strong>output_type</strong> : str, optional</p> <blockquote><div><p>Type of the output map. Can be ‘z_score’, ‘stat’, ‘p_value’, ‘effect_size’, ‘effect_variance’ or ‘all’</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>output</strong> : Nifti1Image or dict</p> <blockquote class=last><div><p>The desired output image(s). If <code class="docutils literal notranslate"><span class=pre>output_type</span> <span class=pre>==</span> <span class=pre>'all'</span></code>, then the output is a dictionary of images, keyed by the type of image.</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.fit><code class=descname>fit</code><span class=sig-paren>(</span><em>run_imgs</em>, <em>events=None</em>, <em>confounds=None</em>, <em>design_matrices=None</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.fit>¶</a></dt><dd><p>Fit the GLM</p> <p>For each run: 1. create design matrix X 2. do a masker job: fMRI_data -> Y 3. fit regression to (Y, X)</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>run_imgs: Niimg-like object or list of Niimg-like objects,</strong></p> <blockquote><div><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html#inputing-data-file-names-or-image-objects>http://nilearn.github.io/manipulating_images/input_output.html#inputing-data-file-names-or-image-objects</a> # noqa:E501 Data on which the GLM will be fitted. If this is a list, the affine is considered the same for all.</p></div></blockquote> <p><strong>events: pandas Dataframe or string or list of pandas DataFrames or</strong></p> <blockquote><div><blockquote><div><p>strings</p></div></blockquote><p>fMRI events used to build design matrices. One events object expected per run_img. Ignored in case designs is not None. If string, then a path to a csv file is expected.</p></div></blockquote> <p><strong>confounds: pandas Dataframe, numpy array or string or</strong></p> <blockquote><div><blockquote><div><p>list of pandas DataFrames, numpy arays or strings</p></div></blockquote><p>Each column in a DataFrame corresponds to a confound variable to be included in the regression model of the respective run_img. The number of rows must match the number of volumes in the respective run_img. Ignored in case designs is not None. If string, then a path to a csv file is expected.</p></div></blockquote> <p><strong>design_matrices: pandas DataFrame or list of pandas DataFrames,</strong></p> <blockquote class=last><div><p>Design matrices that will be used to fit the GLM. If given it takes precedence over events and confounds.</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.fit_transform><code class=descname>fit_transform</code><span class=sig-paren>(</span><em>X</em>, <em>y=None</em>, <em>**fit_params</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.fit_transform>¶</a></dt><dd><p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>X</strong> : {array-like, sparse matrix, dataframe} of shape (n_samples, n_features)</p> <blockquote><div><p>Input samples.</p></div></blockquote> <p><strong>y</strong> : ndarray of shape (n_samples,), default=None</p> <blockquote><div><p>Target values (None for unsupervised transformations).</p></div></blockquote> <p><strong>**fit_params</strong> : dict</p> <blockquote><div><p>Additional fit parameters.</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>X_new</strong> : ndarray array of shape (n_samples, n_features_new)</p> <blockquote class=last><div><p>Transformed array.</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.generate_report><code class=descname>generate_report</code><span class=sig-paren>(</span><em>contrasts</em>, <em>title=None</em>, <em>bg_img='MNI152TEMPLATE'</em>, <em>threshold=3.09</em>, <em>alpha=0.001</em>, <em>cluster_threshold=0</em>, <em>height_control='fpr'</em>, <em>min_distance=8.0</em>, <em>plot_type='slice'</em>, <em>display_mode=None</em>, <em>report_dims=(1600</em>, <em>800)</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.generate_report>¶</a></dt><dd><p>Returns HTMLDocument object for a report which shows all important aspects of a fitted GLM. The object can be opened in a browser, displayed in a notebook, or saved to disk as a standalone HTML file.</p> <p>The GLM must be fitted and have the computed design matrix(ces).</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>A fitted first or second level model object.</strong></p> <p><strong>contrasts: Dict[string, ndarray] or String or List[String] or ndarray or</strong></p> <blockquote><div><p>List[ndarray]</p><p>Contrasts information for a first or second level model.</p><p>Example:</p><blockquote><div><p>Dict of contrast names and coefficients, or list of contrast names or list of contrast coefficients or contrast name or contrast coefficient</p><p>Each contrast name must be a string. Each contrast coefficient must be a list or numpy array of ints.</p></div></blockquote><p>Contrasts are passed to <code class="docutils literal notranslate"><span class=pre>contrast_def</span></code> for FirstLevelModel (<a class="reference internal"href=#nilearn.glm.first_level.FirstLevelModel.compute_contrast title=nilearn.glm.first_level.FirstLevelModel.compute_contrast><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.glm.first_level.FirstLevelModel.compute_contrast</span></code></a>) & second_level_contrast for SecondLevelModel (<a class="reference internal"href=nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel.compute_contrast title=nilearn.glm.second_level.SecondLevelModel.compute_contrast><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.glm.second_level.SecondLevelModel.compute_contrast</span></code></a>)</p></div></blockquote> <p><strong>title: String, optional</strong></p> <blockquote><div><p>If string, represents the web page’s title and primary heading, model type is sub-heading. If None, page titles and headings are autogenerated using contrast names.</p></div></blockquote> <p><strong>bg_img: Niimg-like object</strong></p> <blockquote><div><p>Default is the MNI152 template See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> The background image for mask and stat maps to be plotted on upon. To turn off background image, just pass “bg_img=None”.</p></div></blockquote> <p><strong>threshold: float</strong></p> <blockquote><div><p>Default is 3.09 Cluster forming threshold in same scale as <cite>stat_img</cite> (either a t-scale or z-scale value). Used only if height_control is None.</p></div></blockquote> <p><strong>alpha: float</strong></p> <blockquote><div><p>Default is 0.001 Number controlling the thresholding (either a p-value or q-value). Its actual meaning depends on the height_control parameter. This function translates alpha to a z-scale threshold.</p></div></blockquote> <p><strong>cluster_threshold: int, optional</strong></p> <blockquote><div><p>Default is 0 Cluster size threshold, in voxels.</p></div></blockquote> <p><strong>height_control: string or None</strong></p> <blockquote><div><p>false positive control meaning of cluster forming threshold: ‘fpr’ (default) or ‘fdr’ or ‘bonferroni’ or None</p></div></blockquote> <p><strong>min_distance: `float`</strong></p> <blockquote><div><p>For display purposes only. Minimum distance between subpeaks in mm. Default is 8 mm.</p></div></blockquote> <p><strong>plot_type: String. [‘slice’ (default) or ‘glass’]</strong></p> <blockquote><div><p>Specifies the type of plot to be drawn for the statistical maps.</p></div></blockquote> <p><strong>display_mode: string</strong></p> <blockquote><div><p>Default is ‘z’ if plot_type is ‘slice’; ‘ ortho’ if plot_type is ‘glass’.</p><p>Choose the direction of the cuts: ‘x’ - sagittal, ‘y’ - coronal, ‘z’ - axial, ‘l’ - sagittal left hemisphere only, ‘r’ - sagittal right hemisphere only, ‘ortho’ - three cuts are performed in orthogonal directions.</p><p>Possible values are: ‘ortho’, ‘x’, ‘y’, ‘z’, ‘xz’, ‘yx’, ‘yz’, ‘l’, ‘r’, ‘lr’, ‘lzr’, ‘lyr’, ‘lzry’, ‘lyrz’.</p></div></blockquote> <p><strong>report_dims: Sequence[int, int]</strong></p> <blockquote><div><p>Default is (1600, 800) pixels. Specifies width, height (in pixels) of report window within a notebook. Only applicable when inserting the report into a Jupyter notebook. Can be set after report creation using report.width, report.height.</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first>report_text: HTMLDocument Object</p> <blockquote class=last><div><p>Contains the HTML code for the GLM Report.</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.get_params><code class=descname>get_params</code><span class=sig-paren>(</span><em>deep=True</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.get_params>¶</a></dt><dd><p>Get parameters for this estimator.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>deep</strong> : bool, default=True</p> <blockquote><div><p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>params</strong> : mapping of string to any</p> <blockquote class=last><div><p>Parameter names mapped to their values.</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.predicted><code class=descname>predicted</code><span class=sig-paren>(</span><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.predicted>¶</a></dt><dd><p>Transform voxelwise predicted values to the same shape as the input Nifti1Image(s)</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>output</strong> : list</p> <blockquote class=last><div><p>a list of Nifti1Image(s)</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.r_square><code class=descname>r_square</code><span class=sig-paren>(</span><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.r_square>¶</a></dt><dd><p>Transform voxelwise r-squared values to the same shape as the input Nifti1Image(s)</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>output</strong> : list</p> <blockquote class=last><div><p>a list of Nifti1Image(s)</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.residuals><code class=descname>residuals</code><span class=sig-paren>(</span><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.residuals>¶</a></dt><dd><p>Transform voxelwise residuals to the same shape as the input Nifti1Image(s)</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>output</strong> : list</p> <blockquote class=last><div><p>a list of Nifti1Image(s)</p></div></blockquote></td></tr></tbody></table></dd></dl> <dl class=method><dt id=nilearn.glm.first_level.FirstLevelModel.set_params><code class=descname>set_params</code><span class=sig-paren>(</span><em>**params</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.glm.first_level.FirstLevelModel.set_params>¶</a></dt><dd><p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code class="docutils literal notranslate"><span class=pre>&LTcomponent>__&LTparameter></span></code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>**params</strong> : dict</p> <blockquote><div><p>Estimator parameters.</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>self</strong> : object</p> <blockquote class=last><div><p>Estimator instance.</p></div></blockquote></td></tr></tbody></table></dd></dl></dd></dl><div class=section id=examples-using-nilearn-glm-first-level-firstlevelmodel><h2>8.12.15.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.glm.first_level.FirstLevelModel</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-glm-first-level-firstlevelmodel>¶</a></h2><div tooltip="In this tutorial, we use a General Linear Model (GLM) to compare the fMRI signal during periods..."class=sphx-glr-thumbcontainer><div class=figure id=id1><img alt="Intro to GLM Analysis: a single-session, single-subject fMRI dataset"src=../../_images/sphx_glr_plot_single_subject_single_run_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/plot_single_subject_single_run.html#sphx-glr-auto-examples-plot-single-subject-single-run-py><span class="std std-ref">Intro to GLM Analysis: a single-session, single-subject fMRI dataset</span></a></span></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a decoding experiment. We use the data fr..."class=sphx-glr-thumbcontainer><div class=figure id=id2><img alt="Decoding of a dataset after GLM fit for signal extraction"src=../../_images/sphx_glr_plot_haxby_glm_decoding_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_glm_decoding.html#sphx-glr-auto-examples-02-decoding-plot-haxby-glm-decoding-py><span class="std std-ref">Decoding of a dataset after GLM fit for signal extraction</span></a></span></p></div></div><div tooltip="This example illustrates how to run a fixed effects model based on pre-computed statistics. Thi..."class=sphx-glr-thumbcontainer><div class=figure id=id3><img alt="Example of explicit fixed effects fMRI model fitting"src=../../_images/sphx_glr_plot_fixed_effects_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_fixed_effects.html#sphx-glr-auto-examples-04-glm-first-level-plot-fixed-effects-py><span class="std std-ref">Example of explicit fixed effects fMRI model fitting</span></a></span></p></div></div><div tooltip="This example shows a full step-by-step workflow of fitting a GLM to data extracted from a seed ..."class=sphx-glr-thumbcontainer><div class=figure id=id4><img alt="Default Mode Network extraction of AHDH dataset"src=../../_images/sphx_glr_plot_adhd_dmn_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_adhd_dmn.html#sphx-glr-auto-examples-04-glm-first-level-plot-adhd-dmn-py><span class="std std-ref">Default Mode Network extraction of AHDH dataset</span></a></span></p></div></div><div tooltip="FIR models are used to estimate the hemodyamic response non-parametrically. The example below s..."class=sphx-glr-thumbcontainer><div class=figure id=id5><img alt="Analysis of an fMRI dataset with a Finite Impule Response (FIR) model"src=../../_images/sphx_glr_plot_fir_model_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_fir_model.html#sphx-glr-auto-examples-04-glm-first-level-plot-fir-model-py><span class="std std-ref">Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</span></a></span></p></div></div><div tooltip="The example shows the analysis of an SPM dataset studying face perception.  The anaylsis is per..."class=sphx-glr-thumbcontainer><div class=figure id=id6><img alt="Single-subject data (two sessions) in native space"src=../../_images/sphx_glr_plot_spm_multimodal_faces_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_spm_multimodal_faces.html#sphx-glr-auto-examples-04-glm-first-level-plot-spm-multimodal-faces-py><span class="std std-ref">Single-subject data (two sessions) in native space</span></a></span></p></div></div><div tooltip="Here, we will go through a full step-by-step example of fitting a GLM to experimental data and ..."class=sphx-glr-thumbcontainer><div class=figure id=id7><img alt="Simple example of two-session fMRI model fitting"src=../../_images/sphx_glr_plot_fiac_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_fiac_analysis.html#sphx-glr-auto-examples-04-glm-first-level-plot-fiac-analysis-py><span class="std std-ref">Simple example of two-session fMRI model fitting</span></a></span></p></div></div><div tooltip=" Full step-by-step example of fitting a GLM to perform a first level analysis in an openneuro B..."class=sphx-glr-thumbcontainer><div class=figure id=id8><img alt="First level analysis of a complete BIDS dataset from openneuro"src=../../_images/sphx_glr_plot_bids_features_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py><span class="std std-ref">First level analysis of a complete BIDS dataset from openneuro</span></a></span></p></div></div><div tooltip="Here we fit a First Level GLM with the minimize_memory-argument set to False. By doing so, the ..."class=sphx-glr-thumbcontainer><div class=figure id=id9><img alt="Predicted time series and residuals"src=../../_images/sphx_glr_plot_predictions_residuals_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_predictions_residuals.html#sphx-glr-auto-examples-04-glm-first-level-plot-predictions-residuals-py><span class="std std-ref">Predicted time series and residuals</span></a></span></p></div></div><div tooltip="In this tutorial, we study how first-level models are parametrized for fMRI data analysis and c..."class=sphx-glr-thumbcontainer><div class=figure id=id10><img alt="Understanding parameters of the first-level model"src=../../_images/sphx_glr_plot_first_level_details_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_first_level_details.html#sphx-glr-auto-examples-04-glm-first-level-plot-first-level-details-py><span class="std std-ref">Understanding parameters of the first-level model</span></a></span></p></div></div><div tooltip=" Full step-by-step example of fitting a GLM to perform a first and second level analysis in a B..."class=sphx-glr-thumbcontainer><div class=figure id=id11><img alt="BIDS dataset first and second level analysis"src=../../_images/sphx_glr_plot_bids_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_bids_analysis.html#sphx-glr-auto-examples-07-advanced-plot-bids-analysis-py><span class="std std-ref">BIDS dataset first and second level analysis</span></a></span></p></div></div><div style=clear:both></div></div></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.12.15.1. nilearn.glm.first_level.FirstLevelModel</a><ul><li><a class="reference internal"href=#examples-using-nilearn-glm-first-level-firstlevelmodel>8.12.15.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.glm.first_level.FirstLevelModel</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.glm.threshold_stats_img.html>8.12.14. nilearn.glm.threshold_stats_img</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.glm.first_level.check_design_matrix.html>8.12.15.2. nilearn.glm.first_level.check_design_matrix</a></p><div id=searchbox role=search style=display:none><h3>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input name=q><input type=submit value=Go><input name=check_keywords type=hidden value=yes><input name=area type=hidden value=default></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2020. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 1.8.5. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.glm.first_level.FirstLevelModel.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>