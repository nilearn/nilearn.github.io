<!doctypehtml><html lang=en xmlns=http://www.w3.org/1999/xhtml><meta content=IE=Edge http-equiv=X-UA-Compatible><meta content="text/html; charset=utf-8"http-equiv=Content-Type><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/language_data.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.2.22. nilearn.datasets.fetch_localizer_calculation_task"href=nilearn.datasets.fetch_localizer_calculation_task.html rel=next><link title="8.2.20. nilearn.datasets.fetch_localizer_button_task"href=nilearn.datasets.fetch_localizer_button_task.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script></head><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.2.22. nilearn.datasets.fetch_localizer_calculation_task"accesskey=N href=nilearn.datasets.fetch_localizer_calculation_task.html>next</a> |</li><li class=right><a title="8.2.20. nilearn.datasets.fetch_localizer_button_task"accesskey=P href=nilearn.datasets.fetch_localizer_button_task.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html>8. Reference documentation: all nilearn functions</a> »</li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class="first admonition-title">Note</p><p class=last>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-datasets-fetch-localizer-contrasts><h1>8.2.21. nilearn.datasets.fetch_localizer_contrasts<a title="Permalink to this headline"class=headerlink href=#nilearn-datasets-fetch-localizer-contrasts>¶</a></h1><dl class=function><dt id=nilearn.datasets.fetch_localizer_contrasts><code class=descclassname>nilearn.datasets.</code><code class=descname>fetch_localizer_contrasts</code><span class=sig-paren>(</span><em>contrasts</em>, <em>n_subjects=None</em>, <em>get_tmaps=False</em>, <em>get_masks=False</em>, <em>get_anats=False</em>, <em>data_dir=None</em>, <em>url=None</em>, <em>resume=True</em>, <em>verbose=1</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.datasets.fetch_localizer_contrasts>¶</a></dt><dd><p>Download and load Brainomics/Localizer dataset (94 subjects).</p> <p>“The Functional Localizer is a simple and fast acquisition procedure based on a 5-minute functional magnetic resonance imaging (fMRI) sequence that can be run as easily and as systematically as an anatomical scan. This protocol captures the cerebral bases of auditory and visual perception, motor actions, reading, language comprehension and mental calculation at an individual level. Individual functional maps are reliable and quite precise. The procedure is decribed in more detail on the Functional Localizer page.” (see <a class="reference external"href=http://brainomics.cea.fr/localizer/>http://brainomics.cea.fr/localizer/</a>)</p> <p>You may cite Papadopoulos Orfanos, Dimitri, <em>et al.</em> when using this dataset.</p> <p>Scientific results obtained using this dataset are described in Pinel <em>et al.</em>, 2007.</p> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>contrasts: list of str</strong></p> <blockquote><div><p>The contrasts to be fetched (for all 94 subjects available). Allowed values are:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=p>{</span><span class=s2>"checkerboard"</span><span class=p>,</span>
<span class=s2>"horizontal checkerboard"</span><span class=p>,</span>
<span class=s2>"vertical checkerboard"</span><span class=p>,</span>
<span class=s2>"horizontal vs vertical checkerboard"</span><span class=p>,</span>
<span class=s2>"vertical vs horizontal checkerboard"</span><span class=p>,</span>
<span class=s2>"sentence listening"</span><span class=p>,</span>
<span class=s2>"sentence reading"</span><span class=p>,</span>
<span class=s2>"sentence listening and reading"</span><span class=p>,</span>
<span class=s2>"sentence reading vs checkerboard"</span><span class=p>,</span>
<span class=s2>"calculation (auditory cue)"</span><span class=p>,</span>
<span class=s2>"calculation (visual cue)"</span><span class=p>,</span>
<span class=s2>"calculation (auditory and visual cue)"</span><span class=p>,</span>
<span class=s2>"calculation (auditory cue) vs sentence listening"</span><span class=p>,</span>
<span class=s2>"calculation (visual cue) vs sentence reading"</span><span class=p>,</span>
<span class=s2>"calculation vs sentences"</span><span class=p>,</span>
<span class=s2>"calculation (auditory cue) and sentence listening"</span><span class=p>,</span>
<span class=s2>"calculation (visual cue) and sentence reading"</span><span class=p>,</span>
<span class=s2>"calculation and sentence listening/reading"</span><span class=p>,</span>
<span class=s2>"calculation (auditory cue) and sentence listening vs "</span>
<span class=s2>"calculation (visual cue) and sentence reading"</span><span class=p>,</span>
<span class=s2>"calculation (visual cue) and sentence reading vs checkerboard"</span><span class=p>,</span>
<span class=s2>"calculation and sentence listening/reading vs button press"</span><span class=p>,</span>
<span class=s2>"left button press (auditory cue)"</span><span class=p>,</span>
<span class=s2>"left button press (visual cue)"</span><span class=p>,</span>
<span class=s2>"left button press"</span><span class=p>,</span>
<span class=s2>"left vs right button press"</span><span class=p>,</span>
<span class=s2>"right button press (auditory cue)"</span><span class=p>,</span>
<span class=s2>"right button press (visual cue)"</span><span class=p>,</span>
<span class=s2>"right button press"</span><span class=p>,</span>
<span class=s2>"right vs left button press"</span><span class=p>,</span>
<span class=s2>"button press (auditory cue) vs sentence listening"</span><span class=p>,</span>
<span class=s2>"button press (visual cue) vs sentence reading"</span><span class=p>,</span>
<span class=s2>"button press vs calculation and sentence listening/reading"</span><span class=p>}</span>
</pre></div></div><p>or equivalently on can use the original names:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=p>{</span><span class=s2>"checkerboard"</span><span class=p>,</span>
<span class=s2>"horizontal checkerboard"</span><span class=p>,</span>
<span class=s2>"vertical checkerboard"</span><span class=p>,</span>
<span class=s2>"horizontal vs vertical checkerboard"</span><span class=p>,</span>
<span class=s2>"vertical vs horizontal checkerboard"</span><span class=p>,</span>
<span class=s2>"auditory sentences"</span><span class=p>,</span>
<span class=s2>"visual sentences"</span><span class=p>,</span>
<span class=s2>"auditory&visual sentences"</span><span class=p>,</span>
<span class=s2>"visual sentences vs checkerboard"</span><span class=p>,</span>
<span class=s2>"auditory calculation"</span><span class=p>,</span>
<span class=s2>"visual calculation"</span><span class=p>,</span>
<span class=s2>"auditory&visual calculation"</span><span class=p>,</span>
<span class=s2>"auditory calculation vs auditory sentences"</span><span class=p>,</span>
<span class=s2>"visual calculation vs sentences"</span><span class=p>,</span>
<span class=s2>"auditory&visual calculation vs sentences"</span><span class=p>,</span>
<span class=s2>"auditory processing"</span><span class=p>,</span>
<span class=s2>"visual processing"</span><span class=p>,</span>
<span class=s2>"visual processing vs auditory processing"</span><span class=p>,</span>
<span class=s2>"auditory processing vs visual processing"</span><span class=p>,</span>
<span class=s2>"visual processing vs checkerboard"</span><span class=p>,</span>
<span class=s2>"cognitive processing vs motor"</span><span class=p>,</span>
<span class=s2>"left auditory click"</span><span class=p>,</span>
<span class=s2>"left visual click"</span><span class=p>,</span>
<span class=s2>"left auditory&visual click"</span><span class=p>,</span>
<span class=s2>"left auditory & visual click vs right auditory&visual click"</span><span class=p>,</span>
<span class=s2>"right auditory click"</span><span class=p>,</span>
<span class=s2>"right visual click"</span><span class=p>,</span>
<span class=s2>"right auditory&visual click"</span><span class=p>,</span>
<span class=s2>"right auditory & visual click vs left auditory&visual click"</span><span class=p>,</span>
<span class=s2>"auditory click vs auditory sentences"</span><span class=p>,</span>
<span class=s2>"visual click vs visual sentences"</span><span class=p>,</span>
<span class=s2>"auditory&visual motor vs cognitive processing"</span><span class=p>}</span>
</pre></div></div></div></blockquote> <p><strong>n_subjects: int or list, optional</strong></p> <blockquote><div><p>The number or list of subjects to load. If None is given, all 94 subjects are used.</p></div></blockquote> <p><strong>get_tmaps: boolean</strong></p> <blockquote><div><p>Whether t maps should be fetched or not.</p></div></blockquote> <p><strong>get_masks: boolean</strong></p> <blockquote><div><p>Whether individual masks should be fetched or not.</p></div></blockquote> <p><strong>get_anats: boolean</strong></p> <blockquote><div><p>Whether individual structural images should be fetched or not.</p></div></blockquote> <p><strong>data_dir: string, optional</strong></p> <blockquote><div><p>Path of the data directory. Used to force data storage in a specified location.</p></div></blockquote> <p><strong>url: string, optional</strong></p> <blockquote><div><p>Override download URL. Used for test only (or if you setup a mirror of the data).</p></div></blockquote> <p><strong>resume: bool</strong></p> <blockquote><div><p>Whether to resume download of a partly-downloaded file.</p></div></blockquote> <p><strong>verbose: int</strong></p> <blockquote><div><p>Verbosity level (0 means no message).</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first>data: Bunch</p> <blockquote class=last><div><p>Dictionary-like object, the interest attributes are :</p><ul class=simple><li><dl class="first docutils"><dt>‘cmaps’: string list</dt><dd>Paths to nifti contrast maps</dd></dl></li><li><dl class="first docutils"><dt>‘tmaps’ string list (if ‘get_tmaps’ set to True)</dt><dd>Paths to nifti t maps</dd></dl></li><li><dl class="first docutils"><dt>‘masks’: string list</dt><dd>Paths to nifti files corresponding to the subjects individual masks</dd></dl></li><li><dl class="first docutils"><dt>‘anats’: string</dt><dd>Path to nifti files corresponding to the subjects structural images</dd></dl></li></ul></div></blockquote></td></tr></tbody></table> <div class="admonition seealso"><p class="first admonition-title">See also</p><p class=last><a class="reference internal"href=nilearn.datasets.fetch_localizer_calculation_task.html#nilearn.datasets.fetch_localizer_calculation_task title=nilearn.datasets.fetch_localizer_calculation_task><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_calculation_task</span></code></a>, <a class="reference internal"href=nilearn.datasets.fetch_localizer_button_task.html#nilearn.datasets.fetch_localizer_button_task title=nilearn.datasets.fetch_localizer_button_task><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_button_task</span></code></a></p></div> <p class=rubric>References</p> <ul class=simple><li>Papadopoulos Orfanos, Dimitri, et al. “The Brainomics/Localizer database.” NeuroImage 144.B (2017): 309.</li><li>Pinel, Philippe, et al. “Fast reproducible identification and large-scale databasing of individual functional cognitive networks.” BMC Neuroscience 8.1 (2007): 91.</li></ul></dd></dl><div class=section id=examples-using-nilearn-datasets-fetch-localizer-contrasts><h2>8.2.21.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_contrasts</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-datasets-fetch-localizer-contrasts>¶</a></h2><div tooltip='This script showcases the so-called "All resolution inference" procedure, in which the proporti...'class=sphx-glr-thumbcontainer><div class=figure id=id1><img alt="Second-level fMRI model: true positive proportion in clusters"src=../../_images/sphx_glr_plot_proportion_activated_voxels_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_proportion_activated_voxels.html#sphx-glr-auto-examples-05-glm-second-level-plot-proportion-activated-voxels-py><span class="std std-ref">Second-level fMRI model: true positive proportion in clusters</span></a></span></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a second level analysis in experimental d..."class=sphx-glr-thumbcontainer><div class=figure id=id2><img alt="Second-level fMRI model: a paired-sample test"src=../../_images/sphx_glr_plot_second_level_paired_sample_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_paired_sample_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-paired-sample-test-py><span class="std std-ref">Second-level fMRI model: a paired-sample test</span></a></span></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a second level analysis in experimental d..."class=sphx-glr-thumbcontainer><div class=figure id=id3><img alt="Second-level fMRI model: a two-sample test"src=../../_images/sphx_glr_plot_second_level_two_sample_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_two_sample_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-two-sample-test-py><span class="std std-ref">Second-level fMRI model: a two-sample test</span></a></span></p></div></div><div tooltip="Full step-by-step example of fitting a GLM to perform a second-level analysis (one-sample test)..."class=sphx-glr-thumbcontainer><div class=figure id=id4><img alt="Second-level fMRI model: one sample test"src=../../_images/sphx_glr_plot_second_level_one_sample_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_one_sample_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-one-sample-test-py><span class="std std-ref">Second-level fMRI model: one sample test</span></a></span></p></div></div><div tooltip="This example shows the results obtained in a group analysis using a more complex contrast than ..."class=sphx-glr-thumbcontainer><div class=figure id=id5><img alt="Example of generic design in second-level models"src=../../_images/sphx_glr_plot_second_level_association_test_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_association_test.html#sphx-glr-auto-examples-05-glm-second-level-plot-second-level-association-test-py><span class="std std-ref">Example of generic design in second-level models</span></a></span></p></div></div><div tooltip="This example shows the results obtained in a massively univariate analysis performed at the int..."class=sphx-glr-thumbcontainer><div class=figure id=id6><img alt="Massively univariate analysis of a motor task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_mass_univariate_methods_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html#sphx-glr-auto-examples-07-advanced-plot-localizer-mass-univariate-methods-py><span class="std std-ref">Massively univariate analysis of a motor task from the Localizer dataset</span></a></span></p></div></div><div style=clear:both></div></div></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.2.21. nilearn.datasets.fetch_localizer_contrasts</a><ul><li><a class="reference internal"href=#examples-using-nilearn-datasets-fetch-localizer-contrasts>8.2.21.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_localizer_contrasts</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.datasets.fetch_localizer_button_task.html>8.2.20. nilearn.datasets.fetch_localizer_button_task</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.datasets.fetch_localizer_calculation_task.html>8.2.22. nilearn.datasets.fetch_localizer_calculation_task</a></p><div id=searchbox role=search style=display:none><h3>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input name=q><input type=submit value=Go><input name=check_keywords type=hidden value=yes><input name=area type=hidden value=default></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2020. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 1.8.5. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.datasets.fetch_localizer_contrasts.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>