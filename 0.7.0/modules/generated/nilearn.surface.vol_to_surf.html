<!doctypehtml><html lang=en xmlns=http://www.w3.org/1999/xhtml><meta content=IE=Edge http-equiv=X-UA-Compatible><meta content="text/html; charset=utf-8"http-equiv=Content-Type><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/language_data.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="9. Nilearn usage examples"href=../../auto_examples/index.html rel=next><link title="8.14.2. nilearn.surface.load_surf_mesh"href=nilearn.surface.load_surf_mesh.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script></head><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="9. Nilearn usage examples"accesskey=N href=../../auto_examples/index.html>next</a> |</li><li class=right><a title="8.14.2. nilearn.surface.load_surf_mesh"accesskey=P href=nilearn.surface.load_surf_mesh.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html>8. Reference documentation: all nilearn functions</a> »</li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class="first admonition-title">Note</p><p class=last>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-surface-vol-to-surf><h1>8.14.3. nilearn.surface.vol_to_surf<a title="Permalink to this headline"class=headerlink href=#nilearn-surface-vol-to-surf>¶</a></h1><dl class=function><dt id=nilearn.surface.vol_to_surf><code class=descclassname>nilearn.surface.</code><code class=descname>vol_to_surf</code><span class=sig-paren>(</span><em>img</em>, <em>surf_mesh</em>, <em>radius=3.0</em>, <em>interpolation='linear'</em>, <em>kind='auto'</em>, <em>n_samples=None</em>, <em>mask_img=None</em>, <em>inner_mesh=None</em>, <em>depth=None</em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.surface.vol_to_surf>¶</a></dt><dd><p>Extract surface data from a Nifti image.</p> <div class=versionadded><p><span class=versionmodified>New in version 0.4.0.</span></p></div> <table class="docutils field-list"frame=void rules=none><col class=field-name><col class=field-body><tbody valign=top><tr class="field-odd field"><th class=field-name>Parameters:</th><td class=field-body><p class=first><strong>img</strong> : Niimg-like object, 3d or 4d.</p> <blockquote><div><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></div></blockquote> <p><strong>surf_mesh</strong> : str or numpy.ndarray</p> <blockquote><div><p>Either a file containing surface mesh geometry (valid formats are .gii or Freesurfer specific files such as .orig, .pial, .sphere, .white, .inflated) or a list of two Numpy arrays, the first containing the x-y-z coordinates of the mesh vertices, the second containing the indices (into coords) of the mesh faces.</p></div></blockquote> <p><strong>radius</strong> : float, optional (default=3.).</p> <blockquote><div><p>The size (in mm) of the neighbourhood from which samples are drawn around each node. Ignored if <cite>inner_mesh</cite> is provided.</p></div></blockquote> <p><strong>interpolation</strong> : {‘linear’, ‘nearest’}</p> <blockquote><div><p>How the image intensity is measured at a sample point.</p><ul class=simple><li><dl class="first docutils"><dt>‘linear’ (the default):</dt><dd>Use a trilinear interpolation of neighboring voxels.</dd></dl></li><li><dl class="first docutils"><dt>‘nearest’:</dt><dd>Use the intensity of the nearest voxel.</dd></dl></li></ul><p>For one image, the speed difference is small, ‘linear’ takes about x1.5 more time. For many images, ‘nearest’ scales much better, up to x20 faster.</p></div></blockquote> <p><strong>kind</strong> : {‘auto’, ‘depth’, ‘line’, ‘ball’}</p> <blockquote><div><p>The strategy used to sample image intensities around each vertex.</p><ul class=simple><li><dl class="first docutils"><dt>‘auto’ (the default):</dt><dd>chooses ‘depth’ if <cite>inner_mesh</cite> is provided and ‘line’ otherwise.</dd></dl></li><li><dl class="first docutils"><dt>‘depth’:</dt><dd><cite>inner_mesh</cite> must be a mesh whose nodes correspond to those in <cite>surf_mesh</cite>. For example, <cite>inner_mesh</cite> could be a white matter surface mesh and <cite>surf_mesh</cite> a pial surface mesh. Samples are placed between each pair of corresponding nodes at the specified cortical depths (regularly spaced by default, see <cite>depth</cite> parameter).</dd></dl></li><li><dl class="first docutils"><dt>‘line’:</dt><dd>samples are placed along the normal to the mesh, at the positions specified by <cite>depth</cite>, or by default regularly spaced over the interval [- <cite>radius</cite>, + <cite>radius</cite>].</dd></dl></li><li><dl class="first docutils"><dt>‘ball’:</dt><dd>samples are regularly spaced inside a ball centered at the mesh vertex.</dd></dl></li></ul></div></blockquote> <p><strong>n_samples</strong> : int or None, optional (default=None)</p> <blockquote><div><p>How many samples are drawn around each vertex and averaged. If <code class="docutils literal notranslate"><span class=pre>None</span></code>, use a reasonable default for the chosen sampling strategy (20 for ‘ball’ or 10 for ‘line’). For performance reasons, if using <cite>kind</cite> =”ball”, choose <cite>n_samples</cite> in [10, 20, 40, 80, 160] (default is 20), because cached positions are available.</p></div></blockquote> <p><strong>mask_img</strong> : Niimg-like object or None, optional (default=None)</p> <blockquote><div><p>Samples falling out of this mask or out of the image are ignored. If <code class="docutils literal notranslate"><span class=pre>None</span></code>, don’t apply any mask.</p></div></blockquote> <p><strong>inner_mesh</strong> : str or numpy.ndarray, optional (default=None)</p> <blockquote><div><p>Either a file containing a surface mesh or a pair of ndarrays (coordinates, triangles). If provided this is an inner surface that is nested inside the one represented by <cite>surf_mesh</cite> – e.g. <cite>surf_mesh</cite> is a pial surface and <cite>inner_mesh</cite> a white matter surface. In this case nodes in both meshes must correspond: node i in <cite>surf_mesh</cite> is just across the gray matter thickness from node i in <cite>inner_mesh</cite>. Image values for index i are then sampled along the line joining these two points (if <cite>kind</cite> is ‘auto’ or ‘depth’).</p></div></blockquote> <p><strong>depth</strong> : sequence of floats or None, optional (default=None)</p> <blockquote><div><p>The cortical depth of samples. If provided, n_samples is ignored. When <cite>inner_mesh</cite> is provided, each element of <cite>depth</cite> is a fraction of the distance from <cite>mesh</cite> to <cite>inner_mesh</cite>: 0 is exactly on the outer surface, .5 is halfway, 1. is exactly on the inner surface. <cite>depth</cite> entries can be negative or greater than 1. When <cite>inner_mesh</cite> is not provided and <cite>kind</cite> is “line”, each element of <cite>depth</cite> is a fraction of <cite>radius</cite> along the inwards normal at each mesh node. For example if <cite>radius==1</cite> and <cite>depth==[-.5, 0.]</cite>, for each node values will be sampled .5 mm outside of the surface and exactly at the node position. This parameter is not supported for the “ball” strategy so passing <cite>depth</cite> when <cite>kind==”ball”</cite> results in a <cite>ValueError</cite>.</p></div></blockquote></td></tr><tr class="field-even field"><th class=field-name>Returns:</th><td class=field-body><p class=first><strong>texture</strong> : numpy.ndarray, 1d or 2d.</p> <blockquote class=last><div><p>If 3D image is provided, a 1d vector is returned, containing one value for each mesh node. If 4D image is provided, a 2d array is returned, where each row corresponds to a mesh node.</p></div></blockquote></td></tr></tbody></table> <p class=rubric>Notes</p> <p>This function computes a value for each vertex of the mesh. In order to do so, it selects a few points in the volume surrounding that vertex, interpolates the image intensities at these sampling positions, and averages the results.</p> <p>Three strategies are available to select these positions.</p> <blockquote><div><ul class=simple><li>with ‘depth’, data is sampled at various cortical depths between corresponding nodes of <cite>surface_mesh</cite> and <cite>inner_mesh</cite> (which can be, for example, a pial surface and a white matter surface).</li><li>‘ball’ uses points regularly spaced in a ball centered at the mesh vertex. The radius of the ball is controlled by the parameter <cite>radius</cite>.</li><li>‘line’ starts by drawing the normal to the mesh passing through this vertex. It then selects a segment of this normal, centered at the vertex, of length 2 * <cite>radius</cite>. Image intensities are measured at points regularly spaced on this normal segment, or at positions determined by <cite>depth</cite>.</li><li>(‘auto’ chooses ‘depth’ if <cite>inner_mesh</cite> is provided and ‘line’ otherwise)</li></ul></div></blockquote> <p>You can control how many samples are drawn by setting <cite>n_samples</cite>, or their position by setting <cite>depth</cite>.</p> <p>Once the sampling positions are chosen, those that fall outside of the 3d image (or outside of the mask if you provided one) are discarded. If all sample positions are discarded (which can happen, for example, if the vertex itself is outside of the support of the image), the projection at this vertex will be <code class="docutils literal notranslate"><span class=pre>numpy.nan</span></code>.</p> <p>The 3d image then needs to be interpolated at each of the remaining points. Two options are available: ‘nearest’ selects the value of the nearest voxel, and ‘linear’ performs trilinear interpolation of neighbouring voxels. ‘linear’ may give better results - for example, the projected values are more stable when resampling the 3d image or applying affine transformations to it. For one image, the speed difference is small, ‘linear’ takes about x1.5 more time. For many images, ‘nearest’ scales much better, up to x20 faster.</p> <p>Once the 3d image has been interpolated at each sample point, the interpolated values are averaged to produce the value associated to this particular mesh vertex.</p> <p>WARNING: This function is experimental and details such as the interpolation method are subject to change.</p></dd></dl><div class=section id=examples-using-nilearn-surface-vol-to-surf><h2>8.14.3.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.surface.vol_to_surf</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-surface-vol-to-surf>¶</a></h2><div tooltip="In nilearn, nilearn.surface.vol_to_surf allows us to measure values of a 3d volume at the nodes..."class=sphx-glr-thumbcontainer><div class=figure id=id1><img alt="Technical point: Illustration of the volume to surface sampling schemes"src=../../_images/sphx_glr_plot_surface_projection_strategies_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/01_plotting/plot_surface_projection_strategies.html#sphx-glr-auto-examples-01-plotting-plot-surface-projection-strategies-py><span class="std std-ref">Technical point: Illustration of the volume to surface sampling schemes</span></a></span></p></div></div><div tooltip="project a 3D statistical map onto a cortical mesh using nilearn.surface.vol_to_surf. Display a ..."class=sphx-glr-thumbcontainer><div class=figure id=id2><img alt="Making a surface plot of a 3D statistical map"src=../../_images/sphx_glr_plot_3d_map_to_surface_projection_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/01_plotting/plot_3d_map_to_surface_projection.html#sphx-glr-auto-examples-01-plotting-plot-3d-map-to-surface-projection-py><span class="std std-ref">Making a surface plot of a 3D statistical map</span></a></span></p></div></div><div tooltip="This is a demo for surface-based searchlight decoding, as described in: Chen, Y., Namburi, P., ..."class=sphx-glr-thumbcontainer><div class=figure id=id3><img alt="Cortical surface-based searchlight decoding"src=../../_images/sphx_glr_plot_haxby_searchlight_surface_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight_surface.html#sphx-glr-auto-examples-02-decoding-plot-haxby-searchlight-surface-py><span class="std std-ref">Cortical surface-based searchlight decoding</span></a></span></p></div></div><div tooltip="A full step-by-step example of fitting a GLM to experimental data sampled on the cortical surfa..."class=sphx-glr-thumbcontainer><div class=figure id=id4><img alt="Example of surface-based first-level analysis"src=../../_images/sphx_glr_plot_localizer_surface_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html#sphx-glr-auto-examples-04-glm-first-level-plot-localizer-surface-analysis-py><span class="std std-ref">Example of surface-based first-level analysis</span></a></span></p></div></div><div tooltip=" Full step-by-step example of fitting a GLM (first and second level analysis) in a 10-subjects ..."class=sphx-glr-thumbcontainer><div class=figure id=id5><img alt="Surface-based dataset first and second level analysis of a dataset"src=../../_images/sphx_glr_plot_surface_bids_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_surface_bids_analysis.html#sphx-glr-auto-examples-07-advanced-plot-surface-bids-analysis-py><span class="std std-ref">Surface-based dataset first and second level analysis of a dataset</span></a></span></p></div></div><div style=clear:both></div></div></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.14.3. nilearn.surface.vol_to_surf</a><ul><li><a class="reference internal"href=#examples-using-nilearn-surface-vol-to-surf>8.14.3.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.surface.vol_to_surf</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.surface.load_surf_mesh.html>8.14.2. nilearn.surface.load_surf_mesh</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=../../auto_examples/index.html>9. Nilearn usage examples</a></p><div id=searchbox role=search style=display:none><h3>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input name=q><input type=submit value=Go><input name=check_keywords type=hidden value=yes><input name=area type=hidden value=default></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2020. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 1.8.5. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.surface.vol_to_surf.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>