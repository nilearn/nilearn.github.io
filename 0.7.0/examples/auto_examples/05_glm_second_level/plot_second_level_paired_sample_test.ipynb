{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nSecond-level fMRI model: a paired-sample test\n=============================================\n\nFull step-by-step example of fitting a GLM to perform a second level analysis\nin experimental data and visualizing the results.\n\nMore specifically:\n\n1. A sample of n=16 visual activity fMRIs are downloaded.\n2. A paired one-sample t-test is applied to the brain maps in order to see the effect\n   of the contrast difference across subjects.\n\nThe contrast is between responses to vertical versus horizontal\ncheckerboards that are retinotopically distinct. At the individual\nlevel, these stimuli are sometimes used to map the borders of primary\nvisual areas. At the group level, such a mapping is not possible. Yet,\nwe may observe some significant effects in these areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom nilearn import plotting\nfrom nilearn.datasets import fetch_localizer_contrasts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetch dataset\n--------------\nWe download a list of left vs right button press contrasts from a\nlocalizer dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_subjects = 16\nsample_vertical = fetch_localizer_contrasts(\n    [\"vertical checkerboard\"], n_subjects, get_tmaps=True)\nsample_horizontal = fetch_localizer_contrasts(\n    [\"horizontal checkerboard\"], n_subjects, get_tmaps=True)\n\n# What remains implicit here is that there is a one-to-one\n# correspondence between the two samples: the first image of both\n# samples comes from subject S1, the second from subject S2 etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimate second level model\n---------------------------\nWe define the input maps and the design matrix for the second level model\nand fit it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "second_level_input = sample_vertical['cmaps'] + sample_horizontal['cmaps']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we model the effect of conditions (sample 1 vs sample 2).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\ncondition_effect = np.hstack(([1] * n_subjects, [- 1] * n_subjects))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Subsequently, we can model the subject effect: each subject is observed in sample 1 and sample 2.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_effect = np.vstack((np.eye(n_subjects), np.eye(n_subjects)))\nsubjects = ['S%02d' % i for i in range(1, n_subjects + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then assemble those in a design matrix and\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = pd.DataFrame(\n    np.hstack((condition_effect[:, np.newaxis], subject_effect)),\n    columns=['vertical vs horizontal'] + subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "plot the design_matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_design_matrix\nplot_design_matrix(design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We formally specify the analysis model and fit it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.second_level import SecondLevelModel\nsecond_level_model = SecondLevelModel().fit(\n    second_level_input, design_matrix=design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimating the contrast is very simple. We can just provide the column\nname of the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_map = second_level_model.compute_contrast('vertical vs horizontal',\n                                            output_type='z_score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We threshold the second level contrast and plot it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "threshold = 3.1  # correponds to  p < .001, uncorrected\ndisplay = plotting.plot_glass_brain(\n    z_map, threshold=threshold, colorbar=True, plot_abs=False,\n    title='vertical vs horizontal checkerboard (unc p<0.001')\n\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsurprisingly, we see activity in the primary visual cortex, both positive\nand negative.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}