{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# BIDS dataset first and second level analysis\n\nFull step-by-step example of fitting a :term:`GLM`\nto perform a first and second level\nanalysis in a :term:`BIDS` dataset and visualizing the results.\nDetails about the :term:`BIDS` standard can be consulted at\n[https://bids.neuroimaging.io/](https://bids.neuroimaging.io/).\n\nMore specifically:\n\n1. Download an :term:`fMRI` :term:`BIDS` dataset\n   with two language conditions to contrast.\n2. Extract first level model objects automatically\n   from the :term:`BIDS` dataset.\n3. Fit a second level model on the fitted first level models.\n   Notice that in this case the preprocessed :term:`bold<BOLD>`\n   images were already normalized to the same :term:`MNI` space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch example :term:`BIDS` dataset\nWe download a simplified :term:`BIDS` dataset made available for illustrative\npurposes. It contains only the necessary\ninformation to run a statistical analysis using Nilearn. The raw data\nsubject folders only contain bold.json and events.tsv files, while the\nderivatives folder includes the preprocessed files preproc.nii and the\nconfounds.tsv files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_language_localizer_demo_dataset\n\ndata = fetch_language_localizer_demo_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the location of the dataset on disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data.data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain automatically FirstLevelModel objects and fit arguments\nFrom the dataset directory we automatically obtain\nthe FirstLevelModel objects\nwith their subject_id filled from the :term:`BIDS` dataset.\nMoreover, we obtain for each model a dictionary with run_imgs,\nevents and confounder regressors\nsince in this case a confounds.tsv file is available\nin the :term:`BIDS` dataset.\nTo get the first level models we only have to specify the dataset directory\nand the task_label as specified in the file names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import first_level_from_bids\n\ntask_label = \"languagelocalizer\"\n(\n    models,\n    models_run_imgs,\n    models_events,\n    models_confounds,\n) = first_level_from_bids(\n    data.data_dir,\n    task_label,\n    img_filters=[(\"desc\", \"preproc\")],\n    n_jobs=2,\n    space_label=\"\",\n    smoothing_fwhm=8,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick sanity check on fit arguments\nAdditional checks or information extraction from pre-processed data can\nbe made here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just expect one run_img per subject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nprint([Path(run).name for run in models_run_imgs[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only confounds stored are regressors obtained from motion correction. As\nwe can verify from the column headers of the confounds table corresponding\nto the only run_img present.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(models_confounds[0][0].columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During this acquisition the subject read blocks of sentences and\nconsonant strings. So these are our only two conditions in events.\nWe verify there are 12 blocks for each condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(models_events[0][0][\"trial_type\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First level model estimation\nNow we simply fit each first level model and plot for each subject the\n:term:`contrast` that reveals the language network (language - string).\nNotice that we can define a :term:`contrast`\nusing the names of the conditions specified in the events dataframe.\nSum, subtraction and scalar multiplication are allowed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the threshold as the z-variate with an uncorrected p-value of 0.001.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n\np001_unc = norm.isf(0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare figure for concurrent plot of individual maps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from math import ceil\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nncols = 2\nnrows = ceil(len(models) / ncols)\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 12))\naxes = np.atleast_2d(axes)\nmodel_and_args = zip(\n    models, models_run_imgs, models_events, models_confounds, strict=False\n)\nfor midx, (model, imgs, events, confounds) in enumerate(model_and_args):\n    # fit the GLM\n    model.fit(imgs, events, confounds)\n    # compute the contrast of interest\n    zmap = model.compute_contrast(\"language-string\")\n    plotting.plot_glass_brain(\n        zmap,\n        threshold=p001_unc,\n        title=f\"sub-{model.subject_label}\",\n        axes=axes[int(midx / ncols), int(midx % ncols)],\n        plot_abs=False,\n        colorbar=True,\n        display_mode=\"x\",\n        vmin=-12,\n        vmax=12,\n    )\nfig.suptitle(\"subjects z_map language network (unc p<0.001)\")\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second level model estimation\nWe just have to provide the list of fitted FirstLevelModel objects\nto the SecondLevelModel object for estimation. We can do this because\nall subjects share a similar design matrix (same variables reflected in\ncolumn names).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.second_level import SecondLevelModel\n\nsecond_level_input = models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we apply a smoothing of 8mm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "second_level_model = SecondLevelModel(smoothing_fwhm=8.0, n_jobs=2)\nsecond_level_model = second_level_model.fit(second_level_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing contrasts at the second level is as simple as at the first level.\nSince we are not providing confounders we are performing a one-sample test\nat the second level with the images determined by the specified first level\ncontrast.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zmap = second_level_model.compute_contrast(\n    first_level_contrast=\"language-string\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The group level :term:`contrast` reveals a left lateralized fronto-temporal\nlanguage network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.plot_glass_brain(\n    zmap,\n    threshold=p001_unc,\n    title=\"Group language network (unc p<0.001)\",\n    plot_abs=False,\n    display_mode=\"x\",\n    figure=plt.figure(figsize=(5, 4)),\n)\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate and save the GLM report at the group level.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report_slm = second_level_model.generate_report(\n    contrasts=\"intercept\",\n    first_level_contrast=\"language-string\",\n    threshold=p001_unc,\n    display_mode=\"x\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "View the GLM report at the group level.\n\n.. include:: ../../../examples/report_note.rst\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report_slm"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}