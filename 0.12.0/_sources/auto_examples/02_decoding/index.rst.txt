

.. _sphx_glr_auto_examples_02_decoding:

=========================================
Decoding and predicting from brain images
=========================================

See :ref:`decoding` for more details.



.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This is a demo for surface-based searchlight decoding, as described in :footciteChen2011.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_searchlight_surface_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_searchlight_surface.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Cortical surface-based searchlight decoding</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Full step-by-step example of fitting a GLM to perform a decoding experiment. In this decoding analysis, we will be doing a one-vs-all classification. We use the data from one subject of the Haxby dataset.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_glm_decoding_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_glm_decoding.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Decoding of a dataset after GLM fit for signal extraction</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example does a simple but efficient decoding on the Haxby dataset: using a feature selection, followed by an SVM.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_anova_svm_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_anova_svm.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Decoding with ANOVA + SVM: face vs house in the Haxby dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example uses fast ensembling of regularized models (FREM) to decode a face vs house vs chair discrimination task from :footciteHaxby2001 study. FREM uses an implicit spatial regularization through fast clustering and aggregates a high number of estimators trained on various splits of the training set, thus returning a very robust decoder at a lower computational cost than other spatially regularized methods.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_frem_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_frem.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Decoding with FREM: face vs house vs chair object recognition</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Here we compare different classifiers on a visual object recognition decoding task.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_different_estimators_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_different_estimators.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Different classifiers in decoding the Haxby dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example partly reproduces the encoding model presented in :footciteMiyawaki2008.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_miyawaki_encoding_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_miyawaki_encoding.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Encoding models for visual stimuli from Miyawaki et al. 2008</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example simulates data according to a very simple sketch of brain imaging data and applies machine learning techniques to predict output values.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_simulated_data_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_simulated_data.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Example of pattern recognition on simulated data</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this example, we use fast ensembling of regularized models (FREM) to solve a regression problem, predicting the gain level corresponding to each Beta maps regressed from mixed gambles experiment. FREM uses an implicit spatial regularization through fast clustering and aggregates a high number of  estimators trained on various splits of the training set, thus returning a very robust decoder at a lower computational cost than other spatially regularized methods.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_mixed_gambles_frem_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_mixed_gambles_frem.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">FREM on Jimura et al "mixed gambles" dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this script we reproduce the data analysis conducted by :footciteHaxby2001.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_full_analysis_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_full_analysis.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">ROI-based decoding analysis in Haxby et al. dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example reproduces the experiment presented in :footciteMiyawaki2008.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_miyawaki_reconstruction_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_miyawaki_reconstruction.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Reconstruction of visual stimuli from Miyawaki et al. 2008</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Searchlight analysis requires fitting a classifier a large amount of times. As a result, it is an intrinsically slow method. In order to speed up computing, in this example, Searchlight is run only on one slice on the fMRI (see the generated figures).">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_searchlight_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_searchlight.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Searchlight analysis of face vs house recognition</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Here we set the number of features selected in an Anova-SVC approach to maximize the cross-validation score.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_grid_search_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_grid_search.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Setting a parameter by cross-validation</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="In this script we plot an overview of the stimuli used in :footciteHaxby2001.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_stimuli_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_stimuli.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Show stimuli of Haxby et al. dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="We compare one vs all and one vs one multi-class strategies: the overall cross-validated accuracy and the confusion matrix.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_multiclass_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_multiclass.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">The haxby dataset: different multi-class strategies</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Nilearn&#x27;s Decoder object is a composite estimator that does several things under the hood and can hence be a bit difficult to understand at first.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_haxby_understand_decoder_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_haxby_understand_decoder.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Understanding Decoder</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example uses Voxel-Based Morphometry (VBM) to study the relationship between aging and gray matter density.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_oasis_vbm_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_oasis_vbm.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Voxel-Based Morphometry on Oasis dataset</div>
    </div>


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Predicting age from gray-matter concentration maps from OASIS dataset. Note that age is a continuous variable, we use the regressor here, and not the classification object.">

.. only:: html

  .. image:: /auto_examples/02_decoding/images/thumb/sphx_glr_plot_oasis_vbm_space_net_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_02_decoding_plot_oasis_vbm_space_net.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Voxel-Based Morphometry on Oasis dataset with Space-Net prior</div>
    </div>


.. thumbnail-parent-div-close

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/02_decoding/plot_haxby_searchlight_surface
   /auto_examples/02_decoding/plot_haxby_glm_decoding
   /auto_examples/02_decoding/plot_haxby_anova_svm
   /auto_examples/02_decoding/plot_haxby_frem
   /auto_examples/02_decoding/plot_haxby_different_estimators
   /auto_examples/02_decoding/plot_miyawaki_encoding
   /auto_examples/02_decoding/plot_simulated_data
   /auto_examples/02_decoding/plot_mixed_gambles_frem
   /auto_examples/02_decoding/plot_haxby_full_analysis
   /auto_examples/02_decoding/plot_miyawaki_reconstruction
   /auto_examples/02_decoding/plot_haxby_searchlight
   /auto_examples/02_decoding/plot_haxby_grid_search
   /auto_examples/02_decoding/plot_haxby_stimuli
   /auto_examples/02_decoding/plot_haxby_multiclass
   /auto_examples/02_decoding/plot_haxby_understand_decoder
   /auto_examples/02_decoding/plot_oasis_vbm
   /auto_examples/02_decoding/plot_oasis_vbm_space_net

