{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example of pattern recognition on simulated data\n\nThis example simulates data according to a very simple sketch of brain\nimaging data and applies machine learning techniques to predict output\nvalues.\n\nWe use a very simple generating function to simulate data,\nas in :footcite:t:`Michel2011`, a linear\nmodel with a random design matrix **X**:\n\n\\begin{align}\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\mathbf{e}\\end{align}\n\n* **w**: the weights of the linear model correspond to the predictive\n  brain regions. Here, in the simulations, they form a 3D image with 5, four\n  of which in opposite corners and one in the middle, as plotted below.\n\n* **X**: the design matrix corresponds to the observed :term:`fMRI` data.\n  Here we simulate random normal variables\n  and smooth them as in Gaussian fields.\n\n* **e** is random normal noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn._utils.helpers import check_matplotlib\n\ncheck_matplotlib()\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nibabel import Nifti1Image\nfrom scipy import linalg\nfrom scipy.ndimage import gaussian_filter\nfrom sklearn import linear_model, svm\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\nimport nilearn.masking\nfrom nilearn import decoding\nfrom nilearn.plotting import show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A function to generate data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def create_simulation_data(snr=0, n_samples=2 * 100, size=12, random_state=1):\n    generator = check_random_state(random_state)\n    roi_size = 2  # size / 3\n    smooth_X = 1\n    # Coefs\n    w = np.zeros((size, size, size))\n    w[0:roi_size, 0:roi_size, 0:roi_size] = -0.6\n    w[-roi_size:, -roi_size:, 0:roi_size] = 0.5\n    w[0:roi_size, -roi_size:, -roi_size:] = -0.6\n    w[-roi_size:, 0:roi_size:, -roi_size:] = 0.5\n    w[\n        (size - roi_size) // 2 : (size + roi_size) // 2,\n        (size - roi_size) // 2 : (size + roi_size) // 2,\n        (size - roi_size) // 2 : (size + roi_size) // 2,\n    ] = 0.5\n    w = w.ravel()\n    # Generate smooth background noise\n    XX = generator.randn(n_samples, size, size, size)\n    noise = []\n    for i in range(n_samples):\n        Xi = gaussian_filter(XX[i, :, :, :], smooth_X)\n        Xi = Xi.ravel()\n        noise.append(Xi)\n    noise = np.array(noise)\n    # Generate the signal y\n    y = generator.randn(n_samples)\n    X = np.dot(y[:, np.newaxis], w[np.newaxis])\n    norm_noise = linalg.norm(X, 2) / np.exp(snr / 20.0)\n    noise_coef = norm_noise / linalg.norm(noise, 2)\n    noise *= noise_coef\n    snr = 20 * np.log(linalg.norm(X, 2) / linalg.norm(noise, 2))\n    print(f\"SNR: {snr:.1f} dB\")\n    # Mixing of signal + noise and splitting into train/test\n    X += noise\n    X -= X.mean(axis=-1)[:, np.newaxis]\n    X /= X.std(axis=-1)[:, np.newaxis]\n    X_test = X[n_samples // 2 :, :]\n    X_train = X[: n_samples // 2, :]\n    y_test = y[n_samples // 2 :]\n    y = y[: n_samples // 2]\n\n    return X_train, X_test, y, y_test, snr, w, size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple function to plot slices\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_slices(data, title=None):\n    plt.figure(figsize=(5.5, 2.2))\n    vmax = np.abs(data).max()\n    for i in (0, 6, 11):\n        plt.subplot(1, 3, i // 5 + 1)\n        plt.imshow(\n            data[:, :, i],\n            vmin=-vmax,\n            vmax=vmax,\n            interpolation=\"nearest\",\n            cmap=\"RdBu_r\",\n        )\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(\n        hspace=0.05, wspace=0.05, left=0.03, right=0.97, top=0.9\n    )\n    if title is not None:\n        plt.suptitle(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test, snr, coefs, size = create_simulation_data(\n    snr=-10, n_samples=100, size=12\n)\n\n# Create masks for SearchLight. process_mask is the voxels where SearchLight\n# computation is performed. It is a subset of the brain mask, just to reduce\n# computation time.\nmask = np.ones((size, size, size), dtype=bool)\nmask_img = Nifti1Image(mask.astype(\"uint8\"), np.eye(4))\nprocess_mask = np.zeros((size, size, size), dtype=bool)\nprocess_mask[:, :, 0] = True\nprocess_mask[:, :, 6] = True\nprocess_mask[:, :, 11] = True\nprocess_mask_img = Nifti1Image(process_mask.astype(\"uint8\"), np.eye(4))\n\ncoefs = np.reshape(coefs, [size, size, size])\nplot_slices(coefs, title=\"Ground truth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run different estimators\n\nWe can now run different estimators and look at their prediction score,\nas well as the feature maps that they recover. Namely, we will use\n\n* A :sklearn:`support vector regression </modules/svm.html>`\n\n* An :sklearn:`elastic-net <modules/linear_model.html#elastic-net>`\n\n* A *Bayesian* ridge estimator, i.e. a ridge estimator that sets its\n  parameter according to a metaprior\n\n* A ridge estimator that set its parameter by cross-validation\n\nNote that the `RidgeCV` and the `ElasticNetCV` have names ending in `CV`\nthat stands for `cross-validation`: in the list of possible `alpha`\nvalues that they are given, they choose the best by cross-validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bayesian_ridge = make_pipeline(StandardScaler(), linear_model.BayesianRidge())\n\nestimators = [\n    (\"bayesian_ridge\", bayesian_ridge),\n    (\n        \"enet_cv\",\n        linear_model.ElasticNetCV(alphas=[5, 1, 0.5, 0.1], l1_ratio=0.05),\n    ),\n    (\"ridge_cv\", linear_model.RidgeCV(alphas=[100, 10, 1, 0.1], cv=5)),\n    (\"svr\", svm.SVR(kernel=\"linear\", C=0.001)),\n    (\n        \"searchlight\",\n        decoding.SearchLight(\n            mask_img,\n            process_mask_img=process_mask_img,\n            radius=2.7,\n            scoring=\"r2\",\n            estimator=svm.SVR(kernel=\"linear\"),\n            cv=KFold(n_splits=4),\n            verbose=1,\n            n_jobs=2,\n        ),\n    ),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the estimators\n\nAs the estimators expose a fairly consistent API, we can all fit them in\na for loop: they all have a `fit` method for fitting the data, a `score`\nmethod to retrieve the prediction score, and because they are all linear\nmodels, a `coef_` attribute that stores the coefficients **w** estimated\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for name, estimator in estimators:\n    t1 = time()\n    if name != \"searchlight\":\n        estimator.fit(X_train, y_train)\n    else:\n        X = nilearn.masking.unmask(X_train, mask_img)\n        estimator.fit(X, y_train)\n        del X\n    elapsed_time = time() - t1\n\n    if name != \"searchlight\":\n        if name == \"bayesian_ridge\":\n            coefs = estimator.named_steps[\"bayesianridge\"].coef_\n        else:\n            coefs = estimator.coef_\n        coefs = np.reshape(coefs, [size, size, size])\n        score = estimator.score(X_test, y_test)\n        title = (\n            f\"{name}: prediction score {score:.3f}, \"\n            f\"training time: {elapsed_time:.2f}s\"\n        )\n\n    else:  # Searchlight\n        coefs = estimator.scores_\n        title = (\n            f\"{estimator.__class__.__name__}: \"\n            f\"training time: {elapsed_time:.2f}s\"\n        )\n\n    # We use the plot_slices function provided in the example to\n    # plot the results\n    plot_slices(coefs, title=title)\n\n    print(title)\n\n_, p_values = f_regression(X_train, y_train)\np_values = np.reshape(p_values, (size, size, size))\np_values = -np.log10(p_values)\np_values[np.isnan(p_values)] = 0\np_values[p_values > 10] = 10\nplot_slices(p_values, title=\"f_regress\")\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An exercise to go further\n\nAs an exercise, you can use recursive feature elimination (RFE) with\nthe SVM\n\nRead the object's documentation to find out how to use RFE.\n\n**Performance tip**: increase the `step` parameter, or it will be very\nslow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_selection import RFE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}