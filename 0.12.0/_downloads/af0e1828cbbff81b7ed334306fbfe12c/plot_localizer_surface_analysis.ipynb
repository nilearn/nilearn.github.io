{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example of surface-based first-level analysis\n\nA full step-by-step example of fitting a :term:`GLM`\nto experimental data sampled on the cortical surface\nand visualizing the results.\n\nMore specifically:\n\n1. A sequence of :term:`fMRI` volumes is loaded.\n2. :term:`fMRI` data are projected onto a reference cortical surface\n   (the FreeSurfer template, fsaverage).\n3. A :term:`GLM` is applied to the dataset\n   (effect/covariance, then contrast estimation).\n\nThe result of the analysis are statistical maps that are defined\non the brain mesh.\nWe display them using Nilearn capabilities.\n\nThe projection of :term:`fMRI` data onto a given brain :term:`mesh` requires\nthat both are initially defined in the same space.\n\n* The functional data should be coregistered to the anatomy\n  from which the mesh was obtained.\n\n* Another possibility, used here, is to project\n  the normalized :term:`fMRI` data to an :term:`MNI`-coregistered mesh,\n  such as fsaverage.\n\nThe advantage of this second approach is that it makes it easy to run\nsecond-level analyses on the surface.\nOn the other hand, it is obviously less accurate\nthan using a subject-tailored mesh.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data and analysis parameters\n\nPrepare the timing parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_r = 2.4\nslice_time_ref = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetch the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_localizer_first_level\n\ndata = fetch_localizer_first_level()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project the :term:`fMRI` image to the surface\n\nFor this we need to get a :term:`mesh`\nrepresenting the geometry of the surface.\nWe could use an individual :term:`mesh`,\nbut we first resort to a standard :term:`mesh`,\nthe so-called fsaverage5 template from the FreeSurfer software.\n\nWe use the :class:`~nilearn.surface.SurfaceImage`\nto create an surface object instance\nthat contains both the mesh\n(here we use the one from the fsaverage5 templates)\nand the BOLD data that we project on the surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import load_fsaverage\nfrom nilearn.surface import SurfaceImage\n\nfsaverage5 = load_fsaverage()\nsurface_image = SurfaceImage.from_volume(\n    mesh=fsaverage5[\"pial\"],\n    volume_img=data.epi_img,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform first level analysis\n\nWe can now simply run a GLM by directly passing\nour :class:`~nilearn.surface.SurfaceImage` instance\nas input to FirstLevelModel.fit\n\nHere we use an :term:`HRF` model\ncontaining the Glover model and its time derivative\nThe drift model is implicitly a cosine basis with a period cutoff at 128s.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import FirstLevelModel\n\nglm = FirstLevelModel(\n    t_r=t_r,\n    slice_time_ref=slice_time_ref,\n    hrf_model=\"glover + derivative\",\n    minimize_memory=False,\n).fit(run_imgs=surface_image, events=data.events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate contrasts\n\nSpecify the contrasts.\n\nFor practical purpose, we first generate an identity matrix whose size is\nthe number of columns of the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndesign_matrix = glm.design_matrices_[0]\ncontrast_matrix = np.eye(design_matrix.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At first, we create basic contrasts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts = {\n    column: contrast_matrix[i]\n    for i, column in enumerate(design_matrix.columns)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add some intermediate contrasts and\none :term:`contrast` adding all conditions with some auditory parts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts[\"audio\"] = (\n    basic_contrasts[\"audio_left_hand_button_press\"]\n    + basic_contrasts[\"audio_right_hand_button_press\"]\n    + basic_contrasts[\"audio_computation\"]\n    + basic_contrasts[\"sentence_listening\"]\n)\n\n# one contrast adding all conditions involving instructions reading\nbasic_contrasts[\"visual\"] = (\n    basic_contrasts[\"visual_left_hand_button_press\"]\n    + basic_contrasts[\"visual_right_hand_button_press\"]\n    + basic_contrasts[\"visual_computation\"]\n    + basic_contrasts[\"sentence_reading\"]\n)\n\n# one contrast adding all conditions involving computation\nbasic_contrasts[\"computation\"] = (\n    basic_contrasts[\"visual_computation\"]\n    + basic_contrasts[\"audio_computation\"]\n)\n\n# one contrast adding all conditions involving sentences\nbasic_contrasts[\"sentences\"] = (\n    basic_contrasts[\"sentence_listening\"] + basic_contrasts[\"sentence_reading\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create a dictionary of more relevant contrasts\n\n* ``'left - right button press'``: probes motor activity\n  in left versus right button presses.\n* ``'audio - visual'``: probes the difference of activity between listening\n  to some content or reading the same type of content\n  (instructions, stories).\n* ``'computation - sentences'``: looks at the activity\n  when performing a mental computation task  versus simply reading sentences.\n\nOf course, we could define other contrasts,\nbut we keep only 3 for simplicity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrasts = {\n    \"(left - right) button press\": (\n        basic_contrasts[\"audio_left_hand_button_press\"]\n        - basic_contrasts[\"audio_right_hand_button_press\"]\n        + basic_contrasts[\"visual_left_hand_button_press\"]\n        - basic_contrasts[\"visual_right_hand_button_press\"]\n    ),\n    \"audio - visual\": basic_contrasts[\"audio\"] - basic_contrasts[\"visual\"],\n    \"computation - sentences\": (\n        basic_contrasts[\"computation\"] - basic_contrasts[\"sentences\"]\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's estimate the contrasts by iterating over them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import load_fsaverage_data\nfrom nilearn.plotting import plot_surf_stat_map, show\n\n#  let's make sure we use the same threshold\nthreshold = 3.0\n\nfsaverage_data = load_fsaverage_data(data_type=\"sulcal\")\n\nfor contrast_id, contrast_val in contrasts.items():\n    # compute contrast-related statistics\n    z_score = glm.compute_contrast(contrast_val, stat_type=\"t\")\n\n    hemi = \"left\"\n    if contrast_id == \"(left - right) button press\":\n        hemi = \"both\"\n\n    # we plot it on the surface, on the inflated fsaverage mesh,\n    # together with a suitable background to give an impression\n    # of the cortex folding.\n    plot_surf_stat_map(\n        surf_mesh=fsaverage5[\"inflated\"],\n        stat_map=z_score,\n        hemi=hemi,\n        title=contrast_id,\n        threshold=threshold,\n        bg_map=fsaverage_data,\n        darkness=None,\n    )\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or we can save as an html file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nfrom nilearn.interfaces.bids import save_glm_to_bids\n\noutput_dir = Path.cwd() / \"results\" / \"plot_localizer_surface_analysis\"\noutput_dir.mkdir(exist_ok=True, parents=True)\n\nsave_glm_to_bids(\n    glm,\n    contrasts=contrasts,\n    threshold=threshold,\n    bg_img=load_fsaverage_data(data_type=\"sulcal\", mesh_type=\"inflated\"),\n    height_control=None,\n    prefix=\"sub-01\",\n    out_dir=output_dir,\n)\n\nreport = glm.generate_report(\n    contrasts,\n    threshold=threshold,\n    bg_img=load_fsaverage_data(data_type=\"sulcal\", mesh_type=\"inflated\"),\n    height_control=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This report can be viewed in a notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or in a separate browser window\nreport.open_in_browser()\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report.save_as_html(output_dir / \"report.html\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}