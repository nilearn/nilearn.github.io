<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.6.2. nilearn.input_data.MultiNiftiMasker"href=nilearn.input_data.MultiNiftiMasker.html rel=next><link title="8.5.20. nilearn.image.threshold_img"href=nilearn.image.threshold_img.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.6.2. nilearn.input_data.MultiNiftiMasker"accesskey=N href=nilearn.input_data.MultiNiftiMasker.html>next</a> |</li><li class=right><a title="8.5.20. nilearn.image.threshold_img"accesskey=P href=nilearn.image.threshold_img.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the class signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-input-data-niftimasker><h1><span class=section-number>8.6.1. </span>nilearn.input_data.NiftiMasker<a title="Permalink to this headline"class=headerlink href=#nilearn-input-data-niftimasker>¶</a></h1><dl class="py class"><dt id=nilearn.input_data.NiftiMasker><em class=property>class </em><code class="sig-prename descclassname">nilearn.input_data.</code><code class="sig-name descname">NiftiMasker</code><span class=sig-paren>(</span><em class=sig-param><span class=n>mask_img</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>sessions</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>smoothing_fwhm</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>standardize</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>standardize_confounds</span><span class=o>=</span><span class=default_value>True</span></em>, <em class=sig-param><span class=n>detrend</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>high_variance_confounds</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>low_pass</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>high_pass</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>t_r</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>target_affine</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>target_shape</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>mask_strategy</span><span class=o>=</span><span class=default_value>'background'</span></em>, <em class=sig-param><span class=n>mask_args</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>sample_mask</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>dtype</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>memory_level</span><span class=o>=</span><span class=default_value>1</span></em>, <em class=sig-param><span class=n>memory</span><span class=o>=</span><span class=default_value>Memory(location=None)</span></em>, <em class=sig-param><span class=n>verbose</span><span class=o>=</span><span class=default_value>0</span></em>, <em class=sig-param><span class=n>reports</span><span class=o>=</span><span class=default_value>True</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker>¶</a></dt><dd><p>Applying a mask to extract time-series from Niimg-like objects.</p> <p>NiftiMasker is useful when preprocessing (detrending, standardization, resampling, etc.) of in-mask voxels is necessary. Use case: working with time series of resting-state or task maps.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>mask_img</strong><span class=classifier>Niimg-like object, optional</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Mask for the data. If not given, a mask is computed in the fit step. Optional parameters (mask_args and mask_strategy) can be set to fine tune the mask extraction. If the mask and the images have different resolutions, the images are resampled to the mask resolution. If target_shape and/or target_affine are provided, the mask is resampled first. After this, the images are resampled to the resampled mask.</p></dd><dt><strong>sessions</strong><span class=classifier>numpy array, optional</span></dt><dd><p>Add a session level to the preprocessing. Each session will be detrended independently. Must be a 1D array of n_samples elements.</p></dd><dt><strong>smoothing_fwhm</strong><span class=classifier>float, optional</span></dt><dd><p>If smoothing_fwhm is not None, it gives the full-width half maximum in millimeters of the spatial smoothing to apply to the signal.</p></dd><dt><strong>standardize</strong><span class=classifier>{False, True, ‘zscore’, ‘psc’}, optional</span></dt><dd><p>Strategy to standardize the signal. ‘zscore’: the signal is z-scored. Timeseries are shifted to zero mean and scaled to unit variance. ‘psc’: Timeseries are shifted to zero mean value and scaled to percent signal change (as compared to original mean signal). True : the signal is z-scored. Timeseries are shifted to zero mean and scaled to unit variance. False : Do not standardize the data. Default=False.</p></dd><dt><strong>standardize_confounds</strong><span class=classifier>boolean, optional</span></dt><dd><p>If standardize_confounds is True, the confounds are z-scored: their mean is put to 0 and their variance to 1 in the time dimension. Default=True.</p></dd><dt><strong>high_variance_confounds</strong><span class=classifier>boolean, optional</span></dt><dd><p>If True, high variance confounds are computed on provided image with <a class="reference internal"href=nilearn.image.high_variance_confounds.html#nilearn.image.high_variance_confounds title=nilearn.image.high_variance_confounds><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.image.high_variance_confounds</span></code></a> and default parameters and regressed out. Default=False.</p></dd><dt><strong>detrend</strong><span class=classifier>boolean, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details: <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>. Default=False.</p></dd><dt><strong>low_pass</strong><span class=classifier>None or float, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details: <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>.</p></dd><dt><strong>high_pass</strong><span class=classifier>None or float, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details: <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>.</p></dd><dt><strong>t_r</strong><span class=classifier>float, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details: <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>.</p></dd><dt><strong>target_affine</strong><span class=classifier>3x3 or 4x4 matrix, optional</span></dt><dd><p>This parameter is passed to image.resample_img. Please see the related documentation for details.</p></dd><dt><strong>target_shape</strong><span class=classifier>3-tuple of integers, optional</span></dt><dd><p>This parameter is passed to image.resample_img. Please see the related documentation for details.</p></dd><dt><strong>mask_strategy</strong><span class=classifier>{‘background’, ‘epi’ or ‘template’}, optional</span></dt><dd><p>The strategy used to compute the mask: use ‘background’ if your images present a clear homogeneous background, ‘epi’ if they are raw EPI images, or you could use ‘template’ which will extract the gray matter part of your data by resampling the MNI152 brain mask for your data’s field of view. Depending on this value, the mask will be computed from masking.compute_background_mask, masking.compute_epi_mask or masking.compute_brain_mask. Default=’background’.</p></dd><dt><strong>mask_args</strong><span class=classifier>dict, optional</span></dt><dd><p>If mask is None, these are additional parameters passed to masking.compute_background_mask or masking.compute_epi_mask to fine-tune mask computation. Please see the related documentation for details.</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>Masks the niimgs along time/fourth dimension. This complements 3D masking by the mask_img argument. This masking step is applied before data preprocessing at the beginning of NiftiMasker.transform. This is useful to perform data subselection as part of a scikit-learn pipeline.</p></dd><dt><strong>dtype</strong><span class=classifier>{dtype, “auto”}, optional</span></dt><dd><p>Data type toward which the data should be converted. If “auto”, the data will be converted to int32 if dtype is discrete and float32 if it is continuous.</p></dd><dt><strong>memory</strong><span class=classifier>instance of joblib.Memory or string, optional</span></dt><dd><p>Used to cache the masking process. By default, no caching is done. If a string is given, it is the path to the caching directory.</p></dd><dt><strong>memory_level</strong><span class=classifier>integer, optional</span></dt><dd><p>Rough estimator of the amount of memory used by caching. Higher value means more memory for caching. Default=1.</p></dd><dt><strong>verbose</strong><span class=classifier>integer, optional</span></dt><dd><p>Indicate the level of verbosity. By default, nothing is printed. Default=0.</p></dd><dt><strong>reports</strong><span class=classifier>boolean, optional</span></dt><dd><p>If set to True, data is saved in order to produce a report. Default=True.</p></dd></dl></dd></dl> <div class="admonition seealso"><p class=admonition-title>See also</p><dl class=simple><dt><a class="reference internal"href=nilearn.masking.compute_background_mask.html#nilearn.masking.compute_background_mask title=nilearn.masking.compute_background_mask><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.masking.compute_background_mask</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.masking.compute_epi_mask.html#nilearn.masking.compute_epi_mask title=nilearn.masking.compute_epi_mask><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.masking.compute_epi_mask</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.image.resample_img.html#nilearn.image.resample_img title=nilearn.image.resample_img><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.image.resample_img</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.image.high_variance_confounds.html#nilearn.image.high_variance_confounds title=nilearn.image.high_variance_confounds><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.image.high_variance_confounds</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.masking.apply_mask.html#nilearn.masking.apply_mask title=nilearn.masking.apply_mask><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.masking.apply_mask</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a></dt><dd></dd></dl></div> <dl class="field-list simple"><dt class=field-odd>Attributes</dt><dd class=field-odd><dl class=simple><dt><strong>`mask_img_`</strong><span class=classifier>nibabel.Nifti1Image</span></dt><dd><p>The mask of the data, or the computed one.</p></dd><dt><strong>`affine_`</strong><span class=classifier>4x4 numpy array</span></dt><dd><p>Affine of the transformed image.</p></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.__init__><code class="sig-name descname">__init__</code><span class=sig-paren>(</span><em class=sig-param><span class=n>mask_img</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>sessions</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>smoothing_fwhm</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>standardize</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>standardize_confounds</span><span class=o>=</span><span class=default_value>True</span></em>, <em class=sig-param><span class=n>detrend</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>high_variance_confounds</span><span class=o>=</span><span class=default_value>False</span></em>, <em class=sig-param><span class=n>low_pass</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>high_pass</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>t_r</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>target_affine</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>target_shape</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>mask_strategy</span><span class=o>=</span><span class=default_value>'background'</span></em>, <em class=sig-param><span class=n>mask_args</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>sample_mask</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>dtype</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>memory_level</span><span class=o>=</span><span class=default_value>1</span></em>, <em class=sig-param><span class=n>memory</span><span class=o>=</span><span class=default_value>Memory(location=None)</span></em>, <em class=sig-param><span class=n>verbose</span><span class=o>=</span><span class=default_value>0</span></em>, <em class=sig-param><span class=n>reports</span><span class=o>=</span><span class=default_value>True</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.__init__>¶</a></dt><dd><p>Initialize self. See help(type(self)) for accurate signature.</p></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.generate_report><code class="sig-name descname">generate_report</code><span class=sig-paren>(</span><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.generate_report>¶</a></dt><dd></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.fit><code class="sig-name descname">fit</code><span class=sig-paren>(</span><em class=sig-param><span class=n>imgs</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>y</span><span class=o>=</span><span class=default_value>None</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.fit>¶</a></dt><dd><p>Compute the mask corresponding to the data</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>imgs</strong><span class=classifier>list of Niimg-like objects</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Data on which the mask must be calculated. If this is a list, the affine is considered the same for all.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.transform_single_imgs><code class="sig-name descname">transform_single_imgs</code><span class=sig-paren>(</span><em class=sig-param><span class=n>imgs</span></em>, <em class=sig-param><span class=n>confounds</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>copy</span><span class=o>=</span><span class=default_value>True</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.transform_single_imgs>¶</a></dt><dd><p>Apply mask, spatial and temporal preprocessing</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>imgs</strong><span class=classifier>3D/4D Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Images to process. It must boil down to a 4D image with scans number as last dimension.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like or pandas DataFrame, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details: <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>. shape: (number of scans, number of confounds)</p></dd><dt><strong>copy</strong><span class=classifier>Boolean, optional</span></dt><dd><p>Indicates whether a copy is returned or not. Default=True.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>region_signals</strong><span class=classifier>2D numpy.ndarray</span></dt><dd><p>Signal for each voxel inside the mask. shape: (number of scans, number of voxels)</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.fit_transform><code class="sig-name descname">fit_transform</code><span class=sig-paren>(</span><em class=sig-param><span class=n>X</span></em>, <em class=sig-param><span class=n>y</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>confounds</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=o>**</span><span class=n>fit_params</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.fit_transform>¶</a></dt><dd><p>Fit to data, then transform it</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>X</strong><span class=classifier>Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></dd><dt><strong>y</strong><span class=classifier>numpy array of shape [n_samples], optional</span></dt><dd><p>Target values.</p></dd><dt><strong>confounds</strong><span class=classifier>list of confounds, optional</span></dt><dd><p>List of confounds (2D arrays or filenames pointing to CSV files). Must be of same length than imgs_list.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>X_new</strong><span class=classifier>numpy array of shape [n_samples, n_features_new]</span></dt><dd><p>Transformed array.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.get_params><code class="sig-name descname">get_params</code><span class=sig-paren>(</span><em class=sig-param><span class=n>deep</span><span class=o>=</span><span class=default_value>True</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.get_params>¶</a></dt><dd><p>Get parameters for this estimator.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>deep</strong><span class=classifier>bool, default=True</span></dt><dd><p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>params</strong><span class=classifier>dict</span></dt><dd><p>Parameter names mapped to their values.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.inverse_transform><code class="sig-name descname">inverse_transform</code><span class=sig-paren>(</span><em class=sig-param><span class=n>X</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.inverse_transform>¶</a></dt><dd><p>Transform the 2D data matrix back to an image in brain space.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>X</strong><span class=classifier>Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>img</strong><span class=classifier>Transformed image in brain space.</span></dt><dd></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.set_params><code class="sig-name descname">set_params</code><span class=sig-paren>(</span><em class=sig-param><span class=o>**</span><span class=n>params</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.set_params>¶</a></dt><dd><p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference external"title="(in scikit-learn v0.24)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>). The latter have parameters of the form <code class="docutils literal notranslate"><span class=pre>&LTcomponent>__&LTparameter></span></code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>**params</strong><span class=classifier>dict</span></dt><dd><p>Estimator parameters.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>estimator instance</span></dt><dd><p>Estimator instance.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt id=nilearn.input_data.NiftiMasker.transform><code class="sig-name descname">transform</code><span class=sig-paren>(</span><em class=sig-param><span class=n>imgs</span></em>, <em class=sig-param><span class=n>confounds</span><span class=o>=</span><span class=default_value>None</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.input_data.NiftiMasker.transform>¶</a></dt><dd><p>Apply mask, spatial and temporal preprocessing</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>imgs</strong><span class=classifier>3D/4D Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Images to process. It must boil down to a 4D image with scans number as last dimension.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details. shape: (number of scans, number of confounds)</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>region_signals</strong><span class=classifier>2D numpy.ndarray</span></dt><dd><p>Signal for each element. shape: (number of scans, number of elements)</p></dd></dl></dd></dl></dd></dl></dd></dl><div class=section id=examples-using-nilearn-input-data-niftimasker><h2><span class=section-number>8.6.1.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.input_data.NiftiMasker</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-input-data-niftimasker>¶</a></h2><div tooltip="We compare one vs all and one vs one multi-class strategies: the overall cross-validated accura..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id1><img alt="The haxby dataset: different multi-class strategies"src=../../_images/sphx_glr_plot_haxby_multiclass_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_multiclass.html#sphx-glr-auto-examples-02-decoding-plot-haxby-multiclass-py><span class="std std-ref">The haxby dataset: different multi-class strategies</span></a></span><a title="Permalink to this image"class=headerlink href=#id1>¶</a></p></div></div><div tooltip="Searchlight analysis requires fitting a classifier a large amount of times. As a result, it is ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id2><img alt="Searchlight analysis of face vs house recognition"src=../../_images/sphx_glr_plot_haxby_searchlight_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight.html#sphx-glr-auto-examples-02-decoding-plot-haxby-searchlight-py><span class="std std-ref">Searchlight analysis of face vs house recognition</span></a></span><a title="Permalink to this image"class=headerlink href=#id2>¶</a></p></div></div><div tooltip='In this script we reproduce the data analysis conducted by Haxby et al. in "Distributed and Ove...'class=sphx-glr-thumbcontainer><div class="figure align-default"id=id3><img alt="ROI-based decoding analysis in Haxby et al. dataset"src=../../_images/sphx_glr_plot_haxby_full_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_full_analysis.html#sphx-glr-auto-examples-02-decoding-plot-haxby-full-analysis-py><span class="std std-ref">ROI-based decoding analysis in Haxby et al. dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id3>¶</a></p></div></div><div tooltip="This example uses Voxel-Based Morphometry (VBM) to study the relationship between aging and gra..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id4><img alt="Voxel-Based Morphometry on Oasis dataset"src=../../_images/sphx_glr_plot_oasis_vbm_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_oasis_vbm.html#sphx-glr-auto-examples-02-decoding-plot-oasis-vbm-py><span class="std std-ref">Voxel-Based Morphometry on Oasis dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id4>¶</a></p></div></div><div tooltip="This example partly reproduces the encoding model presented in     `Visual image reconstruction..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id5><img alt="Encoding models for visual stimuli from Miyawaki et al. 2008"src=../../_images/sphx_glr_plot_miyawaki_encoding_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_encoding.html#sphx-glr-auto-examples-02-decoding-plot-miyawaki-encoding-py><span class="std std-ref">Encoding models for visual stimuli from Miyawaki et al. 2008</span></a></span><a title="Permalink to this image"class=headerlink href=#id5>¶</a></p></div></div><div tooltip="This example reproduces the experiment presented in     `Visual image reconstruction from human..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id6><img alt="Reconstruction of visual stimuli from Miyawaki et al. 2008"src=../../_images/sphx_glr_plot_miyawaki_reconstruction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_reconstruction.html#sphx-glr-auto-examples-02-decoding-plot-miyawaki-reconstruction-py><span class="std std-ref">Reconstruction of visual stimuli from Miyawaki et al. 2008</span></a></span><a title="Permalink to this image"class=headerlink href=#id6>¶</a></p></div></div><div tooltip="This example shows how to produce seed-to-voxel correlation maps for a single subject based on ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id7><img alt="Producing single subject maps of seed-to-voxel correlation"src=../../_images/sphx_glr_plot_seed_to_voxel_correlation_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_seed_to_voxel_correlation.html#sphx-glr-auto-examples-03-connectivity-plot-seed-to-voxel-correlation-py><span class="std std-ref">Producing single subject maps of seed-to-voxel correlation</span></a></span><a title="Permalink to this image"class=headerlink href=#id7>¶</a></p></div></div><div tooltip=" Full step-by-step example of fitting a GLM to perform a first level analysis in an openneuro B..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id8><img alt="First level analysis of a complete BIDS dataset from openneuro"src=../../_images/sphx_glr_plot_bids_features_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_bids_features.html#sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py><span class="std std-ref">First level analysis of a complete BIDS dataset from openneuro</span></a></span><a title="Permalink to this image"class=headerlink href=#id8>¶</a></p></div></div><div tooltip="Here is a simple example of automatic mask computation using the nifti masker. The mask is comp..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id9><img alt="Simple example of NiftiMasker use"src=../../_images/sphx_glr_plot_nifti_simple_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_nifti_simple.html#sphx-glr-auto-examples-06-manipulating-images-plot-nifti-simple-py><span class="std std-ref">Simple example of NiftiMasker use</span></a></span><a title="Permalink to this image"class=headerlink href=#id9>¶</a></p></div></div><div tooltip="In this example, the Nifti masker is used to automatically compute a mask."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id10><img alt="Understanding NiftiMasker and mask computation"src=../../_images/sphx_glr_plot_mask_computation_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_mask_computation.html#sphx-glr-auto-examples-06-manipulating-images-plot-mask-computation-py><span class="std std-ref">Understanding NiftiMasker and mask computation</span></a></span><a title="Permalink to this image"class=headerlink href=#id10>¶</a></p></div></div><div tooltip=" This example is meant to demonstrate nilearn as a low-level tools used to combine feature extr..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id11><img alt="Multivariate decompositions: Independent component analysis of fMRI"src=../../_images/sphx_glr_plot_ica_resting_state_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_resting_state.html#sphx-glr-auto-examples-07-advanced-plot-ica-resting-state-py><span class="std std-ref">Multivariate decompositions: Independent component analysis of fMRI</span></a></span><a title="Permalink to this image"class=headerlink href=#id11>¶</a></p></div></div><div tooltip="This example shows how to use the Localizer dataset in a basic analysis. A standard Anova is pe..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id12><img alt="Massively univariate analysis of a calculation task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_simple_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_simple_analysis.html#sphx-glr-auto-examples-07-advanced-plot-localizer-simple-analysis-py><span class="std std-ref">Massively univariate analysis of a calculation task from the Localizer dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id12>¶</a></p></div></div><div tooltip="This example shows the results obtained in a massively univariate analysis performed at the int..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id13><img alt="Massively univariate analysis of a motor task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_mass_univariate_methods_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html#sphx-glr-auto-examples-07-advanced-plot-localizer-mass-univariate-methods-py><span class="std std-ref">Massively univariate analysis of a motor task from the Localizer dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id13>¶</a></p></div></div><div tooltip="This example shows how to download statistical maps from NeuroVault, label them with NeuroSynth..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id14><img alt="NeuroVault cross-study ICA maps."src=../../_images/sphx_glr_plot_ica_neurovault_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_neurovault.html#sphx-glr-auto-examples-07-advanced-plot-ica-neurovault-py><span class="std std-ref">NeuroVault cross-study ICA maps.</span></a></span><a title="Permalink to this image"class=headerlink href=#id14>¶</a></p></div></div><div tooltip="A permuted Ordinary Least Squares algorithm is run at each voxel in order to detemine whether o..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id15><img alt="Massively univariate analysis of face vs house recognition"src=../../_images/sphx_glr_plot_haxby_mass_univariate_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_haxby_mass_univariate.html#sphx-glr-auto-examples-07-advanced-plot-haxby-mass-univariate-py><span class="std std-ref">Massively univariate analysis of face vs house recognition</span></a></span><a title="Permalink to this image"class=headerlink href=#id15>¶</a></p></div></div><div tooltip="This tutorial opens the box of decoding pipelines to bridge integrated functionalities provided..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id16><img alt="Advanced decoding using scikit learn"src=../../_images/sphx_glr_plot_advanced_decoding_scikit_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_advanced_decoding_scikit.html#sphx-glr-auto-examples-07-advanced-plot-advanced-decoding-scikit-py><span class="std std-ref">Advanced decoding using scikit learn</span></a></span><a title="Permalink to this image"class=headerlink href=#id16>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.6.1. nilearn.input_data.NiftiMasker</a><ul><li><a class="reference internal"href=#examples-using-nilearn-input-data-niftimasker>8.6.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.input_data.NiftiMasker</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.image.threshold_img.html><span class=section-number>8.5.20. </span>nilearn.image.threshold_img</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.input_data.MultiNiftiMasker.html><span class=section-number>8.6.2. </span>nilearn.input_data.MultiNiftiMasker</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2020. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 3.4.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.input_data.NiftiMasker.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>