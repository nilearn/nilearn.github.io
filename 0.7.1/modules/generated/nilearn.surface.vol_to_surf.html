<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/gallery.css rel=stylesheet><link href=../../_static/gallery-binder.css rel=stylesheet><link href=../../_static/gallery-dataframe.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="9. Nilearn usage examples"href=../../auto_examples/index.html rel=next><link title="8.14.2. nilearn.surface.load_surf_mesh"href=nilearn.surface.load_surf_mesh.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="9. Nilearn usage examples"accesskey=N href=../../auto_examples/index.html>next</a> |</li><li class=right><a title="8.14.2. nilearn.surface.load_surf_mesh"accesskey=P href=nilearn.surface.load_surf_mesh.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-surface-vol-to-surf><h1><span class=section-number>8.14.3. </span>nilearn.surface.vol_to_surf<a title="Permalink to this headline"class=headerlink href=#nilearn-surface-vol-to-surf>¶</a></h1><dl class="py function"><dt id=nilearn.surface.vol_to_surf><code class="sig-prename descclassname">nilearn.surface.</code><code class="sig-name descname">vol_to_surf</code><span class=sig-paren>(</span><em class=sig-param><span class=n>img</span></em>, <em class=sig-param><span class=n>surf_mesh</span></em>, <em class=sig-param><span class=n>radius</span><span class=o>=</span><span class=default_value>3.0</span></em>, <em class=sig-param><span class=n>interpolation</span><span class=o>=</span><span class=default_value>'linear'</span></em>, <em class=sig-param><span class=n>kind</span><span class=o>=</span><span class=default_value>'auto'</span></em>, <em class=sig-param><span class=n>n_samples</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>mask_img</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>inner_mesh</span><span class=o>=</span><span class=default_value>None</span></em>, <em class=sig-param><span class=n>depth</span><span class=o>=</span><span class=default_value>None</span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.surface.vol_to_surf>¶</a></dt><dd><p>Extract surface data from a Nifti image.</p> <div class=versionadded><p><span class="versionmodified added">New in version 0.4.0.</span></p></div> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>img</strong><span class=classifier>Niimg-like object, 3d or 4d.</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></dd><dt><strong>surf_mesh</strong><span class=classifier>str or numpy.ndarray or Mesh</span></dt><dd><p>Either a file containing surface mesh geometry (valid formats are .gii or Freesurfer specific files such as .orig, .pial, .sphere, .white, .inflated) or two Numpy arrays organized in a list, tuple or a namedtuple with the fields “coordinates” and “faces”, or a Mesh object with “coordinates” and “faces” attributes.</p></dd><dt><strong>radius</strong><span class=classifier>float, optional</span></dt><dd><p>The size (in mm) of the neighbourhood from which samples are drawn around each node. Ignored if <cite>inner_mesh</cite> is provided. Default=3.0.</p></dd><dt><strong>interpolation</strong><span class=classifier>{‘linear’, ‘nearest’}, optional</span></dt><dd><p>How the image intensity is measured at a sample point. Default=’linear’.</p> <ul class=simple><li><dl class=simple><dt>‘linear’:</dt><dd><p>Use a trilinear interpolation of neighboring voxels.</p></dd></dl></li><li><dl class=simple><dt>‘nearest’:</dt><dd><p>Use the intensity of the nearest voxel.</p></dd></dl></li></ul> <p>For one image, the speed difference is small, ‘linear’ takes about x1.5 more time. For many images, ‘nearest’ scales much better, up to x20 faster.</p></dd><dt><strong>kind</strong><span class=classifier>{‘auto’, ‘depth’, ‘line’, ‘ball’}, optional</span></dt><dd><p>The strategy used to sample image intensities around each vertex. Default=’auto’.</p> <ul class=simple><li><dl class=simple><dt>‘auto’:</dt><dd><p>Chooses ‘depth’ if <cite>inner_mesh</cite> is provided and ‘line’ otherwise.</p></dd></dl></li><li><dl class=simple><dt>‘depth’:</dt><dd><p><cite>inner_mesh</cite> must be a mesh whose nodes correspond to those in <cite>surf_mesh</cite>. For example, <cite>inner_mesh</cite> could be a white matter surface mesh and <cite>surf_mesh</cite> a pial surface mesh. Samples are placed between each pair of corresponding nodes at the specified cortical depths (regularly spaced by default, see <cite>depth</cite> parameter).</p></dd></dl></li><li><dl class=simple><dt>‘line’:</dt><dd><p>Samples are placed along the normal to the mesh, at the positions specified by <cite>depth</cite>, or by default regularly spaced over the interval [- <cite>radius</cite>, + <cite>radius</cite>].</p></dd></dl></li><li><dl class=simple><dt>‘ball’:</dt><dd><p>Samples are regularly spaced inside a ball centered at the mesh vertex.</p></dd></dl></li></ul></dd><dt><strong>n_samples</strong><span class=classifier>int or None, optional</span></dt><dd><p>How many samples are drawn around each vertex and averaged. If <code class="docutils literal notranslate"><span class=pre>None</span></code>, use a reasonable default for the chosen sampling strategy (20 for ‘ball’ or 10 for ‘line’). For performance reasons, if using <cite>kind</cite> =”ball”, choose <cite>n_samples</cite> in [10, 20, 40, 80, 160] (default is 20), because cached positions are available.</p></dd><dt><strong>mask_img</strong><span class=classifier>Niimg-like object or None, optional</span></dt><dd><p>Samples falling out of this mask or out of the image are ignored. If <code class="docutils literal notranslate"><span class=pre>None</span></code>, don’t apply any mask.</p></dd><dt><strong>inner_mesh</strong><span class=classifier>str or numpy.ndarray, optional</span></dt><dd><p>Either a file containing a surface mesh or a pair of ndarrays (coordinates, triangles). If provided this is an inner surface that is nested inside the one represented by <cite>surf_mesh</cite> – e.g. <cite>surf_mesh</cite> is a pial surface and <cite>inner_mesh</cite> a white matter surface. In this case nodes in both meshes must correspond: node i in <cite>surf_mesh</cite> is just across the gray matter thickness from node i in <cite>inner_mesh</cite>. Image values for index i are then sampled along the line joining these two points (if <cite>kind</cite> is ‘auto’ or ‘depth’).</p></dd><dt><strong>depth</strong><span class=classifier>sequence of floats or None, optional</span></dt><dd><p>The cortical depth of samples. If provided, n_samples is ignored. When <cite>inner_mesh</cite> is provided, each element of <cite>depth</cite> is a fraction of the distance from <cite>mesh</cite> to <cite>inner_mesh</cite>: 0 is exactly on the outer surface, .5 is halfway, 1. is exactly on the inner surface. <cite>depth</cite> entries can be negative or greater than 1. When <cite>inner_mesh</cite> is not provided and <cite>kind</cite> is “line”, each element of <cite>depth</cite> is a fraction of <cite>radius</cite> along the inwards normal at each mesh node. For example if <cite>radius==1</cite> and <cite>depth==[-.5, 0.]</cite>, for each node values will be sampled .5 mm outside of the surface and exactly at the node position. This parameter is not supported for the “ball” strategy so passing <cite>depth</cite> when <cite>kind==”ball”</cite> results in a <cite>ValueError</cite>.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>texture</strong><span class=classifier>numpy.ndarray, 1d or 2d.</span></dt><dd><p>If 3D image is provided, a 1d vector is returned, containing one value for each mesh node. If 4D image is provided, a 2d array is returned, where each row corresponds to a mesh node.</p></dd></dl></dd></dl> <div class="admonition warning"><p class=admonition-title>Warning</p><p>This function is experimental and details such as the interpolation method are subject to change.</p></div> <p class=rubric>Notes</p> <p>This function computes a value for each vertex of the mesh. In order to do so, it selects a few points in the volume surrounding that vertex, interpolates the image intensities at these sampling positions, and averages the results.</p> <p>Three strategies are available to select these positions.</p> <blockquote><div><ul class=simple><li><p>with ‘depth’, data is sampled at various cortical depths between corresponding nodes of <cite>surface_mesh</cite> and <cite>inner_mesh</cite> (which can be, for example, a pial surface and a white matter surface).</p></li><li><p>‘ball’ uses points regularly spaced in a ball centered at the mesh vertex. The radius of the ball is controlled by the parameter <cite>radius</cite>.</p></li><li><p>‘line’ starts by drawing the normal to the mesh passing through this vertex. It then selects a segment of this normal, centered at the vertex, of length 2 * <cite>radius</cite>. Image intensities are measured at points regularly spaced on this normal segment, or at positions determined by <cite>depth</cite>.</p></li><li><p>(‘auto’ chooses ‘depth’ if <cite>inner_mesh</cite> is provided and ‘line’ otherwise)</p></li></ul></div></blockquote> <p>You can control how many samples are drawn by setting <cite>n_samples</cite>, or their position by setting <cite>depth</cite>.</p> <p>Once the sampling positions are chosen, those that fall outside of the 3d image (or outside of the mask if you provided one) are discarded. If all sample positions are discarded (which can happen, for example, if the vertex itself is outside of the support of the image), the projection at this vertex will be <code class="docutils literal notranslate"><span class=pre>numpy.nan</span></code>.</p> <p>The 3d image then needs to be interpolated at each of the remaining points. Two options are available: ‘nearest’ selects the value of the nearest voxel, and ‘linear’ performs trilinear interpolation of neighbouring voxels. ‘linear’ may give better results - for example, the projected values are more stable when resampling the 3d image or applying affine transformations to it. For one image, the speed difference is small, ‘linear’ takes about x1.5 more time. For many images, ‘nearest’ scales much better, up to x20 faster.</p> <p>Once the 3d image has been interpolated at each sample point, the interpolated values are averaged to produce the value associated to this particular mesh vertex.</p></dd></dl><div class=section id=examples-using-nilearn-surface-vol-to-surf><h2><span class=section-number>8.14.3.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.surface.vol_to_surf</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-surface-vol-to-surf>¶</a></h2><div tooltip="In nilearn, nilearn.surface.vol_to_surf allows us to measure values of a 3d volume at the nodes..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id1><img alt="Technical point: Illustration of the volume to surface sampling schemes"src=../../_images/sphx_glr_plot_surface_projection_strategies_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/01_plotting/plot_surface_projection_strategies.html#sphx-glr-auto-examples-01-plotting-plot-surface-projection-strategies-py><span class="std std-ref">Technical point: Illustration of the volume to surface sampling schemes</span></a></span><a title="Permalink to this image"class=headerlink href=#id1>¶</a></p></div></div><div tooltip="project a 3D statistical map onto a cortical mesh using nilearn.surface.vol_to_surf. Display a ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id2><img alt="Making a surface plot of a 3D statistical map"src=../../_images/sphx_glr_plot_3d_map_to_surface_projection_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/01_plotting/plot_3d_map_to_surface_projection.html#sphx-glr-auto-examples-01-plotting-plot-3d-map-to-surface-projection-py><span class="std std-ref">Making a surface plot of a 3D statistical map</span></a></span><a title="Permalink to this image"class=headerlink href=#id2>¶</a></p></div></div><div tooltip="This is a demo for surface-based searchlight decoding, as described in: Chen, Y., Namburi, P., ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id3><img alt="Cortical surface-based searchlight decoding"src=../../_images/sphx_glr_plot_haxby_searchlight_surface_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight_surface.html#sphx-glr-auto-examples-02-decoding-plot-haxby-searchlight-surface-py><span class="std std-ref">Cortical surface-based searchlight decoding</span></a></span><a title="Permalink to this image"class=headerlink href=#id3>¶</a></p></div></div><div tooltip="A full step-by-step example of fitting a GLM to experimental data sampled on the cortical surfa..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id4><img alt="Example of surface-based first-level analysis"src=../../_images/sphx_glr_plot_localizer_surface_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html#sphx-glr-auto-examples-04-glm-first-level-plot-localizer-surface-analysis-py><span class="std std-ref">Example of surface-based first-level analysis</span></a></span><a title="Permalink to this image"class=headerlink href=#id4>¶</a></p></div></div><div tooltip=" Full step-by-step example of fitting a GLM (first and second level analysis) in a 10-subjects ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id5><img alt="Surface-based dataset first and second level analysis of a dataset"src=../../_images/sphx_glr_plot_surface_bids_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_surface_bids_analysis.html#sphx-glr-auto-examples-07-advanced-plot-surface-bids-analysis-py><span class="std std-ref">Surface-based dataset first and second level analysis of a dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id5>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.14.3. nilearn.surface.vol_to_surf</a><ul><li><a class="reference internal"href=#examples-using-nilearn-surface-vol-to-surf>8.14.3.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.surface.vol_to_surf</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.surface.load_surf_mesh.html><span class=section-number>8.14.2. </span>nilearn.surface.load_surf_mesh</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=../../auto_examples/index.html><span class=section-number>9. </span>Nilearn usage examples</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2020. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 3.4.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.surface.vol_to_surf.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>