<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="8.6.2.1. nilearn.interfaces.fmriprep.load_confounds"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/modules/generated/nilearn.interfaces.fmriprep.load_confounds.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Examples using nilearn.interfaces.fmriprep.load_confounds: Extracting signals from a brain parcellation Extracting signals from a brain parcellation,"property=og:description><meta content=../../_images/sphx_glr_plot_signal_extraction_thumb.png property=og:image><meta content="Extracting signals from a brain parcellation"property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/copybutton.css rel=stylesheet><link href=../../_static/sg_gallery.css rel=stylesheet><link href=../../_static/sg_gallery-binder.css rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/clipboard.min.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.6.2.2. nilearn.interfaces.fmriprep.load_confounds_strategy"href=nilearn.interfaces.fmriprep.load_confounds_strategy.html rel=next><link title="8.6.1.2. nilearn.interfaces.bids.parse_bids_filename"href=nilearn.interfaces.bids.parse_bids_filename.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.6.2.2. nilearn.interfaces.fmriprep.load_confounds_strategy"accesskey=N href=nilearn.interfaces.fmriprep.load_confounds_strategy.html>next</a> |</li><li class=right><a title="8.6.1.2. nilearn.interfaces.bids.parse_bids_filename"accesskey=P href=nilearn.interfaces.bids.parse_bids_filename.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li><a href=../../glossary.html>Glossary</a>| </li><li><a href=../../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-interfaces-fmriprep-load-confounds><h1><span class=section-number>8.6.2.1. </span>nilearn.interfaces.fmriprep.load_confounds<a title="Permalink to this headline"class=headerlink href=#nilearn-interfaces-fmriprep-load-confounds>¶</a></h1><dl class="py function"><dt class="sig sig-object py"id=nilearn.interfaces.fmriprep.load_confounds><span class="sig-prename descclassname"><span class=pre>nilearn.interfaces.fmriprep.</span></span><span class="sig-name descname"><span class=pre>load_confounds</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>img_files</span></span></em>, <em class=sig-param><span class=n><span class=pre>strategy</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>('motion',</span> <span class=pre>'high_pass',</span> <span class=pre>'wm_csf')</span></span></em>, <em class=sig-param><span class=n><span class=pre>motion</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'full'</span></span></em>, <em class=sig-param><span class=n><span class=pre>scrub</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>5</span></span></em>, <em class=sig-param><span class=n><span class=pre>fd_threshold</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>0.2</span></span></em>, <em class=sig-param><span class=n><span class=pre>std_dvars_threshold</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>3</span></span></em>, <em class=sig-param><span class=n><span class=pre>wm_csf</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'basic'</span></span></em>, <em class=sig-param><span class=n><span class=pre>global_signal</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'basic'</span></span></em>, <em class=sig-param><span class=n><span class=pre>compcor</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'anat_combined'</span></span></em>, <em class=sig-param><span class=n><span class=pre>n_compcor</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'all'</span></span></em>, <em class=sig-param><span class=n><span class=pre>ica_aroma</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'full'</span></span></em>, <em class=sig-param><span class=n><span class=pre>demean</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/9ddfa7259/nilearn/interfaces/fmriprep/load_confounds.py#L68><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.interfaces.fmriprep.load_confounds>¶</a></dt><dd><p>Use confounds from <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a>.</p> <p>To enable easy confound variables loading from <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> outputs, <cite>load_confounds</cite> provides an interface that groups subsets of confound variables into noise components and their parameters. It is possible to fine-tune a subset of noise components and their parameters through this function.</p> <p>The implementation will only support <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> functional derivative directory from the 1.2.x series. The <cite>compcor</cite> noise component requires 1.4.x series or above.</p> <div class=versionadded><p><span class="versionmodified added">New in version 0.9.0.</span></p></div> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>img_files</strong><span class=classifier>path to processed image files, optionally as a list.</span></dt><dd><p>Processed nii.gz/dtseries.nii/func.gii file reside in a <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> generated functional derivative directory (i.e.The associated confound files should be in the same directory as the image file). As long as the image file, confound related tsv and json are in the same directory with BIDS-complied names, <cite>load_confounds</cite> can retrieve the relevant files correctly.</p> <ul class=simple><li><p><cite>nii.gz</cite> or <cite>dtseries.nii</cite>: path to files, optionally as a list.</p></li><li><p><cite>func.gii</cite>: list of a pair of paths to files, optionally as a list of lists.</p></li></ul></dd><dt><strong>strategy</strong><span class=classifier>tuple or list of strings.</span></dt><dd><p>Default (“motion”, “high_pass”, “wm_csf”) The type of noise components to include.</p> <ul class=simple><li><p>“motion”: head motion estimates. Associated parameter: <cite>motion</cite></p></li><li><p>“wm_csf” confounds derived from white matter and cerebrospinal fluid. Associated parameter: <cite>wm_csf</cite></p></li><li><p>“global_signal” confounds derived from the global signal. Associated parameter: <cite>global_signal</cite></p></li><li><p>“compcor” confounds derived from CompCor <a class="footnote-reference brackets"href=#behzadi200790 id=id1>1</a>. When using this noise component, “high_pass” must also be applied. Associated parameter: <cite>compcor</cite>, <cite>n_compcor</cite></p></li><li><p>“ica_aroma” confounds derived from ICA-AROMA <a class="footnote-reference brackets"href=#pruim2015 id=id2>2</a>. Associated parameter: <cite>ica_aroma</cite></p></li><li><p>“scrub” regressors for <a class="footnote-reference brackets"href=#power2014 id=id3>3</a> scrubbing approach. Associated parameter: <cite>scrub</cite>, <cite>fd_threshold</cite>, <cite>std_dvars_threshold</cite></p></li></ul> <p>For each component above, associated parameters will be applied if specified. If associated parameters are not specified, any values supplied to the parameters are ignored. For example, <cite>strategy=(‘motion’, ‘global_signal’)</cite> will allow users to supply input to associated parameter <cite>motion</cite> and <cite>global_signal</cite>; if users pass <cite>wm_csf</cite> parameter, it will not be applied as it is not part of the <cite>strategy</cite>.</p> <p>There are two additional noise components with no optional parameters.</p> <ul class=simple><li><p>“non_steady_state” denotes volumes collected before the <a class="reference internal"href=../../glossary.html#term-fMRI><span class="xref std std-term">fMRI</span></a> scanner has reached a stable state.</p></li><li><p>“high_pass” adds discrete cosines transformation basis regressors to handle low-frequency signal drifts.</p></li></ul> <p>Non-steady-state volumes will always be checked. There’s no need to supply this component to the strategy.</p></dd><dt><strong>motion</strong><span class=classifier>{‘basic’, ‘power2’, ‘derivatives’, ‘full’}</span></dt><dd><p>Type of confounds extracted from head motion estimates.</p> <ul class=simple><li><p>“basic” translation/rotation (6 parameters)</p></li><li><p>“power2” translation/rotation + quadratic terms (12 parameters)</p></li><li><p>“derivatives” translation/rotation + derivatives (12 parameters)</p></li><li><p>“full” translation/rotation + derivatives + quadratic terms + power2d derivatives (24 parameters)</p></li></ul></dd><dt><strong>wm_csf</strong><span class=classifier>{‘basic’, ‘power2’, ‘derivatives’, ‘full’}</span></dt><dd><p>Type of confounds extracted from masks of white matter and cerebrospinal fluids.</p> <ul class=simple><li><p>“basic” the averages in each mask (2 parameters)</p></li><li><p>“power2” averages and quadratic terms (4 parameters)</p></li><li><p>“derivatives” averages and derivatives (4 parameters)</p></li><li><p>“full” averages + derivatives + quadratic terms + power2d derivatives (8 parameters)</p></li></ul></dd><dt><strong>global_signal</strong><span class=classifier>{‘basic’, ‘power2’, ‘derivatives’, ‘full’}</span></dt><dd><p>Type of confounds extracted from the global signal.</p> <ul class=simple><li><p>“basic” just the global signal (1 parameter)</p></li><li><p>“power2” global signal and quadratic term (2 parameters)</p></li><li><p>“derivatives” global signal and derivative (2 parameters)</p></li><li><p>“full” global signal + derivatives + quadratic terms + power2d derivatives (4 parameters)</p></li></ul></dd><dt><strong>scrub</strong><span class=classifier>int, default 5</span></dt><dd><p>After accounting for time frames with excessive motion, further remove segments shorter than the given number. The default value is 5 (referred as full scrubbing in <a class="footnote-reference brackets"href=#power2014 id=id4>3</a>). When the value is 0, temove time frames based on excessive framewise displacement and DVARS only. One-hot encoding vectors are added as regressors for each scrubbed frame.</p></dd><dt><strong>fd_threshold</strong><span class=classifier>float, default 0.2</span></dt><dd><p>Framewise displacement threshold for scrub (default = 0.2 mm)</p></dd><dt><strong>std_dvars_threshold</strong><span class=classifier>float, default 3</span></dt><dd><p>Standardized DVARS threshold for scrub (default = 3). DVARs is defined as root mean squared intensity difference of volume N to volume N+1 <a class="footnote-reference brackets"href=#power2012 id=id5>4</a>. D referring to temporal derivative of timecourses, VARS referring to root mean squared variance over voxels.</p></dd><dt><strong>compcor</strong><span class=classifier>{‘anat_combined’, ‘anat_separated’, ‘temporal’, ‘temporal_anat_combined’, ‘temporal_anat_separated’}</span></dt><dd><div class="admonition warning"><p class=admonition-title>Warning</p><p>Require fmriprep >= v:1.4.0.</p></div> <p>Type of confounds extracted from a component based noise correction method <a class="footnote-reference brackets"href=#behzadi200790 id=id6>1</a>.</p> <ul class=simple><li><p>“anat_combined” noise components calculated using a white matter and CSF combined anatomical mask</p></li><li><p>“anat_separated” noise components calculated using white matter mask and CSF mask compcor separately; two sets of scores are concatenated</p></li><li><p>“temporal” noise components calculated using temporal compcor</p></li><li><p>“temporal_anat_combined” components of “temporal” and “anat_combined”</p></li><li><p>“temporal_anat_separated” components of “temporal” and “anat_separated”</p></li></ul></dd><dt><strong>n_compcor</strong><span class=classifier>“all” or int, default “all”</span></dt><dd><p>The number of noise components to be extracted. For acompcor_combined=False, and/or compcor=”full”, this is the number of components per mask. “all”: select all components (50% variance explained by <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> defaults)</p></dd><dt><strong>ica_aroma</strong><span class=classifier>{‘full’, ‘basic’}</span></dt><dd><ul class=simple><li><p>“full”: use <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> output <cite>~desc-smoothAROMAnonaggr_bold.nii.gz</cite>.</p></li><li><p>“basic”: use noise independent components only.</p></li></ul></dd><dt><strong>demean</strong><span class=classifier>boolean, default True</span></dt><dd><p>If True, the confounds are standardized to a zero mean (over time). When using <a class="reference internal"href=nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker title=nilearn.maskers.NiftiMasker><code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiMasker</span></code></a> with default parameters, the recommended option is True. When using <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a> with default parameters, the recommended option is False. When <cite>sample_mask</cite> is not None, the mean is calculated on retained volumes.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>confounds</strong><span class=classifier>pandas.DataFrame, or list of</span></dt><dd><p>A reduced version of <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> confounds based on selected strategy and flags. An intercept is automatically added to the list of confounds. The columns contains the labels of the regressors.</p></dd><dt><strong>sample_mask</strong><span class=classifier>None, numpy.ndarray, or list of</span></dt><dd><p>When no volumns require removal, the value is None. Otherwise, shape: (number of scans - number of volumes removed, ) The index of the niimgs along time/fourth dimension for valid volumes for subsequent analysis. This attribute should be passed to parameter <cite>sample_mask</cite> of <a class="reference internal"href=nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker title=nilearn.maskers.NiftiMasker><code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiMasker</span></code></a> or <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>. Volumns are removed if flagged as following:</p> <ul class=simple><li><p>Non-steady-state volumes (if present)</p></li><li><p>Motion outliers detected by scrubbing</p></li></ul></dd></dl></dd></dl> <div class="admonition seealso"><p class=admonition-title>See also</p><dl class=simple><dt><a class="reference internal"href=nilearn.interfaces.fmriprep.load_confounds_strategy.html#nilearn.interfaces.fmriprep.load_confounds_strategy title=nilearn.interfaces.fmriprep.load_confounds_strategy><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.interfaces.fmriprep.load_confounds_strategy</span></code></a></dt><dd></dd></dl></div> <p class=rubric>Notes</p> <p>The noise components implemented in this class are adapted from <a class="footnote-reference brackets"href=#ciric2017 id=id7>5</a>. Band-pass filter is replaced by high-pass filter. Low-pass filters can be implemented, e.g., through <cite>NifitMaskers</cite>. Other aspects of the preprocessing listed in <a class="footnote-reference brackets"href=#ciric2017 id=id8>5</a> are controlled through <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a>, e.g. distortion correction.</p> <p class=rubric>References</p> <p><dl class="footnote brackets"><dt class=label id=behzadi200790><span class=brackets>1</span><span class=fn-backref>(<a href=#id1>1</a>,<a href=#id6>2</a>)</span></dt><dd><p>Yashar Behzadi, Khaled Restom, Joy Liau, and Thomas T. Liu. A component based noise correction method (compcor) for bold and perfusion based fmri. <em>NeuroImage</em>, 37(1):90–101, 2007. URL: <a class="reference external"href=https://www.sciencedirect.com/science/article/pii/S1053811907003837>https://www.sciencedirect.com/science/article/pii/S1053811907003837</a>, <a class="reference external"href=https://doi.org/https://doi.org/10.1016/j.neuroimage.2007.04.042>doi:https://doi.org/10.1016/j.neuroimage.2007.04.042</a>.</p></dd><dt class=label id=pruim2015><span class=brackets><a class=fn-backref href=#id2>2</a></span></dt><dd><p>Raimon H. R. Pruim, Maarten Mennes, Daan van Rooij, Alberto Llera, Jan K. Buitelaar, and Christian F. Beckmann. ICA-AROMA: a robust ICA-based strategy for removing motion artifacts from fMRI data. <em>Neuroimage</em>, 112:267–277, 2015. <a class="reference external"href=https://doi.org/10.1016/j.neuroimage.2015.02.064>doi:10.1016/j.neuroimage.2015.02.064</a>.</p></dd><dt class=label id=power2014><span class=brackets>3</span><span class=fn-backref>(<a href=#id3>1</a>,<a href=#id4>2</a>)</span></dt><dd><p>Jonathan D. Power, Anish Mitra, Timothy O. Laumann, Abraham Z. Snyder, Bradley L. Schlaggar, and Steven E. Petersen. Methods to detect, characterize, and remove motion artifact in resting state fMRI. <em>NeuroImage</em>, 84:320–341, 2014. URL: <a class="reference external"href=http://www.sciencedirect.com/science/article/pii/S1053811913009117>http://www.sciencedirect.com/science/article/pii/S1053811913009117</a>, <a class="reference external"href=https://doi.org/10.1016/j.neuroimage.2013.08.048>doi:10.1016/j.neuroimage.2013.08.048</a>.</p></dd><dt class=label id=power2012><span class=brackets><a class=fn-backref href=#id5>4</a></span></dt><dd><p>Jonathan D. Power, Kelly A. Barnes, Abraham Z. Snyder, Bradley L. Schlaggar, and Steven E. Petersen. Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion. <em>NeuroImage</em>, 59(3):2142–2154, 2012. URL: <a class="reference external"href="http://www.ncbi.nlm.nih.gov/pubmed/22019881 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3254728">http://www.ncbi.nlm.nih.gov/pubmed/22019881 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3254728</a>, <a class="reference external"href=https://doi.org/10.1016/j.neuroimage.2011.10.018>doi:10.1016/j.neuroimage.2011.10.018</a>.</p></dd><dt class=label id=ciric2017><span class=brackets>5</span><span class=fn-backref>(<a href=#id7>1</a>,<a href=#id8>2</a>)</span></dt><dd><p>Rastko Ciric, Daniel H. Wolf, Jonathan D. Power, David R. Roalf, Graham L. Baum, Kosha Ruparel, Russell T. Shinohara, Mark A. Elliott, Simon B. Eickhoff, Christos Davatzikos, Ruben C. Gur, Raquel E. Gur, Danielle S. Bassett, and Theodore D. Satterthwaite. Benchmarking of participant-level confound regression strategies for the control of motion artifact in studies of functional connectivity. <em>NeuroImage</em>, 154(1):174–187, 2017. <a class="reference external"href=https://doi.org/10.1016/j.neuroimage.2017.03.020>doi:10.1016/j.neuroimage.2017.03.020</a>.</p></dd></dl></dd></dl><div class=section id=examples-using-nilearn-interfaces-fmriprep-load-confounds><h2><span class=section-number>8.6.2.1.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.interfaces.fmriprep.load_confounds</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-interfaces-fmriprep-load-confounds>¶</a></h2><div tooltip="Here we show how to extract signals from a brain parcellation and compute a correlation matrix."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id9><img alt="Extracting signals from a brain parcellation"src=../../_images/sphx_glr_plot_signal_extraction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_signal_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-signal-extraction-py><span class="std std-ref">Extracting signals from a brain parcellation</span></a></span><a title="Permalink to this image"class=headerlink href=#id9>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.6.2.1. nilearn.interfaces.fmriprep.load_confounds</a><ul><li><a class="reference internal"href=#examples-using-nilearn-interfaces-fmriprep-load-confounds>8.6.2.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.interfaces.fmriprep.load_confounds</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.interfaces.bids.parse_bids_filename.html><span class=section-number>8.6.1.2. </span>nilearn.interfaces.bids.parse_bids_filename</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.interfaces.fmriprep.load_confounds_strategy.html><span class=section-number>8.6.2.2. </span>nilearn.interfaces.fmriprep.load_confounds_strategy</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.interfaces.fmriprep.load_confounds.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>