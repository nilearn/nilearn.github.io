<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="8.2.5.1. nilearn.datasets.fetch_neurovault"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/modules/generated/nilearn.datasets.fetch_neurovault.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Examples using nilearn.datasets.fetch_neurovault: NeuroVault cross-study ICA maps. NeuroVault cross-study ICA maps.,"property=og:description><meta content=../../_images/sphx_glr_plot_ica_neurovault_thumb.png property=og:image><meta content="NeuroVault cross-study ICA maps."property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/copybutton.css rel=stylesheet><link href=../../_static/sg_gallery.css rel=stylesheet><link href=../../_static/sg_gallery-binder.css rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/clipboard.min.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.2.5.2. nilearn.datasets.fetch_neurovault_ids"href=nilearn.datasets.fetch_neurovault_ids.html rel=next><link title="8.2.4.8. nilearn.datasets.fetch_neurovault_motor_task"href=nilearn.datasets.fetch_neurovault_motor_task.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.2.5.2. nilearn.datasets.fetch_neurovault_ids"accesskey=N href=nilearn.datasets.fetch_neurovault_ids.html>next</a> |</li><li class=right><a title="8.2.4.8. nilearn.datasets.fetch_neurovault_motor_task"accesskey=P href=nilearn.datasets.fetch_neurovault_motor_task.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li><a href=../../glossary.html>Glossary</a>| </li><li><a href=../../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the function signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-datasets-fetch-neurovault><h1><span class=section-number>8.2.5.1. </span>nilearn.datasets.fetch_neurovault<a title="Permalink to this headline"class=headerlink href=#nilearn-datasets-fetch-neurovault>¶</a></h1><dl class="py function"><dt class="sig sig-object py"id=nilearn.datasets.fetch_neurovault><span class="sig-prename descclassname"><span class=pre>nilearn.datasets.</span></span><span class="sig-name descname"><span class=pre>fetch_neurovault</span></span><span class=sig-paren>(</span><em class=sig-param><span class=pre>max_images=100</span></em>, <em class=sig-param><span class=pre>collection_terms={'number_of_images':</span> <span class=pre>NotNull()}</span></em>, <em class=sig-param><span class=pre>collection_filter=&LTfunction</span> <span class=pre>_empty_filter></span></em>, <em class=sig-param><span class=pre>image_terms={'image_type':</span> <span class=pre>NotEqual('atlas')</span></em>, <em class=sig-param><span class=pre>'is_thresholded':</span> <span class=pre>False</span></em>, <em class=sig-param><span class=pre>'map_type':</span> <span class=pre>NotIn('ROI/mask'</span></em>, <em class=sig-param><span class=pre>'anatomical'</span></em>, <em class=sig-param><span class=pre>'parcellation')</span></em>, <em class=sig-param><span class=pre>'not_mni':</span> <span class=pre>False}</span></em>, <em class=sig-param><span class=pre>image_filter=&LTfunction</span> <span class=pre>_empty_filter></span></em>, <em class=sig-param><span class=pre>mode='download_new'</span></em>, <em class=sig-param><span class=pre>data_dir=None</span></em>, <em class=sig-param><span class=pre>fetch_neurosynth_words=False</span></em>, <em class=sig-param><span class=pre>resample=False</span></em>, <em class=sig-param><span class=pre>vectorize_words=True</span></em>, <em class=sig-param><span class=pre>verbose=3</span></em>, <em class=sig-param><span class=pre>**kwarg_image_filters</span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/9ddfa7259/nilearn/datasets/neurovault.py#L2297><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.datasets.fetch_neurovault>¶</a></dt><dd><p>Download data from neurovault.org that match certain criteria.</p> <p>Any downloaded data is saved on the local disk and subsequent calls to this function will first look for the data locally before querying the server for more if necessary.</p> <p>We explore the metadata for Neurovault collections and images, keeping those that match a certain set of criteria, until we have skimmed through the whole database or until an (optional) maximum number of images to fetch has been reached.</p> <p>For more information, see <a class="footnote-reference brackets"href=#gorgolewski2015neurovault id=id1>1</a>, and <a class="footnote-reference brackets"href=#yarkoni2011large id=id2>2</a>.</p> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>max_images</strong><span class=classifier>int, optional</span></dt><dd><p>Maximum number of images to fetch. Default=100.</p></dd><dt><strong>collection_terms</strong><span class=classifier>dict, optional</span></dt><dd><p>Key, value pairs used to filter collection metadata. Collections for which <code class="docutils literal notranslate"><span class=pre>collection_metadata['key']</span> <span class=pre>==</span> <span class=pre>value</span></code> is not <code class="docutils literal notranslate"><span class=pre>True</span></code> for every key, value pair will be discarded. See documentation for <code class="docutils literal notranslate"><span class=pre>basic_collection_terms</span></code> for a description of the default selection criteria. Default=basic_collection_terms().</p></dd><dt><strong>collection_filter</strong><span class=classifier>Callable, optional</span></dt><dd><p>Collections for which <cite>collection_filter(collection_metadata)</cite> is <code class="docutils literal notranslate"><span class=pre>False</span></code> will be discarded. Default=empty_filter.</p></dd><dt><strong>image_terms</strong><span class=classifier>dict, optional</span></dt><dd><p>Key, value pairs used to filter image metadata. Images for which <code class="docutils literal notranslate"><span class=pre>image_metadata['key']</span> <span class=pre>==</span> <span class=pre>value</span></code> is not <code class="docutils literal notranslate"><span class=pre>True</span></code> for if image_filter != _empty_filter and image_terms = every key, value pair will be discarded. See documentation for <code class="docutils literal notranslate"><span class=pre>basic_image_terms</span></code> for a description of the default selection criteria. Default=basic_image_terms().</p></dd><dt><strong>image_filter</strong><span class=classifier>Callable, optional</span></dt><dd><p>Images for which <cite>image_filter(image_metadata)</cite> is <code class="docutils literal notranslate"><span class=pre>False</span></code> will be discarded. Default=empty_filter.</p></dd><dt><strong>mode</strong><span class=classifier>{‘download_new’, ‘overwrite’, ‘offline’}</span></dt><dd><p>When to fetch an image from the server rather than the local disk.</p> <ul class=simple><li><p>‘download_new’ (the default) means download only files that are not already on disk (regardless of modify date).</p></li><li><p>‘overwrite’ means ignore files on disk and overwrite them.</p></li><li><p>‘offline’ means load only data from disk; don’t query server.</p></li></ul></dd><dt><strong>data_dir</strong><span class=classifier>str, optional</span></dt><dd><p>The directory we want to use for nilearn data. A subdirectory named “neurovault” will contain neurovault data.</p></dd><dt><strong>fetch_neurosynth_words</strong><span class=classifier>bool, optional</span></dt><dd><p>whether to collect words from Neurosynth. Default=False.</p></dd><dt><strong>vectorize_words</strong><span class=classifier>bool, optional</span></dt><dd><p>If neurosynth words are downloaded, create a matrix of word counts and add it to the result. Also add to the result a vocabulary list. See <code class="docutils literal notranslate"><span class=pre>sklearn.CountVectorizer</span></code> for more info. Default=True.</p></dd><dt><strong>resample</strong><span class=classifier>bool, optional (default=False)</span></dt><dd><p>Resamples downloaded images to a 3x3x3 grid before saving them, to save disk space.</p></dd><dt><strong>interpolation</strong><span class=classifier>str, optional</span></dt><dd><p>Can be ‘continuous’, ‘linear’, or ‘nearest’. Indicates the resample method. Default=’continuous’. Argument passed to nilearn.image.resample_img.</p></dd><dt><strong>verbose</strong><span class=classifier>int, optional</span></dt><dd><p>An integer in [0, 1, 2, 3] to control the verbosity level. Default=3.</p></dd><dt><strong>kwarg_image_filters</strong></dt><dd><p>Keyword arguments are understood to be filter terms for images, so for example <code class="docutils literal notranslate"><span class=pre>map_type='Z</span> <span class=pre>map'</span></code> means only download Z-maps; <code class="docutils literal notranslate"><span class=pre>collection_id=35</span></code> means download images from collection 35 only.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl><dt>Bunch</dt><dd><p>A dict-like object which exposes its items as attributes. It contains:</p> <blockquote><div><ul class=simple><li><p>‘images’, the paths to downloaded files.</p></li><li><p>‘images_meta’, the metadata for the images in a list of dictionaries.</p></li><li><p>‘collections_meta’, the metadata for the collections.</p></li><li><p>‘description’, a short description of the Neurovault dataset.</p></li></ul></div></blockquote> <p>If <cite>fetch_neurosynth_words</cite> and <cite>vectorize_words</cite> were set, it also contains:</p> <blockquote><div><ul class=simple><li><p>‘vocabulary’, a list of words</p></li><li><p>‘word_frequencies’, the weight of the words returned by neurosynth.org for each image, such that the weight of word <cite>vocabulary[j]</cite> for the image found in <cite>images[i]</cite> is <cite>word_frequencies[i, j]</cite></p></li></ul></div></blockquote></dd></dl></dd></dl> <div class="admonition seealso"><p class=admonition-title>See also</p><dl class=simple><dt><a class="reference internal"href=nilearn.datasets.fetch_neurovault_ids.html#nilearn.datasets.fetch_neurovault_ids title=nilearn.datasets.fetch_neurovault_ids><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_neurovault_ids</span></code></a></dt><dd><p>Fetch collections and images from Neurovault by explicitly specifying their ids.</p></dd></dl></div> <p class=rubric>Notes</p> <p>Images and collections from disk are fetched before remote data.</p> <p>Some helpers are provided in the <code class="docutils literal notranslate"><span class=pre>neurovault</span></code> module to express filtering criteria more concisely:</p> <blockquote><div><p><code class="docutils literal notranslate"><span class=pre>ResultFilter</span></code>, <code class="docutils literal notranslate"><span class=pre>IsNull</span></code>, <code class="docutils literal notranslate"><span class=pre>NotNull</span></code>, <code class="docutils literal notranslate"><span class=pre>NotEqual</span></code>, <code class="docutils literal notranslate"><span class=pre>GreaterOrEqual</span></code>, <code class="docutils literal notranslate"><span class=pre>GreaterThan</span></code>, <code class="docutils literal notranslate"><span class=pre>LessOrEqual</span></code>, <code class="docutils literal notranslate"><span class=pre>LessThan</span></code>, <code class="docutils literal notranslate"><span class=pre>IsIn</span></code>, <code class="docutils literal notranslate"><span class=pre>NotIn</span></code>, <code class="docutils literal notranslate"><span class=pre>Contains</span></code>, <code class="docutils literal notranslate"><span class=pre>NotContains</span></code>, <code class="docutils literal notranslate"><span class=pre>Pattern</span></code>.</p></div></blockquote> <p>If you pass a single value to match against the collection id (whether as the ‘id’ field of the collection metadata or as the ‘collection_id’ field of the image metadata), the server is directly queried for that collection, so <code class="docutils literal notranslate"><span class=pre>fetch_neurovault(collection_id=40)</span></code> is as efficient as <code class="docutils literal notranslate"><span class=pre>fetch_neurovault(collection_ids=[40])</span></code> (but in the former version the other filters will still be applied). This is not true for the image ids. If you pass a single value to match against any of the fields listed in <code class="docutils literal notranslate"><span class=pre>_COL_FILTERS_AVAILABLE_ON_SERVER</span></code>, i.e., ‘DOI’, ‘name’, and ‘owner’, these filters can be applied by the server, limiting the amount of metadata we have to download: filtering on those fields makes the fetching faster because the filtering takes place on the server side.</p> <p>In <cite>download_new</cite> mode, if a file exists on disk, it is not downloaded again, even if the version on the server is newer. Use <cite>overwrite</cite> mode to force a new download (you can filter on the field <code class="docutils literal notranslate"><span class=pre>modify_date</span></code> to re-download the files that are newer on the server - see Examples section).</p> <p>Tries to yield <cite>max_images</cite> images; stops early if we have fetched all the images matching the filters or if too many images fail to be downloaded in a row.</p> <p class=rubric>References</p> <p><dl class="footnote brackets"><dt class=label id=gorgolewski2015neurovault><span class=brackets><a class=fn-backref href=#id1>1</a></span></dt><dd><p>Krzysztof J. Gorgolewski, Gael Varoquaux, Gabriel Rivera, Yannick Schwarz, Satrajit S. Ghosh, Camille Maumet, Vanessa V. Sochat, Thomas E. Nichols, Russell A. Poldrack, Jean-Baptiste Poline, Tal Yarkoni, and Daniel S. Margulies. Neurovault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain. <em>Frontiers in Neuroinformatics</em>, 9:8, 2015. URL: <a class="reference external"href=https://www.frontiersin.org/article/10.3389/fninf.2015.00008>https://www.frontiersin.org/article/10.3389/fninf.2015.00008</a>, <a class="reference external"href=https://doi.org/10.3389/fninf.2015.00008>doi:10.3389/fninf.2015.00008</a>.</p></dd><dt class=label id=yarkoni2011large><span class=brackets><a class=fn-backref href=#id2>2</a></span></dt><dd><p>Tal Yarkoni, Russell A Poldrack, Thomas E Nichols, David C Van Essen, and Tor D Wager. Large-scale automated synthesis of human functional neuroimaging data. <em>Nature methods</em>, 8(8):665–670, 2011.</p></dd></dl> <p class=rubric>Examples</p> <p>To download <strong>all</strong> the collections and images from Neurovault:</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=n>fetch_neurovault</span><span class=p>(</span><span class=n>max_images</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>collection_terms</span><span class=o>=</span><span class=p>{},</span> <span class=n>image_terms</span><span class=o>=</span><span class=p>{})</span>
</pre></div></div> <p>To further limit the default selection to collections which specify a DOI (which reference a published paper, as they may be more likely to contain good images):</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=n>fetch_neurovault</span><span class=p>(</span>
    <span class=n>max_images</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
    <span class=n>collection_terms</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>basic_collection_terms</span><span class=p>(),</span> <span class=n>DOI</span><span class=o>=</span><span class=n>NotNull</span><span class=p>()))</span>
</pre></div></div> <p>To update all the images (matching the default filters):</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=n>fetch_neurovault</span><span class=p>(</span>
    <span class=n>max_images</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>'overwrite'</span><span class=p>,</span>
    <span class=n>modify_date</span><span class=o>=</span><span class=n>GreaterThan</span><span class=p>(</span><span class=n>newest</span><span class=p>))</span>
</pre></div></div></dd></dl><div class=section id=examples-using-nilearn-datasets-fetch-neurovault><h2><span class=section-number>8.2.5.1.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_neurovault</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-datasets-fetch-neurovault>¶</a></h2><div tooltip="This example shows how to download statistical maps from NeuroVault, label them with NeuroSynth..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id3><img alt="NeuroVault cross-study ICA maps."src=../../_images/sphx_glr_plot_ica_neurovault_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_neurovault.html#sphx-glr-auto-examples-07-advanced-plot-ica-neurovault-py><span class="std std-ref">NeuroVault cross-study ICA maps.</span></a></span><a title="Permalink to this image"class=headerlink href=#id3>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.2.5.1. nilearn.datasets.fetch_neurovault</a><ul><li><a class="reference internal"href=#examples-using-nilearn-datasets-fetch-neurovault>8.2.5.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.datasets.fetch_neurovault</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.datasets.fetch_neurovault_motor_task.html><span class=section-number>8.2.4.8. </span>nilearn.datasets.fetch_neurovault_motor_task</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.datasets.fetch_neurovault_ids.html><span class=section-number>8.2.5.2. </span>nilearn.datasets.fetch_neurovault_ids</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.datasets.fetch_neurovault.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>