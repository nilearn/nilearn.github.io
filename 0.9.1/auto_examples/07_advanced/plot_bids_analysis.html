<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="9.8.3. BIDS dataset first and second level analysis"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/auto_examples/07_advanced/plot_bids_analysis.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Full step-by-step example of fitting a GLM to perform a first and second level analysis in a BIDS dataset and visualizing the results. Details about the BIDS standard can be consulted at http://bid..."property=og:description><meta content=https://nilearn.github.io/_static/nilearn-logo.png property=og:image><meta content=Nilearn property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/copybutton.css rel=stylesheet><link href=../../_static/sg_gallery.css rel=stylesheet><link href=../../_static/sg_gallery-binder.css rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/clipboard.min.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="9.8.4. Functional connectivity predicts age group"href=plot_age_group_prediction_cross_val.html rel=next><link title="9.8.2. Massively univariate analysis of a calculation task from the Localizer dataset"href=plot_localizer_simple_analysis.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../../modules/reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="9.8.4. Functional connectivity predicts age group"accesskey=N href=plot_age_group_prediction_cross_val.html>next</a> |</li><li class=right><a title="9.8.2. Massively univariate analysis of a calculation task from the Localizer dataset"accesskey=P href=plot_localizer_simple_analysis.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../index.html>Examples</a> | </li><li><a href=../../modules/reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li><a href=../../glossary.html>Glossary</a>| </li><li><a href=../../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../index.html><span class=section-number>9. </span>Nilearn usage examples</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="sphx-glr-download-link-note admonition note"><p class=admonition-title>Note</p><p>Click <a class="reference internal"href=#sphx-glr-download-auto-examples-07-advanced-plot-bids-analysis-py><span class="std std-ref">here</span></a> to download the full example code or to run this example in your browser via Binder</p></div><div class="sphx-glr-example-title section"id=bids-dataset-first-and-second-level-analysis><span id=sphx-glr-auto-examples-07-advanced-plot-bids-analysis-py></span><h1><span class=section-number>9.8.3. </span>BIDS dataset first and second level analysis<a title="Permalink to this headline"class=headerlink href=#bids-dataset-first-and-second-level-analysis>¶</a></h1><p>Full step-by-step example of fitting a <a class="reference internal"href=../../glossary.html#term-GLM><span class="xref std std-term">GLM</span></a> to perform a first and second level analysis in a <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset and visualizing the results. Details about the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> standard can be consulted at <a class="reference external"href=http://bids.neuroimaging.io/>http://bids.neuroimaging.io/</a>.</p><p>More specifically:</p><ol class="arabic simple"><li><p>Download an <a class="reference internal"href=../../glossary.html#term-fMRI><span class="xref std std-term">fMRI</span></a> <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset with two language conditions to contrast.</p></li><li><p>Extract first level model objects automatically from the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset.</p></li><li><p>Fit a second level model on the fitted first level models. Notice that in this case the preprocessed <a class="reference internal"href=../../glossary.html#term-BOLD><span class="xref std std-term">bold</span></a> images were already normalized to the same <a class="reference internal"href=../../glossary.html#term-MNI><span class="xref std std-term">MNI</span></a> space.</p></li></ol><p>To run this example, you must launch IPython via <code class="docutils literal notranslate"><span class=pre>ipython</span>
<span class=pre>--matplotlib</span></code> in a terminal, or use the Jupyter notebook.</p><div class="contents local topic"id=contents><p class=topic-title><strong>Contents</strong></p><ul class=simple><li><p><a class="reference internal"href=#fetch-example-bids-dataset id=id1>Fetch example BIDS dataset</a></p></li><li><p><a class="reference internal"href=#obtain-automatically-firstlevelmodel-objects-and-fit-arguments id=id2>Obtain automatically FirstLevelModel objects and fit arguments</a></p></li><li><p><a class="reference internal"href=#quick-sanity-check-on-fit-arguments id=id3>Quick sanity check on fit arguments</a></p></li><li><p><a class="reference internal"href=#first-level-model-estimation id=id4>First level model estimation</a></p></li><li><p><a class="reference internal"href=#second-level-model-estimation id=id5>Second level model estimation</a></p></li></ul></div><div class=section id=fetch-example-bids-dataset><h2><a class=toc-backref href=#id1><span class=section-number>9.8.3.1. </span>Fetch example BIDS dataset</a><a title="Permalink to this headline"class=headerlink href=#fetch-example-bids-dataset>¶</a></h2><p>We download a simplified <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset made available for illustrative purposes. It contains only the necessary information to run a statistical analysis using Nilearn. The raw data subject folders only contain bold.json and events.tsv files, while the derivatives folder includes the preprocessed files preproc.nii and the confounds.tsv files.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.datasets</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_language_localizer_demo_dataset.html#nilearn.datasets.fetch_language_localizer_demo_dataset title=nilearn.datasets.fetch_language_localizer_demo_dataset><span class=n>fetch_language_localizer_demo_dataset</span></a>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>_</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_language_localizer_demo_dataset.html#nilearn.datasets.fetch_language_localizer_demo_dataset title=nilearn.datasets.fetch_language_localizer_demo_dataset><span class=n>fetch_language_localizer_demo_dataset</span></a><span class=p>()</span>
</pre></div></div><p>Here is the location of the dataset on disk.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=nb>print</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>)</span>
</pre></div></div><p class=sphx-glr-script-out>Out:</p><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>/home/nicolas/nilearn_data/fMRI-language-localizer-demo-dataset
</pre></div></div></div><div class=section id=obtain-automatically-firstlevelmodel-objects-and-fit-arguments><h2><a class=toc-backref href=#id2><span class=section-number>9.8.3.2. </span>Obtain automatically FirstLevelModel objects and fit arguments</a><a title="Permalink to this headline"class=headerlink href=#obtain-automatically-firstlevelmodel-objects-and-fit-arguments>¶</a></h2><p>From the dataset directory we automatically obtain the FirstLevelModel objects with their subject_id filled from the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset. Moreover, we obtain for each model a dictionary with run_imgs, events and confounder regressors since in this case a confounds.tsv file is available in the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset. To get the first level models we only have to specify the dataset directory and the task_label as specified in the file names.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.glm.first_level</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.glm.first_level.first_level_from_bids.html#nilearn.glm.first_level.first_level_from_bids title=nilearn.glm.first_level.first_level_from_bids><span class=n>first_level_from_bids</span></a>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>task_label</span></a> <span class=o>=</span> <span class=s1>'languagelocalizer'</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_run_imgs</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_events</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_confounds</span></a> <span class=o>=</span> \
    <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.glm.first_level.first_level_from_bids.html#nilearn.glm.first_level.first_level_from_bids title=nilearn.glm.first_level.first_level_from_bids><span class=n>first_level_from_bids</span></a><span class=p>(</span>
        <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>task_label</span></a><span class=p>,</span>
        <span class=n>img_filters</span><span class=o>=</span><span class=p>[(</span><span class=s1>'desc'</span><span class=p>,</span> <span class=s1>'preproc'</span><span class=p>)])</span>
</pre></div></div><p class=sphx-glr-script-out>Out:</p><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>/home/nicolas/GitRepos/nilearn-fork/nilearn/glm/first_level/first_level.py:901: UserWarning:

SliceTimingRef not found in file /home/nicolas/nilearn_data/fMRI-language-localizer-demo-dataset/derivatives/sub-01/func/sub-01_task-languagelocalizer_desc-preproc_bold.json. It will be assumed that the slice timing reference is 0.0 percent of the repetition time. If it is not the case it will need to be set manually in the generated list of models
</pre></div></div></div><div class=section id=quick-sanity-check-on-fit-arguments><h2><a class=toc-backref href=#id3><span class=section-number>9.8.3.3. </span>Quick sanity check on fit arguments</a><a title="Permalink to this headline"class=headerlink href=#quick-sanity-check-on-fit-arguments>¶</a></h2><p>Additional checks or information extraction from pre-processed data can be made here.</p><p>We just expect one run_img per subject.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>import</span> <span class=nn>os</span>
<span class=nb>print</span><span class=p>([</span><a class="sphx-glr-backref-module-os-path sphx-glr-backref-type-py-function"href=https://docs.python.org/3.8/library/os.path.html#os.path.basename title=os.path.basename><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>basename</span></a><span class=p>(</span><span class=n>run</span><span class=p>)</span> <span class=k>for</span> <span class=n>run</span> <span class=ow>in</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_run_imgs</span></a><span class=p>[</span><span class=mi>0</span><span class=p>]])</span>
</pre></div></div><p class=sphx-glr-script-out>Out:</p><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>['sub-01_task-languagelocalizer_desc-preproc_bold.nii.gz']
</pre></div></div><p>The only confounds stored are regressors obtained from motion correction. As we can verify from the column headers of the confounds table corresponding to the only run_img present.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=nb>print</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_confounds</span></a><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>columns</span><span class=p>)</span>
</pre></div></div><p class=sphx-glr-script-out>Out:</p><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>Index(['RotX', 'RotY', 'RotZ', 'X', 'Y', 'Z'], dtype='object')
</pre></div></div><p>During this acquisition the subject read blocks of sentences and consonant strings. So these are our only two conditions in events. We verify there are 12 blocks for each condition.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=nb>print</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_events</span></a><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>][</span><span class=s1>'trial_type'</span><span class=p>]</span><span class=o>.</span><span class=n>value_counts</span><span class=p>())</span>
</pre></div></div><p class=sphx-glr-script-out>Out:</p><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>language    12
string      12
Name: trial_type, dtype: int64
</pre></div></div></div><div class=section id=first-level-model-estimation><h2><a class=toc-backref href=#id4><span class=section-number>9.8.3.4. </span>First level model estimation</a><a title="Permalink to this headline"class=headerlink href=#first-level-model-estimation>¶</a></h2><p>Now we simply fit each first level model and plot for each subject the <a class="reference internal"href=../../glossary.html#term-contrast><span class="xref std std-term">contrast</span></a> that reveals the language network (language - string). Notice that we can define a contrast using the names of the conditions specified in the events dataframe. Sum, subtraction and scalar multiplication are allowed.</p><p>Set the threshold as the z-variate with an uncorrected p-value of 0.001.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
<a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"href=https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64 title=numpy.float64><span class=n>p001_unc</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-scipy-stats sphx-glr-backref-type-py-method"href=http://scipy.github.io/devdocs/reference/generated/scipy.stats.rv_continuous.isf.html#scipy.stats.rv_continuous.isf title=scipy.stats.rv_continuous.isf><span class=n>norm</span><span class=o>.</span><span class=n>isf</span></a><span class=p>(</span><span class=mf>0.001</span><span class=p>)</span>
</pre></div></div><p>Prepare figure for concurrent plot of individual maps.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn</span> <span class=kn>import</span> <span class=n>plotting</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>

<a class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure title=matplotlib.figure.Figure><span class=n>fig</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray title=numpy.ndarray><span class=n>axes</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots title=matplotlib.pyplot.subplots><span class=n>plt</span><span class=o>.</span><span class=n>subplots</span></a><span class=p>(</span><span class=n>nrows</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>ncols</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span> <span class=mf>4.5</span><span class=p>))</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-function"href=https://docs.python.org/3.8/library/functions.html#zip title=builtins.zip><span class=n>model_and_args</span></a> <span class=o>=</span> <span class=nb>zip</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_run_imgs</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_events</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models_confounds</span></a><span class=p>)</span>
<span class=k>for</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/functions.html#int title=builtins.int><span class=n>midx</span></a><span class=p>,</span> <span class=p>(</span><a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel title=nilearn.glm.first_level.FirstLevelModel><span class=n>model</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>imgs</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>events</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>confounds</span></a><span class=p>)</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-function"href=https://docs.python.org/3.8/library/functions.html#zip title=builtins.zip><span class=n>model_and_args</span></a><span class=p>):</span>
    <span class=c1># fit the GLM</span>
    <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel.fit title=nilearn.glm.first_level.FirstLevelModel.fit><span class=n>model</span><span class=o>.</span><span class=n>fit</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>imgs</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>events</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>confounds</span></a><span class=p>)</span>
    <span class=c1># compute the contrast of interest</span>
    <a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>zmap</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel.compute_contrast title=nilearn.glm.first_level.FirstLevelModel.compute_contrast><span class=n>model</span><span class=o>.</span><span class=n>compute_contrast</span></a><span class=p>(</span><span class=s1>'language-string'</span><span class=p>)</span>
    <a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html#nilearn.plotting.plot_glass_brain title=nilearn.plotting.plot_glass_brain><span class=n>plotting</span><span class=o>.</span><span class=n>plot_glass_brain</span></a><span class=p>(</span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>zmap</span></a><span class=p>,</span> <span class=n>colorbar</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"href=https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64 title=numpy.float64><span class=n>p001_unc</span></a><span class=p>,</span>
                              <span class=n>title</span><span class=o>=</span><span class=p>(</span><span class=s1>'sub-'</span> <span class=o>+</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#str title=builtins.str><span class=n>model</span><span class=o>.</span><span class=n>subject_label</span></a><span class=p>),</span>
                              <a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray title=numpy.ndarray><span class=n>axes</span></a><span class=o>=</span><a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray title=numpy.ndarray><span class=n>axes</span></a><span class=p>[</span><span class=nb>int</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/functions.html#int title=builtins.int><span class=n>midx</span></a> <span class=o>/</span> <span class=mi>5</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/functions.html#int title=builtins.int><span class=n>midx</span></a> <span class=o>%</span> <span class=mi>5</span><span class=p>)],</span>
                              <span class=n>plot_abs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>display_mode</span><span class=o>=</span><span class=s1>'x'</span><span class=p>)</span>
<a class="sphx-glr-backref-module-matplotlib-figure sphx-glr-backref-type-py-method"href=https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.suptitle title=matplotlib.figure.Figure.suptitle><span class=n>fig</span><span class=o>.</span><span class=n>suptitle</span></a><span class=p>(</span><span class=s1>'subjects z_map language network (unc p&LT0.001)'</span><span class=p>)</span>
<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.show.html#nilearn.plotting.show title=nilearn.plotting.show><span class=n>plotting</span><span class=o>.</span><span class=n>show</span></a><span class=p>()</span>
</pre></div></div><img alt="subjects z_map language network (unc p<0.001)"class=sphx-glr-single-img src=../../_images/sphx_glr_plot_bids_analysis_001.png srcset=../../_images/sphx_glr_plot_bids_analysis_001.png></div><div class=section id=second-level-model-estimation><h2><a class=toc-backref href=#id5><span class=section-number>9.8.3.5. </span>Second level model estimation</a><a title="Permalink to this headline"class=headerlink href=#second-level-model-estimation>¶</a></h2><p>We just have to provide the list of fitted FirstLevelModel objects to the SecondLevelModel object for estimation. We can do this because all subjects share a similar design matrix (same variables reflected in column names).</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.glm.second_level</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-class"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel title=nilearn.glm.second_level.SecondLevelModel><span class=n>SecondLevelModel</span></a>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>second_level_input</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>models</span></a>
</pre></div></div><p>Note that we apply a smoothing of 8mm.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel title=nilearn.glm.second_level.SecondLevelModel><span class=n>second_level_model</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-class"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel title=nilearn.glm.second_level.SecondLevelModel><span class=n>SecondLevelModel</span></a><span class=p>(</span><span class=n>smoothing_fwhm</span><span class=o>=</span><span class=mf>8.0</span><span class=p>)</span>
<a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel title=nilearn.glm.second_level.SecondLevelModel><span class=n>second_level_model</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel.fit title=nilearn.glm.second_level.SecondLevelModel.fit><span class=n>second_level_model</span><span class=o>.</span><span class=n>fit</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.8/library/stdtypes.html#list title=builtins.list><span class=n>second_level_input</span></a><span class=p>)</span>
</pre></div></div><p>Computing contrasts at the second level is as simple as at the first level. Since we are not providing confounders we are performing a one-sample test at the second level with the images determined by the specified first level contrast.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>zmap</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-second_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html#nilearn.glm.second_level.SecondLevelModel.compute_contrast title=nilearn.glm.second_level.SecondLevelModel.compute_contrast><span class=n>second_level_model</span><span class=o>.</span><span class=n>compute_contrast</span></a><span class=p>(</span>
    <span class=n>first_level_contrast</span><span class=o>=</span><span class=s1>'language-string'</span><span class=p>)</span>
</pre></div></div><p>The group level contrast reveals a left lateralized fronto-temporal language network.</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html#nilearn.plotting.plot_glass_brain title=nilearn.plotting.plot_glass_brain><span class=n>plotting</span><span class=o>.</span><span class=n>plot_glass_brain</span></a><span class=p>(</span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>zmap</span></a><span class=p>,</span> <span class=n>colorbar</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>threshold</span><span class=o>=</span><a class="sphx-glr-backref-module-numpy sphx-glr-backref-type-py-attribute"href=https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float64 title=numpy.float64><span class=n>p001_unc</span></a><span class=p>,</span>
                          <span class=n>title</span><span class=o>=</span><span class=s1>'Group language network (unc p&LT0.001)'</span><span class=p>,</span>
                          <span class=n>plot_abs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>display_mode</span><span class=o>=</span><span class=s1>'x'</span><span class=p>)</span>
<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.show.html#nilearn.plotting.show title=nilearn.plotting.show><span class=n>plotting</span><span class=o>.</span><span class=n>show</span></a><span class=p>()</span>
</pre></div></div><img alt="plot bids analysis"class=sphx-glr-single-img src=../../_images/sphx_glr_plot_bids_analysis_002.png srcset=../../_images/sphx_glr_plot_bids_analysis_002.png><p class=sphx-glr-timing><strong>Total running time of the script:</strong> ( 0 minutes 59.351 seconds)</p><p><strong>Estimated memory usage:</strong> 129 MB</p><div class="sphx-glr-footer class sphx-glr-footer-example docutils container"id=sphx-glr-download-auto-examples-07-advanced-plot-bids-analysis-py><div class="binder-badge docutils container"><a class="reference external image-reference"href=https://mybinder.org/v2/gh/nilearn/nilearn.github.io/main?filepath=examples/auto_examples/07_advanced/plot_bids_analysis.ipynb><img alt="Launch binder"src=../../_images/binder_badge_logo6.svg width=150px></a></div><div class="sphx-glr-download sphx-glr-download-python docutils container"><p><a class="reference download internal"download href=../../_downloads/e28cbb831d2745bebbf5ca0d60a91b94/plot_bids_analysis.py><code class="xref download docutils literal notranslate"><span class=pre>Download</span> <span class=pre>Python</span> <span class=pre>source</span> <span class=pre>code:</span> <span class=pre>plot_bids_analysis.py</span></code></a></p></div><div class="sphx-glr-download sphx-glr-download-jupyter docutils container"><p><a class="reference download internal"download href=../../_downloads/9ebd34c7329b207c5ba8e2082afa30a0/plot_bids_analysis.ipynb><code class="xref download docutils literal notranslate"><span class=pre>Download</span> <span class=pre>Jupyter</span> <span class=pre>notebook:</span> <span class=pre>plot_bids_analysis.ipynb</span></code></a></p></div></div><p class=sphx-glr-signature><a class="reference external"href=https://sphinx-gallery.github.io>Gallery generated by Sphinx-Gallery</a></p></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>9.8.3. BIDS dataset first and second level analysis</a><ul><li><a class="reference internal"href=#fetch-example-bids-dataset>9.8.3.1. Fetch example BIDS dataset</a></li><li><a class="reference internal"href=#obtain-automatically-firstlevelmodel-objects-and-fit-arguments>9.8.3.2. Obtain automatically FirstLevelModel objects and fit arguments</a></li><li><a class="reference internal"href=#quick-sanity-check-on-fit-arguments>9.8.3.3. Quick sanity check on fit arguments</a></li><li><a class="reference internal"href=#first-level-model-estimation>9.8.3.4. First level model estimation</a></li><li><a class="reference internal"href=#second-level-model-estimation>9.8.3.5. Second level model estimation</a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=plot_localizer_simple_analysis.html><span class=section-number>9.8.2. </span>Massively univariate analysis of a calculation task from the Localizer dataset</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=plot_age_group_prediction_cross_val.html><span class=section-number>9.8.4. </span>Functional connectivity predicts age group</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../../_sources/auto_examples/07_advanced/plot_bids_analysis.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>