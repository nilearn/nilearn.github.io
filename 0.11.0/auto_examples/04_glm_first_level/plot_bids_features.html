<!doctypehtml><html class=no-js data-content_root=../../ lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><meta content="light dark"name=color-scheme><meta content=width=device-width,initial-scale=1 name=viewport><meta content="First level analysis of a complete BIDS dataset from openneuro"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/auto_examples/04_glm_first_level/plot_bids_features.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Full step-by-step example of fitting a GLM to perform a first level analysis in an openneuro BIDS dataset. We demonstrate how BIDS derivatives can be exploited to perform a simple one subject analy..."property=og:description><meta content=https://nilearn.github.io/_static/nilearn-logo.png property=og:image><meta content=Nilearn property=og:image:alt><meta content="Full step-by-step example of fitting a GLM to perform a first level analysis in an openneuro BIDS dataset. We demonstrate how BIDS derivatives can be exploited to perform a simple one subject analy..."name=description><link href=../../search.html rel=search title=Search><link title="Simple example of two-runs fMRI model fitting"href=plot_two_runs_model.html rel=next><link title="Examples of design matrices"href=plot_design_matrix.html rel=prev><link rel="shortcut icon"href=../../_static/favicon.ico><title>First level analysis of a complete BIDS dataset from openneuro - Nilearn</title><link href=../../_static/pygments.css?v=045299b1 rel=stylesheet><link href=../../_static/styles/furo.css?v=354aac6f rel=stylesheet><link href=../../_static/copybutton.css?v=76b2166b rel=stylesheet><link href=../../_static/sg_gallery.css?v=d2d258e8 rel=stylesheet><link href=../../_static/sg_gallery-binder.css?v=f4aeca0c rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css?v=2082cf3c rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css?v=1277b6f3 rel=stylesheet><link href=../../_static/sphinx-design.min.css?v=95c83b7e rel=stylesheet><link href=../../_static/styles/furo-extensions.css?v=302659d7 rel=stylesheet><link href=../../_static/custom.css?v=07d12563 rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/fontawesome.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/solid.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/brands.min.css rel=stylesheet><style>body{--color-code-background:#fff;--color-code-foreground:black;--admonition-font-size:100%;--admonition-title-font-size:100%;--color-announcement-background:#fbb360;--color-announcement-text:#111418;--color-admonition-title--note:#448aff;--color-admonition-title-background--note:#448aff10}@media not print{body[data-theme=dark]{--color-code-background:#232629;--color-code-foreground:#ccc;--color-announcement-background:#935610;--color-announcement-text:#fff}@media (prefers-color-scheme:dark){body:not([data-theme=light]){--color-code-background:#232629;--color-code-foreground:#ccc;--color-announcement-background:#935610;--color-announcement-text:#fff}}}</style><body><script>document.body.dataset.theme = localStorage.getItem("theme") || "auto";</script><svg style=display:none xmlns=http://www.w3.org/2000/svg><symbol viewbox="0 0 24 24"id=svg-toc><title>Contents</title><svg viewbox="0 0 1024 1024"fill=currentColor stroke=currentColor stroke-width=0><path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/></svg></symbol><symbol viewbox="0 0 24 24"id=svg-menu><title>Menu</title><svg viewbox="0 0 24 24"class=feather-menu fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 xmlns=http://www.w3.org/2000/svg><line x1=3 x2=21 y1=12 y2=12></line><line x1=3 x2=21 y1=6 y2=6></line><line x1=3 x2=21 y1=18 y2=18></line></svg></symbol><symbol viewbox="0 0 24 24"id=svg-arrow-right><title>Expand</title><svg viewbox="0 0 24 24"class=feather-chevron-right fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 xmlns=http://www.w3.org/2000/svg><polyline points="9 18 15 12 9 6"></polyline></svg></symbol><symbol viewbox="0 0 24 24"id=svg-sun><title>Light mode</title><svg viewbox="0 0 24 24"class=feather-sun fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><circle cx=12 cy=12 r=5></circle><line x1=12 x2=12 y1=1 y2=3></line><line x1=12 x2=12 y1=21 y2=23></line><line x1=4.22 x2=5.64 y1=4.22 y2=5.64></line><line x1=18.36 x2=19.78 y1=18.36 y2=19.78></line><line x1=1 x2=3 y1=12 y2=12></line><line x1=21 x2=23 y1=12 y2=12></line><line x1=4.22 x2=5.64 y1=19.78 y2=18.36></line><line x1=18.36 x2=19.78 y1=5.64 y2=4.22></line></svg></symbol><symbol viewbox="0 0 24 24"id=svg-moon><title>Dark mode</title><svg viewbox="0 0 24 24"class=icon-tabler-moon fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0z"fill=none stroke=none /><path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"/></svg></symbol><symbol viewbox="0 0 24 24"id=svg-sun-with-moon><title>Auto light/dark, in light mode</title><svg viewbox="0 0 24 24"class=icon-custom-derived-from-feather-sun-and-tabler-moon fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><path d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"style=opacity:.5 /><line x1=14.5 x2=14.5 y1=3.25 y2=1.25 /><line x1=14.5 x2=14.5 y1=15.85 y2=17.85 /><line x1=10.044 x2=8.63 y1=5.094 y2=3.68 /><line x1=19 x2=20.414 y1=14.05 y2=15.464 /><line x1=8.2 x2=6.2 y1=9.55 y2=9.55 /><line x1=20.8 x2=22.8 y1=9.55 y2=9.55 /><line x1=10.044 x2=8.63 y1=14.006 y2=15.42 /><line x1=19 x2=20.414 y1=5.05 y2=3.636 /><circle cx=14.5 cy=9.55 r=3.6 /></svg></symbol><symbol viewbox="0 0 24 24"id=svg-moon-with-sun><title>Auto light/dark, in dark mode</title><svg viewbox="0 0 24 24"class=icon-custom-derived-from-feather-sun-and-tabler-moon fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/><line style=opacity:.5 x1=18 x2=18 y1=3.705 y2=2.5 /><line style=opacity:.5 x1=18 x2=18 y1=11.295 y2=12.5 /><line style=opacity:.5 x1=15.316 x2=14.464 y1=4.816 y2=3.964 /><line style=opacity:.5 x1=20.711 x2=21.563 y1=10.212 y2=11.063 /><line style=opacity:.5 x1=14.205 x2=13.001 y1=7.5 y2=7.5 /><line style=opacity:.5 x1=21.795 x2=23 y1=7.5 y2=7.5 /><line style=opacity:.5 x1=15.316 x2=14.464 y1=10.184 y2=11.036 /><line style=opacity:.5 x1=20.711 x2=21.563 y1=4.789 y2=3.937 /><circle cx=18 cy=7.5 r=2.169 style=opacity:.5 /></svg></symbol><symbol viewbox="0 0 24 24"id=svg-pencil><svg viewbox="0 0 24 24"class=icon-tabler-pencil-code fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4"/><path d="M13.5 6.5l4 4"/><path d="M20 21l2 -2l-2 -2"/><path d="M17 17l-2 2l2 2"/></svg></symbol><symbol viewbox="0 0 24 24"id=svg-eye><svg viewbox="0 0 24 24"class=icon-tabler-eye-code fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1 xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0z"fill=none stroke=none /><path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0"/><path d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008"/><path d="M20 21l2 -2l-2 -2"/><path d="M17 17l-2 2l2 2"/></svg></symbol></svg><input class=sidebar-toggle id=__navigation name=__navigation type=checkbox><input class=sidebar-toggle id=__toc name=__toc type=checkbox><label class="overlay sidebar-overlay"for=__navigation><div class=visually-hidden>Hide navigation sidebar</div></label><label class="overlay toc-overlay"for=__toc><div class=visually-hidden>Hide table of contents sidebar</div></label><a class="skip-to-content muted-link"href=#furo-main-content>Skip to content</a><div class=page><header class=mobile-header><div class=header-left><label class=nav-overlay-icon for=__navigation><div class=visually-hidden>Toggle site navigation sidebar</div> <i class=icon><svg><use href=#svg-menu></use></svg></i></label></div><div class=header-center><a href=../../index.html><div class=brand>Nilearn</div></a></div><div class=header-right><div class="theme-toggle-container theme-toggle-header"><button class=theme-toggle><div class=visually-hidden>Toggle Light / Dark / Auto color theme</div> <svg class=theme-icon-when-auto-light><use href=#svg-sun-with-moon></use></svg> <svg class=theme-icon-when-auto-dark><use href=#svg-moon-with-sun></use></svg> <svg class=theme-icon-when-dark><use href=#svg-moon></use></svg> <svg class=theme-icon-when-light><use href=#svg-sun></use></svg></button></div><label class="toc-overlay-icon toc-header-icon"for=__toc><div class=visually-hidden>Toggle table of contents sidebar</div> <i class=icon><svg><use href=#svg-toc></use></svg></i></label></div></header><aside class=sidebar-drawer><div class=sidebar-container><div class=sidebar-sticky><a class=sidebar-brand href=../../index.html> <div class=sidebar-logo-container><img alt=Logo class=sidebar-logo src=../../_static/nilearn-transparent.png></div> <span class=sidebar-brand-text>Nilearn</span> </a><form action=../../search.html class=sidebar-search-container role=search><input aria-label=Search class=sidebar-search name=q placeholder=Search><input name=check_keywords type=hidden value=yes><input name=area type=hidden value=default></form><div id=searchbox></div><div class=sidebar-scroll><div class=sidebar-tree><ul class=current><li class=toctree-l1><a class="reference internal"href=../../quickstart.html>Quickstart</a></li><li class="toctree-l1 current has-children"><a class="reference internal"href=../index.html>Examples</a><input checked class=toctree-checkbox id=toctree-checkbox-1 name=toctree-checkbox-1 role=switch type=checkbox><label for=toctree-checkbox-1><div class=visually-hidden>Toggle navigation of Examples</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul class=current><li class="toctree-l2 has-children"><a class="reference internal"href=../00_tutorials/index.html>Basic tutorials</a><input class=toctree-checkbox id=toctree-checkbox-2 name=toctree-checkbox-2 role=switch type=checkbox><label for=toctree-checkbox-2><div class=visually-hidden>Toggle navigation of Basic tutorials</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_python_101.html>Basic numerics and plotting with Python</a></li><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_3d_and_4d_niimg.html>3D and 4D niimgs: handling and visualizing</a></li><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_nilearn_101.html>Basic nilearn example: manipulating and looking at data</a></li><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_surface_101.html>Working with Surface images</a></li><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_decoding_tutorial.html>A introduction tutorial to fMRI decoding</a></li><li class=toctree-l3><a class="reference internal"href=../00_tutorials/plot_single_subject_single_run.html>Intro to GLM Analysis: a single-run, single-subject fMRI dataset</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../01_plotting/index.html>Visualization of brain images</a><input class=toctree-checkbox id=toctree-checkbox-3 name=toctree-checkbox-3 role=switch type=checkbox><label for=toctree-checkbox-3><div class=visually-hidden>Toggle navigation of Visualization of brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_demo_glass_brain.html>Glass brain plotting in nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_visualize_megatrawls_netmats.html>Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_prob_atlas.html>Visualizing 4D probabilistic atlas maps</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_atlas.html>Basic Atlas plotting</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_overlay.html>Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_dim_plotting.html>Controlling the contrast of the background when plotting</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_multiscale_parcellations.html>Visualizing multiscale functional brain parcellations</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_colormaps.html>Matplotlib colormaps in Nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_visualization.html>NeuroImaging volumes visualization</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_carpet.html>Visualizing global patterns with a carpet plot</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_haxby_masks.html>Plot Haxby masks</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_surface_projection_strategies.html>Technical point: Illustration of the volume to surface sampling schemes</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_demo_plotting.html>Plotting tools in nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_surf_atlas.html>Loading and plotting of a cortical surface atlas</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_demo_more_plotting.html>More plotting tools from nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_demo_glass_brain_extensive.html>Glass brain plotting in nilearn (all options)</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_surf_stat_map.html>Seed-based connectivity on the surface</a></li><li class=toctree-l3><a class="reference internal"href=../01_plotting/plot_3d_map_to_surface_projection.html>Making a surface plot of a 3D statistical map</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../02_decoding/index.html>Decoding and predicting from brain images</a><input class=toctree-checkbox id=toctree-checkbox-4 name=toctree-checkbox-4 role=switch type=checkbox><label for=toctree-checkbox-4><div class=visually-hidden>Toggle navigation of Decoding and predicting from brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_stimuli.html>Show stimuli of Haxby et al. dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_mixed_gambles_frem.html>FREM on Jimura et al “mixed gambles” dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_oasis_vbm_space_net.html>Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_frem.html>Decoding with FREM: face vs house vs chair object recognition</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_anova_svm.html>Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_multiclass.html>The haxby dataset: different multi-class strategies</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_searchlight_surface.html>Cortical surface-based searchlight decoding</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_searchlight.html>Searchlight analysis of face vs house recognition</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_glm_decoding.html>Decoding of a dataset after GLM fit for signal extraction</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_full_analysis.html>ROI-based decoding analysis in Haxby et al. dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_grid_search.html>Setting a parameter by cross-validation</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_oasis_vbm.html>Voxel-Based Morphometry on Oasis dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_different_estimators.html>Different classifiers in decoding the Haxby dataset</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_haxby_understand_decoder.html>Understanding <code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.decoding.Decoder</span></code></a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_simulated_data.html>Example of pattern recognition on simulated data</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_miyawaki_encoding.html>Encoding models for visual stimuli from Miyawaki et al. 2008</a></li><li class=toctree-l3><a class="reference internal"href=../02_decoding/plot_miyawaki_reconstruction.html>Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../03_connectivity/index.html>Functional connectivity</a><input class=toctree-checkbox id=toctree-checkbox-5 name=toctree-checkbox-5 role=switch type=checkbox><label for=toctree-checkbox-5><div class=visually-hidden>Toggle navigation of Functional connectivity</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_inverse_covariance_connectome.html>Computing a connectome with sparse inverse covariance</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_probabilistic_atlas_extraction.html>Extracting signals of a probabilistic atlas of functional regions</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_simulated_connectome.html>Connectivity structure estimation on simulated data</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_multi_subject_connectome.html>Group Sparse inverse covariance for multi-subject connectome</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_compare_decomposition.html>Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_seed_to_voxel_correlation.html>Producing single subject maps of seed-to-voxel correlation</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_extract_regions_dictlearning_maps.html>Regions extraction using dictionary learning and functional connectomes</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_group_level_connectivity.html>Classification of age groups using functional connectivity</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_signal_extraction.html>Extracting signals from a brain parcellation</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_atlas_comparison.html>Comparing connectomes on different reference atlases</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_sphere_based_connectome.html>Extract signals on spheres and plot a connectome</a></li><li class=toctree-l3><a class="reference internal"href=../03_connectivity/plot_data_driven_parcellations.html>Clustering methods to learn a brain parcellation from fMRI</a></li></ul></li><li class="toctree-l2 current has-children"><a class="reference internal"href=index.html>GLM: First level analysis</a><input checked class=toctree-checkbox id=toctree-checkbox-6 name=toctree-checkbox-6 role=switch type=checkbox><label for=toctree-checkbox-6><div class=visually-hidden>Toggle navigation of GLM: First level analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul class=current><li class=toctree-l3><a class="reference internal"href=plot_adhd_dmn.html>Default Mode Network extraction of ADHD dataset</a></li><li class=toctree-l3><a class="reference internal"href=plot_fir_model.html>Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li><li class=toctree-l3><a class="reference internal"href=plot_localizer_surface_analysis.html>Example of surface-based first-level analysis</a></li><li class=toctree-l3><a class="reference internal"href=plot_write_events_file.html>Generate an events.tsv file for the NeuroSpin localizer task</a></li><li class=toctree-l3><a class="reference internal"href=plot_spm_multimodal_faces.html>Single-subject data (two runs) in native space</a></li><li class=toctree-l3><a class="reference internal"href=plot_hrf.html>Example of MRI response functions</a></li><li class=toctree-l3><a class="reference internal"href=plot_predictions_residuals.html>Predicted time series and residuals</a></li><li class=toctree-l3><a class="reference internal"href=plot_design_matrix.html>Examples of design matrices</a></li><li class="toctree-l3 current current-page"><a class="current reference internal"href=#>First level analysis of a complete BIDS dataset from openneuro</a></li><li class=toctree-l3><a class="reference internal"href=plot_two_runs_model.html>Simple example of two-runs fMRI model fitting</a></li><li class=toctree-l3><a class="reference internal"href=plot_first_level_details.html>Understanding parameters of the first-level model</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../05_glm_second_level/index.html>GLM: Second level analysis</a><input class=toctree-checkbox id=toctree-checkbox-7 name=toctree-checkbox-7 role=switch type=checkbox><label for=toctree-checkbox-7><div class=visually-hidden>Toggle navigation of GLM: Second level analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_second_level_design_matrix.html>Example of second level design matrix</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_proportion_activated_voxels.html>Second-level fMRI model: true positive proportion in clusters</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_thresholding.html>Statistical testing of a second-level analysis</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_second_level_two_sample_test.html>Second-level fMRI model: two-sample test, unpaired and paired</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_oasis.html>Voxel-Based Morphometry on OASIS dataset</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_second_level_one_sample_test.html>Second-level fMRI model: one sample test</a></li><li class=toctree-l3><a class="reference internal"href=../05_glm_second_level/plot_second_level_association_test.html>Example of generic design in second-level models</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../06_manipulating_images/index.html>Manipulating brain image volumes</a><input class=toctree-checkbox id=toctree-checkbox-8 name=toctree-checkbox-8 role=switch type=checkbox><label for=toctree-checkbox-8><div class=visually-hidden>Toggle navigation of Manipulating brain image volumes</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_negate_image.html>Negating an image with math_img</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_compare_mean_image.html>Comparing the means of 2 images</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_smooth_mean_image.html>Smoothing an image</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_extract_regions_labels_image.html>Breaking an atlas of labels in separated regions</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_extract_rois_smith_atlas.html>Regions Extraction of Default Mode Networks using Smith Atlas</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_resample_to_template.html>Resample an image to a template</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_nifti_simple.html>Simple example of NiftiMasker use</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_extract_rois_statistical_maps.html>Region Extraction using a t-statistical map (3D)</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_nifti_labels_simple.html>Extracting signals from brain regions using the NiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_mask_computation.html>Understanding NiftiMasker and mask computation</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_affine_transformation.html>Visualization of affine resamplings</a></li><li class=toctree-l3><a class="reference internal"href=../06_manipulating_images/plot_roi_extraction.html>Computing a Region of Interest (ROI) mask manually</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../07_advanced/index.html>Advanced statistical analysis of brain images</a><input class=toctree-checkbox id=toctree-checkbox-9 name=toctree-checkbox-9 role=switch type=checkbox><label for=toctree-checkbox-9><div class=visually-hidden>Toggle navigation of Advanced statistical analysis of brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_ica_resting_state.html>Multivariate decompositions: Independent component analysis of fMRI</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_localizer_simple_analysis.html>Massively univariate analysis of a calculation task from the Localizer dataset</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_copy_headers_math_img.html>Copying headers from input images with <code class="docutils literal notranslate"><span class=pre>math_img</span></code></a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_bids_analysis.html>BIDS dataset first and second level analysis</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_neurovault_meta_analysis.html>NeuroVault meta-analysis of stop-go paradigm studies</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_surface_bids_analysis.html>Surface-based dataset first and second level analysis of a dataset</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_age_group_prediction_cross_val.html>Functional connectivity predicts age group</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_ica_neurovault.html>NeuroVault cross-study ICA maps</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_localizer_mass_univariate_methods.html>Massively univariate analysis of a motor task from the Localizer dataset</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_surface_image_and_maskers.html>A short demo of the surface images & maskers</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_haxby_mass_univariate.html>Massively univariate analysis of face vs house recognition</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_advanced_decoding_scikit.html>Advanced decoding using scikit learn</a></li><li class=toctree-l3><a class="reference internal"href=../07_advanced/plot_beta_series.html>Beta-Series Modeling for Task-Based Functional Connectivity and Decoding</a></li></ul></li></ul></li><li class="toctree-l1 has-children"><a class="reference internal"href=../../user_guide.html>User guide</a><input class=toctree-checkbox id=toctree-checkbox-10 name=toctree-checkbox-10 role=switch type=checkbox><label for=toctree-checkbox-10><div class=visually-hidden>Toggle navigation of User guide</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l2><a class="reference internal"href=../../introduction.html>1. Introduction</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#what-is-nilearn>2. What is <code class="docutils literal notranslate"><span class=pre>nilearn</span></code>?</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#using-nilearn-for-the-first-time>3. Using <code class="docutils literal notranslate"><span class=pre>nilearn</span></code> for the first time</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#machine-learning-applications-to-neuroimaging>4. Machine learning applications to Neuroimaging</a></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../decoding/index.html>5. Decoding and MVPA: predicting from brain images</a><input class=toctree-checkbox id=toctree-checkbox-11 name=toctree-checkbox-11 role=switch type=checkbox><label for=toctree-checkbox-11><div class=visually-hidden>Toggle navigation of 5. Decoding and MVPA: predicting from brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../decoding/decoding_intro.html>5.1. An introduction to decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/estimator_choice.html>5.2. Choosing the right predictive model for neuroimaging</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/frem.html>5.3. FREM: fast ensembling of regularized models for robust decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/space_net.html>5.4. SpaceNet: decoding with spatial structure for better maps</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/searchlight.html>5.5. Searchlight : finding voxels containing information</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/going_further.html>5.6. Running scikit-learn functions for more control on the analysis</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../connectivity/index.html>6. Functional connectivity and resting state</a><input class=toctree-checkbox id=toctree-checkbox-12 name=toctree-checkbox-12 role=switch type=checkbox><label for=toctree-checkbox-12><div class=visually-hidden>Toggle navigation of 6. Functional connectivity and resting state</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../connectivity/functional_connectomes.html>6.1. Extracting times series to build a functional connectome</a></li><li class="toctree-l3 has-children"><a class="reference internal"href=../../connectivity/connectome_extraction.html>6.2. Connectome extraction: inverse covariance for direct connections</a><input class=toctree-checkbox id=toctree-checkbox-13 name=toctree-checkbox-13 role=switch type=checkbox><label for=toctree-checkbox-13><div class=visually-hidden>Toggle navigation of 6.2. Connectome extraction: inverse covariance for direct connections</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l4><a class="reference internal"href=../../developers/group_sparse_covariance.html>6.2.3.1. Group-sparse covariance estimation</a></li></ul></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/resting_state_networks.html>6.3. Extracting functional brain networks: ICA and related</a></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/region_extraction.html>6.4. Region Extraction for better brain parcellations</a></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/parcellating.html>6.5. Clustering to parcellate the brain in regions</a></li></ul></li><li class=toctree-l2><a class="reference internal"href=../../plotting/index.html>7. Plotting brain images</a></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../glm/index.html>8. Analyzing fMRI using GLMs</a><input class=toctree-checkbox id=toctree-checkbox-14 name=toctree-checkbox-14 role=switch type=checkbox><label for=toctree-checkbox-14><div class=visually-hidden>Toggle navigation of 8. Analyzing fMRI using GLMs</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../glm/glm_intro.html>8.1. An introduction to GLMs in fMRI statistical analysis</a></li><li class=toctree-l3><a class="reference internal"href=../../glm/first_level_model.html>8.2. First level models</a></li><li class=toctree-l3><a class="reference internal"href=../../glm/second_level_model.html>8.3. Second level models</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../manipulating_images/index.html>9. Manipulation brain volumes with nilearn</a><input class=toctree-checkbox id=toctree-checkbox-15 name=toctree-checkbox-15 role=switch type=checkbox><label for=toctree-checkbox-15><div class=visually-hidden>Toggle navigation of 9. Manipulation brain volumes with nilearn</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/input_output.html>9.1. Input and output: neuroimaging data representation</a></li><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/manipulating_images.html>9.2. Manipulating images: resampling, smoothing, masking, ROIs…</a></li><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/masker_objects.html>9.3. From neuroimaging volumes to data matrices: the masker objects</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../building_blocks/index.html>10. Advanced usage: manual pipelines and scaling up</a><input class=toctree-checkbox id=toctree-checkbox-16 name=toctree-checkbox-16 role=switch type=checkbox><label for=toctree-checkbox-16><div class=visually-hidden>Toggle navigation of 10. Advanced usage: manual pipelines and scaling up</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../building_blocks/manual_pipeline.html>10.1. Building your own neuroimaging machine-learning pipeline</a></li><li class=toctree-l3><a class="reference internal"href=../../building_blocks/neurovault.html>10.2. Downloading statistical maps from the Neurovault repository</a></li></ul></li></ul></li><li class="toctree-l1 has-children"><a class="reference internal"href=../../modules/index.html>API References</a><input class=toctree-checkbox id=toctree-checkbox-17 name=toctree-checkbox-17 role=switch type=checkbox><label for=toctree-checkbox-17><div class=visually-hidden>Toggle navigation of API References</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/connectome.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.connectome</span></code>: Functional Connectivity</a><input class=toctree-checkbox id=toctree-checkbox-18 name=toctree-checkbox-18 role=switch type=checkbox><label for=toctree-checkbox-18><div class=visually-hidden>Toggle navigation of nilearn.connectome: Functional Connectivity</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.ConnectivityMeasure.html>nilearn.connectome.ConnectivityMeasure</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.GroupSparseCovariance.html>nilearn.connectome.GroupSparseCovariance</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.GroupSparseCovarianceCV.html>nilearn.connectome.GroupSparseCovarianceCV</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.sym_matrix_to_vec.html>nilearn.connectome.sym_matrix_to_vec</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.vec_to_sym_matrix.html>nilearn.connectome.vec_to_sym_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.group_sparse_covariance.html>nilearn.connectome.group_sparse_covariance</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.cov_to_corr.html>nilearn.connectome.cov_to_corr</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.connectome.prec_to_partial.html>nilearn.connectome.prec_to_partial</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/datasets.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.datasets</span></code>: Automatic Dataset Fetching</a><input class=toctree-checkbox id=toctree-checkbox-19 name=toctree-checkbox-19 role=switch type=checkbox><label for=toctree-checkbox-19><div class=visually-hidden>Toggle navigation of nilearn.datasets: Automatic Dataset Fetching</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_icbm152_2009.html>nilearn.datasets.fetch_icbm152_2009</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_icbm152_brain_gm_mask.html>nilearn.datasets.fetch_icbm152_brain_gm_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_surf_fsaverage.html>nilearn.datasets.fetch_surf_fsaverage</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_fsaverage.html>nilearn.datasets.load_fsaverage</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_fsaverage_data.html>nilearn.datasets.load_fsaverage_data</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_brain_mask.html>nilearn.datasets.load_mni152_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_gm_mask.html>nilearn.datasets.load_mni152_gm_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_gm_template.html>nilearn.datasets.load_mni152_gm_template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_template.html>nilearn.datasets.load_mni152_template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_wm_mask.html>nilearn.datasets.load_mni152_wm_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_mni152_wm_template.html>nilearn.datasets.load_mni152_wm_template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/icbm152_2009.html>ICBM 152 template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fsaverage.html>fsaverage template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fsaverage3.html>fsaverage3 template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fsaverage4.html>fsaverage4 template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fsaverage5.html>fsaverage5 template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fsaverage6.html>fsaverage6 template</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_aal.html>nilearn.datasets.fetch_atlas_aal</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_basc_multiscale_2015.html>nilearn.datasets.fetch_atlas_basc_multiscale_2015</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_destrieux_2009.html>nilearn.datasets.fetch_atlas_destrieux_2009</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html>nilearn.datasets.fetch_atlas_harvard_oxford</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_juelich.html>nilearn.datasets.fetch_atlas_juelich</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_pauli_2017.html>nilearn.datasets.fetch_atlas_pauli_2017</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_schaefer_2018.html>nilearn.datasets.fetch_atlas_schaefer_2018</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_surf_destrieux.html>nilearn.datasets.fetch_atlas_surf_destrieux</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_talairach.html>nilearn.datasets.fetch_atlas_talairach</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_yeo_2011.html>nilearn.datasets.fetch_atlas_yeo_2011</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_coords_dosenbach_2010.html>nilearn.datasets.fetch_coords_dosenbach_2010</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_coords_power_2011.html>nilearn.datasets.fetch_coords_power_2011</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_coords_seitzman_2018.html>nilearn.datasets.fetch_coords_seitzman_2018</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_allen_2011.html>nilearn.datasets.fetch_atlas_allen_2011</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_craddock_2012.html>nilearn.datasets.fetch_atlas_craddock_2012</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_difumo.html>nilearn.datasets.fetch_atlas_difumo</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_harvard_oxford.html>nilearn.datasets.fetch_atlas_harvard_oxford</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_juelich.html>nilearn.datasets.fetch_atlas_juelich</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_msdl.html>nilearn.datasets.fetch_atlas_msdl</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_pauli_2017.html>nilearn.datasets.fetch_atlas_pauli_2017</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_atlas_smith_2009.html>nilearn.datasets.fetch_atlas_smith_2009</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/aal.html>AAL atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/allen_rsn_2011.html>Allen 2011 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/basc_multiscale_2015.html>BASC multiscale atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/craddock_2012.html>Craddock 2012 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/destrieux_surface.html>Destrieux atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/difumo_atlases.html>DiFuMo atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/harvard_oxford.html>Harvard Oxford atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/juelich.html>Juelich atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/msdl_atlas.html>MSDL atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/pauli_2017.html>Pauli 2007 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/schaefer_2018.html>Schaefer 2018 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/smith_2009.html>Smith 2009 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/talairach_atlas.html>Talairach atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/yeo_2011.html>Yeo 2011 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/dosenbach_2010.html>Dosenbach 2010 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/power_2011.html>Power 2011 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/seitzman_2018.html>Seitzman 2018 atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_abide_pcp.html>nilearn.datasets.fetch_abide_pcp</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_adhd.html>nilearn.datasets.fetch_adhd</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_bids_langloc_dataset.html>nilearn.datasets.fetch_bids_langloc_dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_development_fmri.html>nilearn.datasets.fetch_development_fmri</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_ds000030_urls.html>nilearn.datasets.fetch_ds000030_urls</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_fiac_first_level.html>nilearn.datasets.fetch_fiac_first_level</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_haxby.html>nilearn.datasets.fetch_haxby</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_language_localizer_demo_dataset.html>nilearn.datasets.fetch_language_localizer_demo_dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_localizer_first_level.html>nilearn.datasets.fetch_localizer_first_level</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_miyawaki2008.html>nilearn.datasets.fetch_miyawaki2008</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_spm_auditory.html>nilearn.datasets.fetch_spm_auditory</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_spm_multimodal_fmri.html>nilearn.datasets.fetch_spm_multimodal_fmri</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_surf_nki_enhanced.html>nilearn.datasets.fetch_surf_nki_enhanced</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_nki.html>nilearn.datasets.load_nki</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/ABIDE_pcp.html>ABIDE PCP dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/adhd.html>ADHD dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/bids_langloc.html>BIDS language localizer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/development_fmri.html>development fMRI dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/fiac.html>fiac first level dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/haxby2001.html>Haxby dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/language_localizer_demo.html>language localizer demo dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/localizer_first_level.html>localizer first level dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/miyawaki2008.html>Miyawaki 2008 dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/spm_auditory.html>SPM auditory dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/spm_multimodal.html>SPM multimodal dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/nki_enhanced_surface.html>NKI enhanced surface dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/brainomics_localizer.html>Brainomics Localizer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_localizer_button_task.html>nilearn.datasets.fetch_localizer_button_task</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_localizer_calculation_task.html>nilearn.datasets.fetch_localizer_calculation_task</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_localizer_contrasts.html>nilearn.datasets.fetch_localizer_contrasts</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_megatrawls_netmats.html>nilearn.datasets.fetch_megatrawls_netmats</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_mixed_gambles.html>nilearn.datasets.fetch_mixed_gambles</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_oasis_vbm.html>nilearn.datasets.fetch_oasis_vbm</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_neurovault_auditory_computation_task.html>nilearn.datasets.fetch_neurovault_auditory_computation_task</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_neurovault_motor_task.html>nilearn.datasets.fetch_neurovault_motor_task</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/Megatrawls.html>MegaTrawls Network Matrices HCP</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/mixed_gambles.html>Mixed gambles statistical maps</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/oasis1.html>OASIS volume based morphometry maps</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_neurovault.html>nilearn.datasets.fetch_neurovault</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_neurovault_ids.html>nilearn.datasets.fetch_neurovault_ids</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.fetch_openneuro_dataset.html>nilearn.datasets.fetch_openneuro_dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.get_data_dirs.html>nilearn.datasets.get_data_dirs</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.patch_openneuro_dataset.html>nilearn.datasets.patch_openneuro_dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.select_from_index.html>nilearn.datasets.select_from_index</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.datasets.load_sample_motor_activation_image.html>nilearn.datasets.load_sample_motor_activation_image</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/description/neurovault.html>Neurovault statistical maps</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/decoding.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.decoding</span></code>: Decoding</a><input class=toctree-checkbox id=toctree-checkbox-20 name=toctree-checkbox-20 role=switch type=checkbox><label for=toctree-checkbox-20><div class=visually-hidden>Toggle navigation of nilearn.decoding: Decoding</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.Decoder.html>nilearn.decoding.Decoder</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.DecoderRegressor.html>nilearn.decoding.DecoderRegressor</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.FREMClassifier.html>nilearn.decoding.FREMClassifier</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.FREMRegressor.html>nilearn.decoding.FREMRegressor</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.SpaceNetClassifier.html>nilearn.decoding.SpaceNetClassifier</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.SpaceNetRegressor.html>nilearn.decoding.SpaceNetRegressor</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decoding.SearchLight.html>nilearn.decoding.SearchLight</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/decomposition.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.decomposition</span></code>: Multivariate Decompositions</a><input class=toctree-checkbox id=toctree-checkbox-21 name=toctree-checkbox-21 role=switch type=checkbox><label for=toctree-checkbox-21><div class=visually-hidden>Toggle navigation of nilearn.decomposition: Multivariate Decompositions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decomposition.CanICA.html>nilearn.decomposition.CanICA</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.decomposition.DictLearning.html>nilearn.decomposition.DictLearning</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/glm.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.glm</span></code>: Generalized Linear Models</a><input class=toctree-checkbox id=toctree-checkbox-22 name=toctree-checkbox-22 role=switch type=checkbox><label for=toctree-checkbox-22><div class=visually-hidden>Toggle navigation of nilearn.glm: Generalized Linear Models</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.Contrast.html>nilearn.glm.Contrast</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.FContrastResults.html>nilearn.glm.FContrastResults</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.TContrastResults.html>nilearn.glm.TContrastResults</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.ARModel.html>nilearn.glm.ARModel</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.OLSModel.html>nilearn.glm.OLSModel</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.LikelihoodModelResults.html>nilearn.glm.LikelihoodModelResults</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.RegressionResults.html>nilearn.glm.RegressionResults</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.SimpleRegressionResults.html>nilearn.glm.SimpleRegressionResults</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.compute_contrast.html>nilearn.glm.compute_contrast</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.compute_fixed_effects.html>nilearn.glm.compute_fixed_effects</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.expression_to_contrast_vector.html>nilearn.glm.expression_to_contrast_vector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.fdr_threshold.html>nilearn.glm.fdr_threshold</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.cluster_level_inference.html>nilearn.glm.cluster_level_inference</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.threshold_stats_img.html>nilearn.glm.threshold_stats_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html>nilearn.glm.first_level.FirstLevelModel</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.check_design_matrix.html>nilearn.glm.first_level.check_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.compute_regressor.html>nilearn.glm.first_level.compute_regressor</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.first_level_from_bids.html>nilearn.glm.first_level.first_level_from_bids</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.glover_dispersion_derivative.html>nilearn.glm.first_level.glover_dispersion_derivative</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.glover_hrf.html>nilearn.glm.first_level.glover_hrf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.glover_time_derivative.html>nilearn.glm.first_level.glover_time_derivative</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html>nilearn.glm.first_level.make_first_level_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.mean_scaling.html>nilearn.glm.first_level.mean_scaling</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.run_glm.html>nilearn.glm.first_level.run_glm</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.spm_dispersion_derivative.html>nilearn.glm.first_level.spm_dispersion_derivative</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.spm_hrf.html>nilearn.glm.first_level.spm_hrf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.first_level.spm_time_derivative.html>nilearn.glm.first_level.spm_time_derivative</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.second_level.SecondLevelModel.html>nilearn.glm.second_level.SecondLevelModel</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.second_level.make_second_level_design_matrix.html>nilearn.glm.second_level.make_second_level_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.glm.second_level.non_parametric_inference.html>nilearn.glm.second_level.non_parametric_inference</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/image.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.image</span></code>: Image Processing and Resampling Utilities</a><input class=toctree-checkbox id=toctree-checkbox-23 name=toctree-checkbox-23 role=switch type=checkbox><label for=toctree-checkbox-23><div class=visually-hidden>Toggle navigation of nilearn.image: Image Processing and Resampling Utilities</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.binarize_img.html>nilearn.image.binarize_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.clean_img.html>nilearn.image.clean_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.concat_imgs.html>nilearn.image.concat_imgs</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.coord_transform.html>nilearn.image.coord_transform</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.copy_img.html>nilearn.image.copy_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.crop_img.html>nilearn.image.crop_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.get_data.html>nilearn.image.get_data</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.high_variance_confounds.html>nilearn.image.high_variance_confounds</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.index_img.html>nilearn.image.index_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.iter_img.html>nilearn.image.iter_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.largest_connected_component_img.html>nilearn.image.largest_connected_component_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.load_img.html>nilearn.image.load_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.math_img.html>nilearn.image.math_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.mean_img.html>nilearn.image.mean_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.new_img_like.html>nilearn.image.new_img_like</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.resample_img.html>nilearn.image.resample_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.resample_to_img.html>nilearn.image.resample_to_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.reorder_img.html>nilearn.image.reorder_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.smooth_img.html>nilearn.image.smooth_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.swap_img_hemispheres.html>nilearn.image.swap_img_hemispheres</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.image.threshold_img.html>nilearn.image.threshold_img</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/interfaces.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.interfaces</span></code>: Loading components from interfaces</a><input class=toctree-checkbox id=toctree-checkbox-24 name=toctree-checkbox-24 role=switch type=checkbox><label for=toctree-checkbox-24><div class=visually-hidden>Toggle navigation of nilearn.interfaces: Loading components from interfaces</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.bids.get_bids_files.html>nilearn.interfaces.bids.get_bids_files</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.bids.parse_bids_filename.html>nilearn.interfaces.bids.parse_bids_filename</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.bids.save_glm_to_bids.html>nilearn.interfaces.bids.save_glm_to_bids</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.fmriprep.load_confounds.html>nilearn.interfaces.fmriprep.load_confounds</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.fmriprep.load_confounds_strategy.html>nilearn.interfaces.fmriprep.load_confounds_strategy</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.interfaces.fsl.get_design_from_fslmat.html>nilearn.interfaces.fsl.get_design_from_fslmat</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/maskers.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.maskers</span></code>: Extracting Signals from Brain Images</a><input class=toctree-checkbox id=toctree-checkbox-25 name=toctree-checkbox-25 role=switch type=checkbox><label for=toctree-checkbox-25><div class=visually-hidden>Toggle navigation of nilearn.maskers: Extracting Signals from Brain Images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.BaseMasker.html>nilearn.maskers.BaseMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.NiftiMasker.html>nilearn.maskers.NiftiMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.MultiNiftiMasker.html>nilearn.maskers.MultiNiftiMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.NiftiLabelsMasker.html>nilearn.maskers.NiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.MultiNiftiLabelsMasker.html>nilearn.maskers.MultiNiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.NiftiMapsMasker.html>nilearn.maskers.NiftiMapsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.MultiNiftiMapsMasker.html>nilearn.maskers.MultiNiftiMapsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.NiftiSpheresMasker.html>nilearn.maskers.NiftiSpheresMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.SurfaceLabelsMasker.html>nilearn.maskers.SurfaceLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.maskers.SurfaceMasker.html>nilearn.maskers.SurfaceMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated_reports/masker_reports_examples.html>Examples masker reports</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/masking.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.masking</span></code>: Data Masking Utilities</a><input class=toctree-checkbox id=toctree-checkbox-26 name=toctree-checkbox-26 role=switch type=checkbox><label for=toctree-checkbox-26><div class=visually-hidden>Toggle navigation of nilearn.masking: Data Masking Utilities</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_epi_mask.html>nilearn.masking.compute_epi_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_multi_epi_mask.html>nilearn.masking.compute_multi_epi_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_brain_mask.html>nilearn.masking.compute_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_multi_brain_mask.html>nilearn.masking.compute_multi_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_background_mask.html>nilearn.masking.compute_background_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.compute_multi_background_mask.html>nilearn.masking.compute_multi_background_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.intersect_masks.html>nilearn.masking.intersect_masks</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.apply_mask.html>nilearn.masking.apply_mask</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.masking.unmask.html>nilearn.masking.unmask</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/mass_univariate.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.mass_univariate</span></code>: Mass-Univariate Analysis</a><input class=toctree-checkbox id=toctree-checkbox-27 name=toctree-checkbox-27 role=switch type=checkbox><label for=toctree-checkbox-27><div class=visually-hidden>Toggle navigation of nilearn.mass_univariate: Mass-Univariate Analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.mass_univariate.permuted_ols.html>nilearn.mass_univariate.permuted_ols</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/plotting.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.plotting</span></code>: Plotting Brain Data</a><input class=toctree-checkbox id=toctree-checkbox-28 name=toctree-checkbox-28 role=switch type=checkbox><label for=toctree-checkbox-28><div class=visually-hidden>Toggle navigation of nilearn.plotting: Plotting Brain Data</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.find_cut_slices.html>nilearn.plotting.find_cut_slices</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.find_xyz_cut_coords.html>nilearn.plotting.find_xyz_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.find_parcellation_cut_coords.html>nilearn.plotting.find_parcellation_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.find_probabilistic_atlas_cut_coords.html>nilearn.plotting.find_probabilistic_atlas_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_anat.html>nilearn.plotting.plot_anat</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_img.html>nilearn.plotting.plot_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_epi.html>nilearn.plotting.plot_epi</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_matrix.html>nilearn.plotting.plot_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_roi.html>nilearn.plotting.plot_roi</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_stat_map.html>nilearn.plotting.plot_stat_map</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html>nilearn.plotting.plot_glass_brain</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_connectome.html>nilearn.plotting.plot_connectome</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_markers.html>nilearn.plotting.plot_markers</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_prob_atlas.html>nilearn.plotting.plot_prob_atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_carpet.html>nilearn.plotting.plot_carpet</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_surf.html>nilearn.plotting.plot_surf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_surf_roi.html>nilearn.plotting.plot_surf_roi</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_surf_contours.html>nilearn.plotting.plot_surf_contours</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_surf_stat_map.html>nilearn.plotting.plot_surf_stat_map</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_img_on_surf.html>nilearn.plotting.plot_img_on_surf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_img_comparison.html>nilearn.plotting.plot_img_comparison</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_design_matrix.html>nilearn.plotting.plot_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_design_matrix_correlation.html>nilearn.plotting.plot_design_matrix_correlation</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_event.html>nilearn.plotting.plot_event</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.plot_contrast_matrix.html>nilearn.plotting.plot_contrast_matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.view_surf.html>nilearn.plotting.view_surf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.view_img_on_surf.html>nilearn.plotting.view_img_on_surf</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.view_connectome.html>nilearn.plotting.view_connectome</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.view_markers.html>nilearn.plotting.view_markers</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.view_img.html>nilearn.plotting.view_img</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.show.html>nilearn.plotting.show</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.OrthoProjector.html>nilearn.plotting.displays.OrthoProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.XZProjector.html>nilearn.plotting.displays.XZProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YZProjector.html>nilearn.plotting.displays.YZProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YXProjector.html>nilearn.plotting.displays.YXProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.XProjector.html>nilearn.plotting.displays.XProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YProjector.html>nilearn.plotting.displays.YProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.ZProjector.html>nilearn.plotting.displays.ZProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LZRYProjector.html>nilearn.plotting.displays.LZRYProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LYRZProjector.html>nilearn.plotting.displays.LYRZProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LYRProjector.html>nilearn.plotting.displays.LYRProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LZRProjector.html>nilearn.plotting.displays.LZRProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LRProjector.html>nilearn.plotting.displays.LRProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.LProjector.html>nilearn.plotting.displays.LProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.RProjector.html>nilearn.plotting.displays.RProjector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.BaseAxes.html>nilearn.plotting.displays.BaseAxes</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.CutAxes.html>nilearn.plotting.displays.CutAxes</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.GlassBrainAxes.html>nilearn.plotting.displays.GlassBrainAxes</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.BaseSlicer.html>nilearn.plotting.displays.BaseSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.OrthoSlicer.html>nilearn.plotting.displays.OrthoSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.PlotlySurfaceFigure.html>nilearn.plotting.displays.PlotlySurfaceFigure</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.TiledSlicer.html>nilearn.plotting.displays.TiledSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.MosaicSlicer.html>nilearn.plotting.displays.MosaicSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.XZSlicer.html>nilearn.plotting.displays.XZSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YZSlicer.html>nilearn.plotting.displays.YZSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YXSlicer.html>nilearn.plotting.displays.YXSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.XSlicer.html>nilearn.plotting.displays.XSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.YSlicer.html>nilearn.plotting.displays.YSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.ZSlicer.html>nilearn.plotting.displays.ZSlicer</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.get_projector.html>nilearn.plotting.displays.get_projector</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.plotting.displays.get_slicer.html>nilearn.plotting.displays.get_slicer</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/regions.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.regions</span></code>: Operating on Regions</a><input class=toctree-checkbox id=toctree-checkbox-29 name=toctree-checkbox-29 role=switch type=checkbox><label for=toctree-checkbox-29><div class=visually-hidden>Toggle navigation of nilearn.regions: Operating on Regions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.RegionExtractor.html>nilearn.regions.RegionExtractor</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.Parcellations.html>nilearn.regions.Parcellations</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.ReNA.html>nilearn.regions.ReNA</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.HierarchicalKMeans.html>nilearn.regions.HierarchicalKMeans</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.connected_regions.html>nilearn.regions.connected_regions</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.connected_label_regions.html>nilearn.regions.connected_label_regions</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.img_to_signals_labels.html>nilearn.regions.img_to_signals_labels</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.signals_to_img_labels.html>nilearn.regions.signals_to_img_labels</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.img_to_signals_maps.html>nilearn.regions.img_to_signals_maps</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.signals_to_img_maps.html>nilearn.regions.signals_to_img_maps</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.regions.recursive_neighbor_agglomeration.html>nilearn.regions.recursive_neighbor_agglomeration</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/reporting.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.reporting</span></code>: Reporting Functions</a><input class=toctree-checkbox id=toctree-checkbox-30 name=toctree-checkbox-30 role=switch type=checkbox><label for=toctree-checkbox-30><div class=visually-hidden>Toggle navigation of nilearn.reporting: Reporting Functions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.reporting.HTMLReport.html>nilearn.reporting.HTMLReport</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.reporting.get_clusters_table.html>nilearn.reporting.get_clusters_table</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.reporting.make_glm_report.html>nilearn.reporting.make_glm_report</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated_reports/glm_reports_examples.html>Examples of GLM reports</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/signal.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.signal</span></code>: Preprocessing Time Series</a><input class=toctree-checkbox id=toctree-checkbox-31 name=toctree-checkbox-31 role=switch type=checkbox><label for=toctree-checkbox-31><div class=visually-hidden>Toggle navigation of nilearn.signal: Preprocessing Time Series</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.signal.butterworth.html>nilearn.signal.butterworth</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.signal.clean.html>nilearn.signal.clean</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.signal.high_variance_confounds.html>nilearn.signal.high_variance_confounds</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../modules/surface.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.surface</span></code>: Manipulating Surface Data</a><input class=toctree-checkbox id=toctree-checkbox-32 name=toctree-checkbox-32 role=switch type=checkbox><label for=toctree-checkbox-32><div class=visually-hidden>Toggle navigation of nilearn.surface: Manipulating Surface Data</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.FileMesh.html>nilearn.surface.FileMesh</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.InMemoryMesh.html>nilearn.surface.InMemoryMesh</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.PolyData.html>nilearn.surface.PolyData</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.PolyMesh.html>nilearn.surface.PolyMesh</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.SurfaceImage.html>nilearn.surface.SurfaceImage</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.SurfaceMesh.html>nilearn.surface.SurfaceMesh</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.load_surf_data.html>nilearn.surface.load_surf_data</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.load_surf_mesh.html>nilearn.surface.load_surf_mesh</a></li><li class=toctree-l3><a class="reference internal"href=../../modules/generated/nilearn.surface.vol_to_surf.html>nilearn.surface.vol_to_surf</a></li></ul></li></ul></li><li class=toctree-l1><a class="reference internal"href=../../glossary.html>Glossary</a></li></ul><p class=caption role=heading><span class=caption-text>Development</span></p><ul><li class=toctree-l1><a class="reference internal"href=../../development.html>Contributing</a></li><li class=toctree-l1><a class="reference internal"href=../../maintenance.html>Maintenance</a></li><li class=toctree-l1><a class="reference internal"href=../../changes/whats_new.html>What’s new</a></li><li class=toctree-l1><a class="reference internal"href=../../authors.html>Team</a></li><li class=toctree-l1><a class="reference external"href=https://github.com/nilearn/nilearn>GitHub Repository</a></li></ul></div></div></div></div></aside><div class=main><div class=content><div class=article-container><a class="back-to-top muted-link"href=#> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path></svg> <span>Back to top</span> </a><div class=content-icon-container><div class=view-this-page><a title="View this page"class=muted-link href=https://github.com/nilearn/nilearn/blob/main/doc/auto_examples/04_glm_first_level/plot_bids_features.rst?plain=true> <svg><use href=#svg-eye></use></svg> <span class=visually-hidden>View this page</span> </a></div><div class=edit-this-page><a title="Edit this page"class=muted-link href=https://github.com/nilearn/nilearn/edit/main/examples/04_glm_first_level/plot_bids_features.py target=_blank> <svg viewbox="0 0 24 24"aria-hidden=true fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1.5><path d="M0 0h24v24H0z"fill=none stroke=none /><path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4"/><line x1=13.5 x2=17.5 y1=6.5 y2=10.5 /></svg> <span class=visually-hidden>Edit this page</span> </a></div><div class="theme-toggle-container theme-toggle-content"><button class=theme-toggle><div class=visually-hidden>Toggle Light / Dark / Auto color theme</div> <svg class=theme-icon-when-auto-light><use href=#svg-sun-with-moon></use></svg> <svg class=theme-icon-when-auto-dark><use href=#svg-moon-with-sun></use></svg> <svg class=theme-icon-when-dark><use href=#svg-moon></use></svg> <svg class=theme-icon-when-light><use href=#svg-sun></use></svg></button></div><label class="toc-overlay-icon toc-content-icon"for=__toc><div class=visually-hidden>Toggle table of contents sidebar</div> <i class=icon><svg><use href=#svg-toc></use></svg></i></label></div><article id=furo-main-content role=main><div class="sphx-glr-download-link-note admonition note"><p class=admonition-title>Note</p><p><a class="reference internal"href=#sphx-glr-download-auto-examples-04-glm-first-level-plot-bids-features-py><span class="std std-ref">Go to the end</span></a> to download the full example code. or to run this example in your browser via Binder</p></div><section class=sphx-glr-example-title id=first-level-analysis-of-a-complete-bids-dataset-from-openneuro><span id=sphx-glr-auto-examples-04-glm-first-level-plot-bids-features-py></span><h1>First level analysis of a complete BIDS dataset from openneuro<a title="Link to this heading"class=headerlink href=#first-level-analysis-of-a-complete-bids-dataset-from-openneuro>¶</a></h1><p>Full step-by-step example of fitting a <a class="reference internal"href=../../glossary.html#term-GLM><span class="xref std std-term">GLM</span></a> to perform a first level analysis in an openneuro <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset. We demonstrate how <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> derivatives can be exploited to perform a simple one subject analysis with minimal code. Details about the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> standard are available at <a class="reference external"href=https://bids.neuroimaging.io/>https://bids.neuroimaging.io/</a>. We also demonstrate how to download individual groups of files from the Openneuro s3 bucket.</p><p>More specifically:</p><ol class="arabic simple"><li><p>Download an <a class="reference internal"href=../../glossary.html#term-fMRI><span class="xref std std-term">fMRI</span></a> <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset with derivatives from openneuro.</p></li><li><p>Extract first level model objects automatically from the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset.</p></li><li><p>Demonstrate Quality assurance of Nilearn estimation against available FSL. estimation in the openneuro dataset.</p></li><li><p>Display contrast plot and uncorrected first level statistics table report.</p></li></ol><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn</span> <span class=kn>import</span> <span class=n>plotting</span>
</pre></div></div><section id=fetch-openneuro-bids-dataset><h2>Fetch openneuro <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset<a title="Link to this heading"class=headerlink href=#fetch-openneuro-bids-dataset>¶</a></h2><p>We download one subject from the stopsignal task in the ds000030 V4 <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset available in openneuro. This dataset contains the necessary information to run a statistical analysis using Nilearn. The dataset also contains statistical results from a previous FSL analysis that we can employ for comparison with the Nilearn estimation.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.datasets</span> <span class=kn>import</span> <span class=p>(</span>
    <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_ds000030_urls.html#nilearn.datasets.fetch_ds000030_urls title=nilearn.datasets.fetch_ds000030_urls><span class=n>fetch_ds000030_urls</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_openneuro_dataset.html#nilearn.datasets.fetch_openneuro_dataset title=nilearn.datasets.fetch_openneuro_dataset><span class=n>fetch_openneuro_dataset</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.select_from_index.html#nilearn.datasets.select_from_index title=nilearn.datasets.select_from_index><span class=n>select_from_index</span></a><span class=p>,</span>
<span class=p>)</span>

<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>_</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>urls</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_ds000030_urls.html#nilearn.datasets.fetch_ds000030_urls title=nilearn.datasets.fetch_ds000030_urls><span class=n>fetch_ds000030_urls</span></a><span class=p>()</span>

<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>exclusion_patterns</span></a> <span class=o>=</span> <span class=p>[</span>
    <span class=s2>"*group*"</span><span class=p>,</span>
    <span class=s2>"*phenotype*"</span><span class=p>,</span>
    <span class=s2>"*mriqc*"</span><span class=p>,</span>
    <span class=s2>"*parameter_plots*"</span><span class=p>,</span>
    <span class=s2>"*physio_plots*"</span><span class=p>,</span>
    <span class=s2>"*space-fsaverage*"</span><span class=p>,</span>
    <span class=s2>"*space-T1w*"</span><span class=p>,</span>
    <span class=s2>"*dwi*"</span><span class=p>,</span>
    <span class=s2>"*beh*"</span><span class=p>,</span>
    <span class=s2>"*task-bart*"</span><span class=p>,</span>
    <span class=s2>"*task-rest*"</span><span class=p>,</span>
    <span class=s2>"*task-scap*"</span><span class=p>,</span>
    <span class=s2>"*task-task*"</span><span class=p>,</span>
<span class=p>]</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>urls</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.select_from_index.html#nilearn.datasets.select_from_index title=nilearn.datasets.select_from_index><span class=n>select_from_index</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>urls</span></a><span class=p>,</span> <span class=n>exclusion_filters</span><span class=o>=</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>exclusion_patterns</span></a><span class=p>,</span> <span class=n>n_subjects</span><span class=o>=</span><span class=mi>1</span>
<span class=p>)</span>

<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>_</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-datasets sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.datasets.fetch_openneuro_dataset.html#nilearn.datasets.fetch_openneuro_dataset title=nilearn.datasets.fetch_openneuro_dataset><span class=n>fetch_openneuro_dataset</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>urls</span></a><span class=o>=</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>urls</span></a><span class=p>)</span>
</pre></div></div><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>[get_dataset_dir] Dataset found in /home/remi/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed
[get_dataset_dir] Dataset found in /home/remi/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed
</pre></div></div></section><section id=obtain-firstlevelmodel-objects-automatically-and-fit-arguments><h2>Obtain FirstLevelModel objects automatically and fit arguments<a title="Link to this heading"class=headerlink href=#obtain-firstlevelmodel-objects-automatically-and-fit-arguments>¶</a></h2><p>From the dataset directory we automatically obtain FirstLevelModel objects with their subject_id filled from the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset. Moreover we obtain, for each model, the list of run images and their respective events and confound regressors. Those are inferred from the confounds.tsv files available in the <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> dataset. To get the first level models we have to specify the dataset directory, the task_label and the space_label as specified in the file names. We also have to provide the folder with the desired derivatives, that in this case were produced by the <a class="reference internal"href=../../glossary.html#term-fMRIPrep><span class="xref std std-term">fMRIPrep</span></a> <a class="reference internal"href=../../glossary.html#term-BIDS><span class="xref std std-term">BIDS</span></a> app.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.glm.first_level</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.glm.first_level.first_level_from_bids.html#nilearn.glm.first_level.first_level_from_bids title=nilearn.glm.first_level.first_level_from_bids><span class=n>first_level_from_bids</span></a>

<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>task_label</span></a> <span class=o>=</span> <span class=s2>"stopsignal"</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>space_label</span></a> <span class=o>=</span> <span class=s2>"MNI152NLin2009cAsym"</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>derivatives_folder</span></a> <span class=o>=</span> <span class=s2>"derivatives/fmriprep"</span>
<span class=p>(</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_run_imgs</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_events</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_confounds</span></a><span class=p>,</span>
<span class=p>)</span> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.glm.first_level.first_level_from_bids.html#nilearn.glm.first_level.first_level_from_bids title=nilearn.glm.first_level.first_level_from_bids><span class=n>first_level_from_bids</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>task_label</span></a><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>space_label</span></a><span class=p>,</span>
    <span class=n>smoothing_fwhm</span><span class=o>=</span><span class=mf>5.0</span><span class=p>,</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>derivatives_folder</span></a><span class=o>=</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>derivatives_folder</span></a><span class=p>,</span>
    <span class=n>n_jobs</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
<span class=p>)</span>
</pre></div></div><p>Access the model and model arguments of the subject and process events.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel title=nilearn.glm.first_level.FirstLevelModel><span class=n>model</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>imgs</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>events</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>confounds</span></a> <span class=o>=</span> <span class=p>(</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models</span></a><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_run_imgs</span></a><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_events</span></a><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
    <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>models_confounds</span></a><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>subject</span></a> <span class=o>=</span> <span class=sa>f</span><span class=s2>"sub-</span><span class=si>{</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>model</span><span class=o>.</span><span class=n>subject_label</span></a><span class=si>}</span><span class=s2>"</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/functions.html#bool title=builtins.bool><span class=n>model</span><span class=o>.</span><span class=n>minimize_memory</span></a> <span class=o>=</span> <span class=kc>False</span>  <span class=c1># override default</span>

<span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path title=pathlib.Path><span class=n>Path</span></a>

<span class=kn>from</span> <span class=nn>nilearn.interfaces.fsl</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-interfaces-fsl sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.interfaces.fsl.get_design_from_fslmat.html#nilearn.interfaces.fsl.get_design_from_fslmat title=nilearn.interfaces.fsl.get_design_from_fslmat><span class=n>get_design_from_fslmat</span></a>

<a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>fsl_design_matrix_path</span></a> <span class=o>=</span> <span class=p>(</span>
    <a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path title=pathlib.Path><span class=n>Path</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>)</span>
    <span class=o>/</span> <span class=s2>"derivatives"</span>
    <span class=o>/</span> <span class=s2>"task"</span>
    <span class=o>/</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>subject</span></a>
    <span class=o>/</span> <span class=s2>"stopsignal.feat"</span>
    <span class=o>/</span> <span class=s2>"design.mat"</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame title=pandas.core.frame.DataFrame><span class=n>design_matrix</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-interfaces-fsl sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.interfaces.fsl.get_design_from_fslmat.html#nilearn.interfaces.fsl.get_design_from_fslmat title=nilearn.interfaces.fsl.get_design_from_fslmat><span class=n>get_design_from_fslmat</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>fsl_design_matrix_path</span></a><span class=p>,</span> <span class=n>column_names</span><span class=o>=</span><span class=kc>None</span>
<span class=p>)</span>
</pre></div></div><p>We identify the columns of the Go and StopSuccess conditions of the design matrix inferred from the FSL file, to use them later for contrast definition.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>design_columns</span></a> <span class=o>=</span> <span class=p>[</span>
    <span class=sa>f</span><span class=s2>"cond_</span><span class=si>{</span><span class=nb>int</span><span class=p>(</span><span class=n>i</span><span class=p>)</span><span class=si>:</span><span class=s2>02</span><span class=si>}</span><span class=s2>"</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><a class="sphx-glr-backref-module-pandas-core-indexes-base sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index title=pandas.core.indexes.base.Index><span class=n>design_matrix</span><span class=o>.</span><span class=n>columns</span></a><span class=p>))</span>
<span class=p>]</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>design_columns</span></a><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=s2>"Go"</span>
<a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>design_columns</span></a><span class=p>[</span><span class=mi>4</span><span class=p>]</span> <span class=o>=</span> <span class=s2>"StopSuccess"</span>
<a class="sphx-glr-backref-module-pandas-core-indexes-base sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.html#pandas.Index title=pandas.core.indexes.base.Index><span class=n>design_matrix</span><span class=o>.</span><span class=n>columns</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>design_columns</span></a>
</pre></div></div></section><section id=first-level-model-estimation-one-subject><h2>First level model estimation (one subject)<a title="Link to this heading"class=headerlink href=#first-level-model-estimation-one-subject>¶</a></h2><p>We fit the first level model for one subject.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel.fit title=nilearn.glm.first_level.FirstLevelModel.fit><span class=n>model</span><span class=o>.</span><span class=n>fit</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>imgs</span></a><span class=p>,</span> <span class=n>design_matrices</span><span class=o>=</span><span class=p>[</span><a class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame title=pandas.core.frame.DataFrame><span class=n>design_matrix</span></a><span class=p>])</span>
</pre></div></div><div class="output_subarea output_html rendered_html output_result"><style>#sk-container-id-6{--sklearn-color-text:black;--sklearn-color-line:gray;--sklearn-color-unfitted-level-0:#fff5e6;--sklearn-color-unfitted-level-1:#f6e4d2;--sklearn-color-unfitted-level-2:#ffe0b3;--sklearn-color-unfitted-level-3:chocolate;--sklearn-color-fitted-level-0:#f0f8ff;--sklearn-color-fitted-level-1:#d4ebff;--sklearn-color-fitted-level-2:#b3dbfd;--sklearn-color-fitted-level-3:cornflowerblue;--sklearn-color-text-on-default-background:var(--sg-text-color,var(--theme-code-foreground,var(--jp-content-font-color1,black)));--sklearn-color-background:var(--sg-background-color,var(--theme-background,var(--jp-layout-color0,white)));--sklearn-color-border-box:var(--sg-text-color,var(--theme-code-foreground,var(--jp-content-font-color1,black)));--sklearn-color-icon:#696969;@media (prefers-color-scheme:dark){&{--sklearn-color-text-on-default-background:var(--sg-text-color,var(--theme-code-foreground,var(--jp-content-font-color1,white)));--sklearn-color-background:var(--sg-background-color,var(--theme-background,var(--jp-layout-color0,#111)));--sklearn-color-border-box:var(--sg-text-color,var(--theme-code-foreground,var(--jp-content-font-color1,white)));--sklearn-color-icon:#878787}}}#sk-container-id-6{color:var(--sklearn-color-text)}#sk-container-id-6 pre{padding:0}#sk-container-id-6 input.sk-hidden--visually{clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);border:0;width:1px;height:1px;margin:-1px;padding:0;position:absolute;overflow:hidden}#sk-container-id-6 div.sk-dashed-wrapped{border:1px dashed var(--sklearn-color-line);box-sizing:border-box;background-color:var(--sklearn-color-background);margin:0 .4em .5em;padding-bottom:.4em}#sk-container-id-6 div.sk-container{position:relative;display:inline-block!important}#sk-container-id-6 div.sk-text-repr-fallback{display:none}div.sk-parallel-item,div.sk-serial,div.sk-item{background-image:linear-gradient(var(--sklearn-color-text-on-default-background),var(--sklearn-color-text-on-default-background));background-position:50%;background-repeat:no-repeat;background-size:2px 100%}#sk-container-id-6 div.sk-parallel-item:after{content:"";border-bottom:2px solid var(--sklearn-color-text-on-default-background);flex-grow:1;width:100%}#sk-container-id-6 div.sk-parallel{background-color:var(--sklearn-color-background);justify-content:center;align-items:stretch;display:flex;position:relative}#sk-container-id-6 div.sk-parallel-item{flex-direction:column;display:flex}#sk-container-id-6 div.sk-parallel-item:first-child:after{align-self:flex-end;width:50%}#sk-container-id-6 div.sk-parallel-item:last-child:after{align-self:flex-start;width:50%}#sk-container-id-6 div.sk-parallel-item:only-child:after{width:0}#sk-container-id-6 div.sk-serial{background-color:var(--sklearn-color-background);flex-direction:column;align-items:center;padding-left:1em;padding-right:1em;display:flex}#sk-container-id-6 div.sk-toggleable{background-color:var(--sklearn-color-background)}#sk-container-id-6 label.sk-toggleable__label{cursor:pointer;box-sizing:border-box;text-align:center;width:100%;margin-bottom:0;padding:.5em;display:block}#sk-container-id-6 label.sk-toggleable__label-arrow:before{content:"▸";float:left;color:var(--sklearn-color-icon);margin-right:.25em}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before{color:var(--sklearn-color-text)}#sk-container-id-6 div.sk-toggleable__content{text-align:left;background-color:var(--sklearn-color-unfitted-level-0);max-width:0;max-height:0;overflow:hidden}#sk-container-id-6 div.sk-toggleable__content.fitted{background-color:var(--sklearn-color-fitted-level-0)}#sk-container-id-6 div.sk-toggleable__content pre{color:var(--sklearn-color-text);background-color:var(--sklearn-color-unfitted-level-0);border-radius:.25em;margin:.2em}#sk-container-id-6 div.sk-toggleable__content.fitted pre{background-color:var(--sklearn-color-fitted-level-0)}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content{max-width:100%;max-height:200px;overflow:auto}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before{content:"▾"}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label{color:var(--sklearn-color-text);background-color:var(--sklearn-color-unfitted-level-2)}#sk-container-id-6 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label{background-color:var(--sklearn-color-fitted-level-2)}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label{background-color:var(--sklearn-color-unfitted-level-2)}#sk-container-id-6 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label{background-color:var(--sklearn-color-fitted-level-2)}#sk-container-id-6 div.sk-label label.sk-toggleable__label,#sk-container-id-6 div.sk-label label{color:var(--sklearn-color-text-on-default-background)}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label{color:var(--sklearn-color-text);background-color:var(--sklearn-color-unfitted-level-2)}#sk-container-id-6 div.sk-label.fitted:hover label.sk-toggleable__label.fitted{color:var(--sklearn-color-text);background-color:var(--sklearn-color-fitted-level-2)}#sk-container-id-6 div.sk-label label{font-family:monospace;font-weight:700;line-height:1.2em;display:inline-block}#sk-container-id-6 div.sk-label-container{text-align:center}#sk-container-id-6 div.sk-estimator{border:1px dotted var(--sklearn-color-border-box);box-sizing:border-box;background-color:var(--sklearn-color-unfitted-level-0);border-radius:.25em;margin-bottom:.5em;font-family:monospace}#sk-container-id-6 div.sk-estimator.fitted{background-color:var(--sklearn-color-fitted-level-0)}#sk-container-id-6 div.sk-estimator:hover{background-color:var(--sklearn-color-unfitted-level-2)}#sk-container-id-6 div.sk-estimator.fitted:hover{background-color:var(--sklearn-color-fitted-level-2)}.sk-estimator-doc-link,a:link.sk-estimator-doc-link,a:visited.sk-estimator-doc-link{float:right;background-color:var(--sklearn-color-background);border:var(--sklearn-color-unfitted-level-1)1pt solid;color:var(--sklearn-color-unfitted-level-1);border-radius:1em;width:1em;height:1em;margin-left:1ex;font-family:monospace;font-size:smaller;line-height:1em;text-decoration:none!important}.sk-estimator-doc-link.fitted,a:link.sk-estimator-doc-link.fitted,a:visited.sk-estimator-doc-link.fitted{border:var(--sklearn-color-fitted-level-1)1pt solid;color:var(--sklearn-color-fitted-level-1)}div.sk-estimator:hover .sk-estimator-doc-link:hover,.sk-estimator-doc-link:hover,div.sk-label-container:hover .sk-estimator-doc-link:hover,.sk-estimator-doc-link:hover{background-color:var(--sklearn-color-unfitted-level-3);color:var(--sklearn-color-background);text-decoration:none}div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,.sk-estimator-doc-link.fitted:hover,div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,.sk-estimator-doc-link.fitted:hover{background-color:var(--sklearn-color-fitted-level-3);color:var(--sklearn-color-background);text-decoration:none}.sk-estimator-doc-link span{z-index:9999;color:var(--sklearn-color-text);background:var(--sklearn-color-unfitted-level-0);border:.5pt solid var(--sklearn-color-unfitted-level-3);width:min-content;min-width:20ex;max-width:50ex;margin:.5ex;padding:.5ex;font-weight:400;display:none;position:relative;right:.2ex;box-shadow:2pt 2pt 4pt #999}.sk-estimator-doc-link.fitted span{background:var(--sklearn-color-fitted-level-0);border:var(--sklearn-color-fitted-level-3)}.sk-estimator-doc-link:hover span{display:block}#sk-container-id-6 a.estimator_doc_link{float:right;background-color:var(--sklearn-color-background);color:var(--sklearn-color-unfitted-level-1);border:var(--sklearn-color-unfitted-level-1)1pt solid;border-radius:1rem;width:1rem;height:1rem;font-family:monospace;font-size:1rem;line-height:1em;text-decoration:none}#sk-container-id-6 a.estimator_doc_link.fitted{border:var(--sklearn-color-fitted-level-1)1pt solid;color:var(--sklearn-color-fitted-level-1)}#sk-container-id-6 a.estimator_doc_link:hover{background-color:var(--sklearn-color-unfitted-level-3);color:var(--sklearn-color-background);text-decoration:none}#sk-container-id-6 a.estimator_doc_link.fitted:hover{background-color:var(--sklearn-color-fitted-level-3)}</style><div class=sk-top-container id=sk-container-id-6><div class=sk-text-repr-fallback><pre>FirstLevelModel(fir_delays=[0], memory=Memory(location=None),
                minimize_memory=False, n_jobs=2, smoothing_fwhm=5.0,
                subject_label='10159', t_r=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=sk-container hidden><div class=sk-item><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually"checked id=sk-estimator-id-6 type=checkbox><label class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted"for=sk-estimator-id-6> FirstLevelModel<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>FirstLevelModel(fir_delays=[0], memory=Memory(location=None),
                minimize_memory=False, n_jobs=2, smoothing_fwhm=5.0,
                subject_label='10159', t_r=2)</pre></div></div></div></div></div></div><br><br><p>Then we compute the StopSuccess - Go contrast. We can use the column names of the design matrix.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>z_map</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-method"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel.compute_contrast title=nilearn.glm.first_level.FirstLevelModel.compute_contrast><span class=n>model</span><span class=o>.</span><span class=n>compute_contrast</span></a><span class=p>(</span><span class=s2>"StopSuccess - Go"</span><span class=p>)</span>
</pre></div></div><p>We show the agreement between the Nilearn estimation and the FSL estimation available in the dataset.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>import</span> <span class=nn>nibabel</span> <span class=k>as</span> <span class=nn>nib</span>

<a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>fsl_z_map</span></a> <span class=o>=</span> <span class=n>nib</span><span class=o>.</span><span class=n>load</span><span class=p>(</span>
    <a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path title=pathlib.Path><span class=n>Path</span></a><span class=p>(</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>data_dir</span></a><span class=p>)</span>
    <span class=o>/</span> <span class=s2>"derivatives"</span>
    <span class=o>/</span> <span class=s2>"task"</span>
    <span class=o>/</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>subject</span></a>
    <span class=o>/</span> <span class=s2>"stopsignal.feat"</span>
    <span class=o>/</span> <span class=s2>"stats"</span>
    <span class=o>/</span> <span class=s2>"zstat12.nii.gz"</span>
<span class=p>)</span>

<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
<span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>

<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html#nilearn.plotting.plot_glass_brain title=nilearn.plotting.plot_glass_brain><span class=n>plotting</span><span class=o>.</span><span class=n>plot_glass_brain</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>z_map</span></a><span class=p>,</span>
    <span class=n>colorbar</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>threshold</span><span class=o>=</span><a class="sphx-glr-backref-module-scipy-stats sphx-glr-backref-type-py-method"href=https://scipy.github.io/devdocs/reference/generated/scipy.stats.rv_continuous.isf.html#scipy.stats.rv_continuous.isf title=scipy.stats.rv_continuous.isf><span class=n>norm</span><span class=o>.</span><span class=n>isf</span></a><span class=p>(</span><span class=mf>0.001</span><span class=p>),</span>
    <span class=n>title</span><span class=o>=</span><span class=s1>'Nilearn Z map of "StopSuccess - Go" (unc p&LT0.001)'</span><span class=p>,</span>
    <span class=n>plot_abs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
    <span class=n>display_mode</span><span class=o>=</span><span class=s2>"ortho"</span><span class=p>,</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html#nilearn.plotting.plot_glass_brain title=nilearn.plotting.plot_glass_brain><span class=n>plotting</span><span class=o>.</span><span class=n>plot_glass_brain</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>fsl_z_map</span></a><span class=p>,</span>
    <span class=n>colorbar</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>threshold</span><span class=o>=</span><a class="sphx-glr-backref-module-scipy-stats sphx-glr-backref-type-py-method"href=https://scipy.github.io/devdocs/reference/generated/scipy.stats.rv_continuous.isf.html#scipy.stats.rv_continuous.isf title=scipy.stats.rv_continuous.isf><span class=n>norm</span><span class=o>.</span><span class=n>isf</span></a><span class=p>(</span><span class=mf>0.001</span><span class=p>),</span>
    <span class=n>title</span><span class=o>=</span><span class=s1>'FSL Z map of "StopSuccess - Go" (unc p&LT0.001)'</span><span class=p>,</span>
    <span class=n>plot_abs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
    <span class=n>display_mode</span><span class=o>=</span><span class=s2>"ortho"</span><span class=p>,</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show title=matplotlib.pyplot.show><span class=n>plt</span><span class=o>.</span><span class=n>show</span></a><span class=p>()</span>

<span class=kn>from</span> <span class=nn>nilearn.plotting</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_img_comparison.html#nilearn.plotting.plot_img_comparison title=nilearn.plotting.plot_img_comparison><span class=n>plot_img_comparison</span></a>

<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_img_comparison.html#nilearn.plotting.plot_img_comparison title=nilearn.plotting.plot_img_comparison><span class=n>plot_img_comparison</span></a><span class=p>(</span>
    <span class=p>[</span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>z_map</span></a><span class=p>],</span> <span class=p>[</span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>fsl_z_map</span></a><span class=p>],</span> <a class="sphx-glr-backref-module-nilearn-maskers sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker title=nilearn.maskers.NiftiMasker><span class=n>model</span><span class=o>.</span><span class=n>masker_</span></a><span class=p>,</span> <span class=n>ref_label</span><span class=o>=</span><span class=s2>"Nilearn"</span><span class=p>,</span> <span class=n>src_label</span><span class=o>=</span><span class=s2>"FSL"</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show title=matplotlib.pyplot.show><span class=n>plt</span><span class=o>.</span><span class=n>show</span></a><span class=p>()</span>
</pre></div></div><ul class=sphx-glr-horizontal><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_001.png srcset=../../_images/sphx_glr_plot_bids_features_001.png></li><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_002.png srcset=../../_images/sphx_glr_plot_bids_features_002.png></li><li><img alt="Histogram of imgs values"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_003.png srcset=../../_images/sphx_glr_plot_bids_features_003.png></li></ul></section><section id=simple-statistical-report-of-thresholded-contrast><h2>Simple statistical report of thresholded contrast<a title="Link to this heading"class=headerlink href=#simple-statistical-report-of-thresholded-contrast>¶</a></h2><p>We display the <a class="reference internal"href=../../glossary.html#term-contrast><span class="xref std std-term">contrast</span></a> plot and table with cluster information.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.plotting</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_contrast_matrix.html#nilearn.plotting.plot_contrast_matrix title=nilearn.plotting.plot_contrast_matrix><span class=n>plot_contrast_matrix</span></a>

<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_contrast_matrix.html#nilearn.plotting.plot_contrast_matrix title=nilearn.plotting.plot_contrast_matrix><span class=n>plot_contrast_matrix</span></a><span class=p>(</span><span class=s2>"StopSuccess - Go"</span><span class=p>,</span> <a class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame title=pandas.core.frame.DataFrame><span class=n>design_matrix</span></a><span class=p>)</span>
<a class="sphx-glr-backref-module-nilearn-plotting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.plotting.plot_glass_brain.html#nilearn.plotting.plot_glass_brain title=nilearn.plotting.plot_glass_brain><span class=n>plotting</span><span class=o>.</span><span class=n>plot_glass_brain</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>z_map</span></a><span class=p>,</span>
    <span class=n>colorbar</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
    <span class=n>threshold</span><span class=o>=</span><a class="sphx-glr-backref-module-scipy-stats sphx-glr-backref-type-py-method"href=https://scipy.github.io/devdocs/reference/generated/scipy.stats.rv_continuous.isf.html#scipy.stats.rv_continuous.isf title=scipy.stats.rv_continuous.isf><span class=n>norm</span><span class=o>.</span><span class=n>isf</span></a><span class=p>(</span><span class=mf>0.001</span><span class=p>),</span>
    <span class=n>plot_abs</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
    <span class=n>display_mode</span><span class=o>=</span><span class=s2>"z"</span><span class=p>,</span>
    <span class=n>figure</span><span class=o>=</span><a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure title=matplotlib.pyplot.figure><span class=n>plt</span><span class=o>.</span><span class=n>figure</span></a><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>)),</span>
<span class=p>)</span>
<a class="sphx-glr-backref-module-matplotlib-pyplot sphx-glr-backref-type-py-function"href=https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html#matplotlib.pyplot.show title=matplotlib.pyplot.show><span class=n>plt</span><span class=o>.</span><span class=n>show</span></a><span class=p>()</span>
</pre></div></div><ul class=sphx-glr-horizontal><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_004.png srcset=../../_images/sphx_glr_plot_bids_features_004.png></li><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_005.png srcset=../../_images/sphx_glr_plot_bids_features_005.png></li></ul><p>We can get a latex table from a Pandas Dataframe for display and publication purposes</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.reporting</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-reporting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.reporting.get_clusters_table.html#nilearn.reporting.get_clusters_table title=nilearn.reporting.get_clusters_table><span class=n>get_clusters_table</span></a>

<a class="sphx-glr-backref-module-pandas-core-frame sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html#pandas.DataFrame title=pandas.core.frame.DataFrame><span class=n>table</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-reporting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.reporting.get_clusters_table.html#nilearn.reporting.get_clusters_table title=nilearn.reporting.get_clusters_table><span class=n>get_clusters_table</span></a><span class=p>(</span><a class="sphx-glr-backref-module-nibabel-nifti1 sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image title=nibabel.nifti1.Nifti1Image><span class=n>z_map</span></a><span class=p>,</span> <a class="sphx-glr-backref-module-scipy-stats sphx-glr-backref-type-py-method"href=https://scipy.github.io/devdocs/reference/generated/scipy.stats.rv_continuous.isf.html#scipy.stats.rv_continuous.isf title=scipy.stats.rv_continuous.isf><span class=n>norm</span><span class=o>.</span><span class=n>isf</span></a><span class=p>(</span><span class=mf>0.001</span><span class=p>),</span> <span class=mi>10</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><a class="sphx-glr-backref-module-pandas sphx-glr-backref-type-py-method"href=https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_latex.html#pandas.DataFrame.to_latex title=pandas.DataFrame.to_latex><span class=n>table</span><span class=o>.</span><span class=n>to_latex</span></a><span class=p>())</span>
</pre></div></div><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>\begin{tabular}{llrrrrl}
\toprule
 & Cluster ID & X & Y & Z & Peak Stat & Cluster Size (mm3) \\
\midrule
0 & 1 & -66.000000 & -45.000000 & 22.000000 & 5.307532 & 6300 \\
1 & 1a & -66.000000 & -33.000000 & 18.000000 & 4.668929 &  \\
2 & 1b & -48.000000 & -36.000000 & 14.000000 & 4.534376 &  \\
3 & 1c & -57.000000 & -48.000000 & 10.000000 & 4.254210 &  \\
4 & 2 & -42.000000 & 15.000000 & 26.000000 & 4.918703 & 2520 \\
5 & 2a & -51.000000 & 9.000000 & 34.000000 & 4.715845 &  \\
6 & 2b & -42.000000 & 9.000000 & 30.000000 & 4.683343 &  \\
7 & 2c & -57.000000 & 12.000000 & 38.000000 & 4.587956 &  \\
8 & 3 & 57.000000 & -27.000000 & 2.000000 & 4.692869 & 504 \\
9 & 3a & 66.000000 & -27.000000 & 2.000000 & 3.664250 &  \\
10 & 4 & 42.000000 & 9.000000 & 34.000000 & 4.461193 & 540 \\
11 & 5 & 6.000000 & 18.000000 & 34.000000 & 4.257986 & 2520 \\
12 & 5a & -3.000000 & 15.000000 & 46.000000 & 4.078390 &  \\
13 & 5b & 0.000000 & 0.000000 & 38.000000 & 3.815609 &  \\
14 & 5c & 3.000000 & 9.000000 & 50.000000 & 3.798387 &  \\
15 & 6 & 6.000000 & 6.000000 & 54.000000 & 4.208105 & 468 \\
16 & 6a & 6.000000 & 3.000000 & 62.000000 & 3.348351 &  \\
17 & 7 & -45.000000 & 21.000000 & 2.000000 & 4.190472 & 504 \\
18 & 7a & -54.000000 & 21.000000 & 6.000000 & 3.385929 &  \\
19 & 8 & 45.000000 & -21.000000 & 42.000000 & 4.163956 & 432 \\
20 & 9 & 63.000000 & -24.000000 & 30.000000 & 4.079389 & 360 \\
21 & 10 & -12.000000 & 6.000000 & 6.000000 & 4.056165 & 792 \\
22 & 10a & -9.000000 & -3.000000 & 10.000000 & 3.726486 &  \\
23 & 10b & -9.000000 & 6.000000 & 14.000000 & 3.710553 &  \\
24 & 11 & -27.000000 & 45.000000 & 18.000000 & 4.043724 & 432 \\
25 & 12 & 3.000000 & -24.000000 & 30.000000 & 3.950054 & 360 \\
26 & 13 & 12.000000 & -72.000000 & 22.000000 & 3.937283 & 360 \\
27 & 14 & 33.000000 & 42.000000 & 34.000000 & 3.906274 & 756 \\
28 & 14a & 30.000000 & 45.000000 & 26.000000 & 3.882906 &  \\
29 & 15 & 51.000000 & -30.000000 & 14.000000 & 3.776293 & 648 \\
\bottomrule
\end{tabular}
</pre></div></div></section><section id=saving-model-outputs-to-disk><h2>Saving model outputs to disk<a title="Link to this heading"class=headerlink href=#saving-model-outputs-to-disk>¶</a></h2><p>We can now easily save the main results, the model metadata and an HTML report to the disk.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>output_dir</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-method"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path.cwd title=pathlib.Path.cwd><span class=n>Path</span><span class=o>.</span><span class=n>cwd</span></a><span class=p>()</span> <span class=o>/</span> <span class=s2>"results"</span> <span class=o>/</span> <span class=s2>"plot_bids_features"</span>
<a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-method"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path.mkdir title=pathlib.Path.mkdir><span class=n>output_dir</span><span class=o>.</span><span class=n>mkdir</span></a><span class=p>(</span><span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>parents</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

<span class=kn>from</span> <span class=nn>nilearn.interfaces.bids</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-interfaces-bids sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.interfaces.bids.save_glm_to_bids.html#nilearn.interfaces.bids.save_glm_to_bids title=nilearn.interfaces.bids.save_glm_to_bids><span class=n>save_glm_to_bids</span></a>

<a class="sphx-glr-backref-module-nilearn-interfaces-bids sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.interfaces.bids.save_glm_to_bids.html#nilearn.interfaces.bids.save_glm_to_bids title=nilearn.interfaces.bids.save_glm_to_bids><span class=n>save_glm_to_bids</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel title=nilearn.glm.first_level.FirstLevelModel><span class=n>model</span></a><span class=p>,</span>
    <span class=n>contrasts</span><span class=o>=</span><span class=s2>"StopSuccess - Go"</span><span class=p>,</span>
    <span class=n>contrast_types</span><span class=o>=</span><span class=p>{</span><span class=s2>"StopSuccess - Go"</span><span class=p>:</span> <span class=s2>"t"</span><span class=p>},</span>
    <span class=n>out_dir</span><span class=o>=</span><a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>output_dir</span></a> <span class=o>/</span> <span class=s2>"derivatives"</span> <span class=o>/</span> <span class=s2>"nilearn_glm"</span><span class=p>,</span>
    <span class=n>prefix</span><span class=o>=</span><span class=sa>f</span><span class=s2>"</span><span class=si>{</span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#str title=builtins.str><span class=n>subject</span></a><span class=si>}</span><span class=s2>_task-stopsignal"</span><span class=p>,</span>
<span class=p>)</span>
</pre></div></div><ul class=sphx-glr-horizontal><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_006.png srcset=../../_images/sphx_glr_plot_bids_features_006.png></li><li><img alt="plot bids features"class=sphx-glr-multi-img src=../../_images/sphx_glr_plot_bids_features_007.png srcset=../../_images/sphx_glr_plot_bids_features_007.png></li></ul><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>/home/remi/github/nilearn/nilearn_doc_build/examples/04_glm_first_level/plot_bids_features.py:227: UserWarning:

Contrast name "StopSuccess - Go" changed to "stopsuccessMinusGo"
</pre></div></div><p>View the generated files</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>files</span></a> <span class=o>=</span> <span class=nb>sorted</span><span class=p>((</span><a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>output_dir</span></a> <span class=o>/</span> <span class=s2>"derivatives"</span> <span class=o>/</span> <span class=s2>"nilearn_glm"</span><span class=p>)</span><span class=o>.</span><span class=n>glob</span><span class=p>(</span><span class=s2>"**/*"</span><span class=p>))</span>
<span class=nb>print</span><span class=p>(</span><span class=s2>"</span><span class=se>\n</span><span class=s2>"</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=nb>str</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>relative_to</span><span class=p>(</span><a class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.PosixPath title=pathlib.PosixPath><span class=n>output_dir</span></a><span class=p>))</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <a class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=https://docs.python.org/3.9/library/stdtypes.html#list title=builtins.list><span class=n>files</span></a><span class=p>]))</span>
</pre></div></div><div class="sphx-glr-script-out highlight-none notranslate"><div class=highlight><pre><span></span>derivatives/nilearn_glm/dataset_description.json
derivatives/nilearn_glm/sub-10159
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_design.svg
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_stat-effect_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_stat-p_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_stat-t_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_stat-variance_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_contrast-stopsuccessMinusGo_stat-z_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_design.json
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_design.svg
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_design.tsv
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_report.html
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_stat-errorts_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_stat-rsquared_statmap.nii.gz
derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_statmap.json
</pre></div></div></section><section id=only-generating-the-html-report><h2>Only generating the HTML report<a title="Link to this heading"class=headerlink href=#only-generating-the-html-report>¶</a></h2><p>Using the computed FirstLevelModel and <a class="reference internal"href=../../glossary.html#term-contrast><span class="xref std std-term">contrast</span></a> information, we can quickly also also only create a summary report.</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=kn>from</span> <span class=nn>nilearn.reporting</span> <span class=kn>import</span> <a class="sphx-glr-backref-module-nilearn-reporting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.reporting.make_glm_report.html#nilearn.reporting.make_glm_report title=nilearn.reporting.make_glm_report><span class=n>make_glm_report</span></a>

<a class="sphx-glr-backref-module-nilearn-reporting sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.reporting.HTMLReport.html#nilearn.reporting.HTMLReport title=nilearn.reporting.HTMLReport><span class=n>report</span></a> <span class=o>=</span> <a class="sphx-glr-backref-module-nilearn-reporting sphx-glr-backref-type-py-function"href=../../modules/generated/nilearn.reporting.make_glm_report.html#nilearn.reporting.make_glm_report title=nilearn.reporting.make_glm_report><span class=n>make_glm_report</span></a><span class=p>(</span>
    <a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel title=nilearn.glm.first_level.FirstLevelModel><span class=n>model</span></a><span class=o>=</span><a class="sphx-glr-backref-module-nilearn-glm-first_level sphx-glr-backref-type-py-class sphx-glr-backref-instance"href=../../modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel title=nilearn.glm.first_level.FirstLevelModel><span class=n>model</span></a><span class=p>,</span>
    <span class=n>contrasts</span><span class=o>=</span><span class=s2>"StopSuccess - Go"</span><span class=p>,</span>
<span class=p>)</span>
</pre></div></div><p>We have several ways to access the report:</p><div class="highlight-Python notranslate"><div class=highlight><pre><span></span><span class=c1># report  # This report can be viewed in a notebook</span>
<span class=c1># report.open_in_browser()</span>

<span class=c1># or we can save as an html file</span>
<span class=c1># report.save_as_html(output_dir / 'report.html')</span>
</pre></div></div><p class=sphx-glr-timing><strong>Total running time of the script:</strong> (1 minutes 0.185 seconds)</p><p><strong>Estimated memory usage:</strong> 900 MB</p><div class="sphx-glr-footer sphx-glr-footer-example docutils container"id=sphx-glr-download-auto-examples-04-glm-first-level-plot-bids-features-py><div class="binder-badge docutils container"><a class="reference external image-reference"href=https://mybinder.org/v2/gh/nilearn/nilearn/0.11.0?urlpath=lab/tree/notebooks/auto_examples/04_glm_first_level/plot_bids_features.ipynb><img alt="Launch binder"src=../../_images/binder_badge_logo4.svg style=width:150px> </a></div><div class="sphx-glr-download sphx-glr-download-jupyter docutils container"><p><a class="reference download internal"download href=../../_downloads/8ff74e423338ffab362778cbddf34aa4/plot_bids_features.ipynb><code class="xref download docutils literal notranslate"><span class=pre>Download</span> <span class=pre>Jupyter</span> <span class=pre>notebook:</span> <span class=pre>plot_bids_features.ipynb</span></code></a></p></div><div class="sphx-glr-download sphx-glr-download-python docutils container"><p><a class="reference download internal"download href=../../_downloads/1c5676eb2c85c27dffcfba93ccaa900c/plot_bids_features.py><code class="xref download docutils literal notranslate"><span class=pre>Download</span> <span class=pre>Python</span> <span class=pre>source</span> <span class=pre>code:</span> <span class=pre>plot_bids_features.py</span></code></a></p></div><div class="sphx-glr-download sphx-glr-download-zip docutils container"><p><a class="reference download internal"download href=../../_downloads/88e47e929fc88b6225f015fb94da5609/plot_bids_features.zip><code class="xref download docutils literal notranslate"><span class=pre>Download</span> <span class=pre>zipped:</span> <span class=pre>plot_bids_features.zip</span></code></a></p></div></div><p class=sphx-glr-signature><a class="reference external"href=https://sphinx-gallery.github.io>Gallery generated by Sphinx-Gallery</a></p></section></section></article></div><footer><div class=related-pages><a class=next-page href=plot_two_runs_model.html> <div class=page-info><div class=context><span>Next</span></div><div class=title>Simple example of two-runs fMRI model fitting</div></div> <svg class=furo-related-icon><use href=#svg-arrow-right></use></svg> </a><a class=prev-page href=plot_design_matrix.html> <svg class=furo-related-icon><use href=#svg-arrow-right></use></svg> <div class=page-info><div class=context><span>Previous</span></div><div class=title>Examples of design matrices</div></div> </a></div><div class=bottom-of-page><div class=left-details><div class=copyright>Copyright © The nilearn developers</div> Made with <a href=https://www.sphinx-doc.org/>Sphinx</a> and <a class=muted-link href=https://pradyunsg.me>@pradyunsg</a>'s <a href=https://github.com/pradyunsg/furo>Furo</a></div><div class=right-details><div class=icons><a class="muted-link fa-brands fa-solid fa-github fa-2x"aria-label=GitHub href=https://github.com/nilearn/nilearn></a><a class="muted-link fa-brands fa-solid fa-twitter fa-2x"aria-label=Twitter href=https://twitter.com/nilearn></a><a class="muted-link fa-brands fa-solid fa-bluesky fa-2x"aria-label=Bluesky href=https://bsky.app/profile/nilearn.bsky.social></a><a class="muted-link fa-brands fa-solid fa-mastodon fa-2x"aria-label=Mastodon href=https://fosstodon.org/@nilearn></a><a class="muted-link fa-brands fa-solid fa-discord fa-2x"aria-label=Discord href=https://discord.gg/SsQABEJHkZ></a><a class="muted-link fa-brands fa-solid fa-youtube fa-2x"aria-label=Youtube href=https://www.youtube.com/channel/UCU6BMAi2zOhNFnDkbdevmPw></a></div></div></div></footer></div><aside class=toc-drawer><div class="toc-sticky toc-scroll"><div class=toc-title-container><span class=toc-title> On this page </span></div><div class=toc-tree-container><div class=toc-tree><ul><li><a class="reference internal"href=#>First level analysis of a complete BIDS dataset from openneuro</a><ul><li><a class="reference internal"href=#fetch-openneuro-bids-dataset>Fetch openneuro <span class="xref std std-term">BIDS</span> dataset</a></li><li><a class="reference internal"href=#obtain-firstlevelmodel-objects-automatically-and-fit-arguments>Obtain FirstLevelModel objects automatically and fit arguments</a></li><li><a class="reference internal"href=#first-level-model-estimation-one-subject>First level model estimation (one subject)</a></li><li><a class="reference internal"href=#simple-statistical-report-of-thresholded-contrast>Simple statistical report of thresholded contrast</a></li><li><a class="reference internal"href=#saving-model-outputs-to-disk>Saving model outputs to disk</a></li><li><a class="reference internal"href=#only-generating-the-html-report>Only generating the HTML report</a></li></ul></li></ul></div></div></div></aside></div></div><script src=../../_static/documentation_options.js?v=5929fcd5></script><script src=../../_static/doctools.js?v=9a2dae69></script><script src=../../_static/sphinx_highlight.js?v=dc90522c></script><script src=../../_static/scripts/furo.js?v=5fa4622c></script><script src=../../_static/clipboard.min.js?v=a7894cd8></script><script src=../../_static/copybutton.js?v=4ea706d9></script><script src=../../_static/design-tabs.js?v=f930bc37></script></body></html>