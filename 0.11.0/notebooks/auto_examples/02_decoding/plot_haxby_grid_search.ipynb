{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting a parameter by cross-validation\n\nHere we set the number of features selected in an Anova-SVC approach to\nmaximize the cross-validation score.\n\nAfter separating 2 runs for validation, we vary that parameter and\nmeasure the cross-validation score. We also measure the prediction score\non the left-out validation data. As we can see, the two scores vary by a\nsignificant amount: this is due to sampling noise in cross validation,\nand choosing the parameter k to maximize the cross-validation score,\nmight not maximize the score on left-out data.\n\nThus using data to maximize a cross-validation score computed on that\nsame data is likely to be too optimistic and lead to an overfit.\n\nThe proper approach is known as a \"nested cross-validation\". It consists\nin doing cross-validation loops to set the model parameters inside the\ncross-validation loop used to judge the prediction performance: the\nparameters are set separately on each fold, never using the data used to\nmeasure performance.\n\nFor decoding tasks, in nilearn, this can be done using the\n:class:`nilearn.decoding.Decoder` object, which will automatically select\nthe best parameters of an estimator from a grid of parameter values.\n\nOne difficulty is that the Decoder object is a composite estimator: a\npipeline of feature selection followed by Support Vector Machine. Tuning\nthe SVM's parameters is already done automatically inside the Decoder, but\nperforming cross-validation for the feature selection must be done\nmanually.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Haxby dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\nfrom nilearn.plotting import show\n\n# by default 2nd subject data will be fetched on which we run our analysis\nhaxby_dataset = datasets.fetch_haxby()\nfmri_img = haxby_dataset.func[0]\nmask_img = haxby_dataset.mask\n\n# print basic information on the dataset\nprint(f\"Mask nifti image (3D) is located at: {haxby_dataset.mask}\")\nprint(f\"Functional nifti image (4D) are located at: {haxby_dataset.func[0]}\")\n\n# Load the behavioral data\nimport pandas as pd\n\nlabels = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\ny = labels[\"labels\"]\n\n\n# Keep only data corresponding to shoes or bottles\nfrom nilearn.image import index_img\n\ncondition_mask = y.isin([\"shoe\", \"bottle\"])\n\nfmri_niimgs = index_img(fmri_img, condition_mask)\ny = y[condition_mask]\nrun = labels[\"chunks\"][condition_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## :term:`ANOVA` pipeline with :class:`nilearn.decoding.Decoder` object\n\nNilearn Decoder object aims to provide smooth user experience by acting as a\npipeline of several tasks: preprocessing with NiftiMasker, reducing dimension\nby selecting only relevant features with :term:`ANOVA`\n-- a classical univariate feature selection based on F-test,\nand then decoding with different types of estimators\n(in this example is Support Vector Machine with a linear kernel)\non nested cross-validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decoding import Decoder\n\n# We provide a grid of hyperparameter values to the Decoder's internal\n# cross-validation. If no param_grid is provided, the Decoder will use a\n# default grid with sensible values for the chosen estimator\nparam_grid = [\n    {\n        \"penalty\": [\"l2\"],\n        \"dual\": [True],\n        \"C\": [100, 1000],\n    },\n    {\n        \"penalty\": [\"l1\"],\n        \"dual\": [False],\n        \"C\": [100, 1000],\n    },\n]\n\n# Here screening_percentile is set to 2 percent, meaning around 800\n# features will be selected with ANOVA.\ndecoder = Decoder(\n    estimator=\"svc\",\n    cv=5,\n    mask=mask_img,\n    smoothing_fwhm=4,\n    standardize=\"zscore_sample\",\n    screening_percentile=2,\n    param_grid=param_grid,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit the Decoder and predict the responses\nAs a complete pipeline by itself, decoder will perform cross-validation\nfor the estimator, in this case Support Vector Machine. We can output the\nbest parameters selected for each cross-validation fold. See\nhttps://scikit-learn.org/stable/modules/cross_validation.html for an\nexcellent explanation of how cross-validation works.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Fit the Decoder\ndecoder.fit(fmri_niimgs, y)\n\n# Print the best parameters for each fold\nfor i, (best_c, best_penalty, best_dual, cv_score) in enumerate(\n    zip(\n        decoder.cv_params_[\"shoe\"][\"C\"],\n        decoder.cv_params_[\"shoe\"][\"penalty\"],\n        decoder.cv_params_[\"shoe\"][\"dual\"],\n        decoder.cv_scores_[\"shoe\"],\n    )\n):\n    print(\n        f\"Fold {i + 1} | Best SVM parameters: C={best_c}\"\n        f\", penalty={best_penalty}, dual={best_dual} with score: {cv_score}\"\n    )\n\n# Output the prediction with Decoder\ny_pred = decoder.predict(fmri_niimgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute prediction scores with different values of screening percentile\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nscreening_percentile_range = [2, 4, 8, 16, 32, 64]\ncv_scores = []\nval_scores = []\n\nfor sp in screening_percentile_range:\n    decoder = Decoder(\n        estimator=\"svc\",\n        mask=mask_img,\n        smoothing_fwhm=4,\n        cv=3,\n        standardize=\"zscore_sample\",\n        screening_percentile=sp,\n        param_grid=param_grid,\n    )\n    decoder.fit(index_img(fmri_niimgs, run < 10), y[run < 10])\n    cv_scores.append(np.mean(decoder.cv_scores_[\"bottle\"]))\n    print(f\"Sreening Percentile: {sp:.3f}\")\n    print(f\"Mean CV score: {cv_scores[-1]:.4f}\")\n\n    y_pred = decoder.predict(index_img(fmri_niimgs, run == 10))\n    val_scores.append(np.mean(y_pred == y[run == 10]))\n    print(f\"Validation score: {val_scores[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nested cross-validation\nWe are going to tune the parameter 'screening_percentile' in the\npipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n\ncv = KFold(n_splits=3)\nnested_cv_scores = []\n\nfor train, test in cv.split(run):\n    y_train = np.array(y)[train]\n    y_test = np.array(y)[test]\n    val_scores = []\n\n    for sp in screening_percentile_range:\n        decoder = Decoder(\n            estimator=\"svc\",\n            mask=mask_img,\n            smoothing_fwhm=4,\n            cv=3,\n            standardize=\"zscore_sample\",\n            screening_percentile=sp,\n            param_grid=param_grid,\n        )\n        decoder.fit(index_img(fmri_niimgs, train), y_train)\n        y_pred = decoder.predict(index_img(fmri_niimgs, test))\n        val_scores.append(np.mean(y_pred == y_test))\n\n    nested_cv_scores.append(np.max(val_scores))\n\nprint(f\"Nested CV score: {np.mean(nested_cv_scores):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the prediction scores using matplotlib\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(cv_scores, label=\"Cross validation scores\")\nplt.plot(val_scores, label=\"Left-out validation data scores\")\nplt.xticks(\n    np.arange(len(screening_percentile_range)), screening_percentile_range\n)\nplt.axis(\"tight\")\nplt.xlabel(\"ANOVA screening percentile\")\n\nplt.axhline(\n    np.mean(nested_cv_scores), label=\"Nested cross-validation\", color=\"r\"\n)\n\nplt.legend(loc=\"best\", frameon=False)\nshow()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
