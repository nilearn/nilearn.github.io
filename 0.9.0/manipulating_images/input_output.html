<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="6.1. Input and output: neuroimaging data representation"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/manipulating_images/input_output.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Contents: Inputing data: file names or image objects, Fetching open datasets from Internet, Understanding neuroimaging data., Inputing data: file names or image objects: File names and objects, 3D ..."property=og:description><meta content=https://nilearn.github.io/_static/nilearn-logo.png property=og:image><meta content=Nilearn property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../_static/pygments.css rel=stylesheet><link href=../_static/nature.css rel=stylesheet><link href=../_static/copybutton.css rel=stylesheet><link href=../_static/sg_gallery.css rel=stylesheet><link href=../_static/sg_gallery-binder.css rel=stylesheet><link href=../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../ id=documentation_options src=../_static/documentation_options.js></script><script src=../_static/jquery.js></script><script src=../_static/underscore.js></script><script src=../_static/doctools.js></script><script src=../_static/clipboard.min.js></script><script src=../_static/copybutton.js></script><link rel="shortcut icon"href=../_static/favicon.ico><link href=../search.html rel=search title=Search><link title="6.2. Manipulating images: resampling, smoothing, masking, ROIs…"href=manipulating_images.html rel=next><link title="6. Manipulation brain volumes with nilearn"href=index.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../index.html> <img alt="Nilearn logo"border=0 src=../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=data_preparation.html>Nifti IO</a></li><li><a href=../modules/reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../py-modindex.html>modules</a></li><li class=right><a title="6.2. Manipulating images: resampling, smoothing, masking, ROIs…"accesskey=N href=manipulating_images.html>next</a> |</li><li class=right><a title="6. Manipulation brain volumes with nilearn"accesskey=P href=index.html>previous</a> |</li><li><a href=../index.html>Nilearn Home</a> | </li><li><a href=../user_guide.html>User Guide</a> | </li><li><a href=../auto_examples/index.html>Examples</a> | </li><li><a href=../modules/reference.html>Reference</a> | </li><li id=navbar-about><a href=../authors.html>About</a>| </li><li><a href=../glossary.html>Glossary</a>| </li><li><a href=../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=index.html><span class=section-number>6. </span>Manipulation brain volumes with nilearn</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class=section id=input-and-output-neuroimaging-data-representation><span id=extracting-data></span><h1><span class=section-number>6.1. </span>Input and output: neuroimaging data representation<a title="Permalink to this headline"class=headerlink href=#input-and-output-neuroimaging-data-representation>¶</a></h1><div class="contents local topic"id=contents><p class=topic-title><strong>Contents</strong></p><ul class=simple><li><p><a class="reference internal"href=#inputing-data-file-names-or-image-objects id=id1>Inputing data: file names or image objects</a></p></li><li><p><a class="reference internal"href=#fetching-open-datasets-from-internet id=id2>Fetching open datasets from Internet</a></p></li><li><p><a class="reference internal"href=#understanding-neuroimaging-data id=id3>Understanding neuroimaging data</a></p></li></ul></div><div class=line-block><div class=line><br></div></div><div class=section id=inputing-data-file-names-or-image-objects><span id=loading-data></span><h2><a class=toc-backref href=#id1><span class=section-number>6.1.1. </span>Inputing data: file names or image objects</a><a title="Permalink to this headline"class=headerlink href=#inputing-data-file-names-or-image-objects>¶</a></h2><div class=section id=file-names-and-objects-3d-and-4d-images><h3><span class=section-number>6.1.1.1. </span>File names and objects, 3D and 4D images<a title="Permalink to this headline"class=headerlink href=#file-names-and-objects-3d-and-4d-images>¶</a></h3><p>All Nilearn functions accept file names as arguments:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=kn>from</span> <span class=nn>nilearn</span> <span class=kn>import</span> <span class=n>image</span>
<span class=gp>>>> </span><span class=n>smoothed_img</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>smooth_img</span><span class=p>(</span><span class=s1>'/home/user/t_map001.nii'</span><span class=p>)</span>
</pre></div></div><p>Nilearn can operate on either file names or <a class="reference external"href=http://nipy.org/nibabel/nibabel_images.html>NiftiImage objects</a>. The later represent the data loaded in memory. In the example above, the function <a class="reference internal"href=../modules/generated/nilearn.image.smooth_img.html#nilearn.image.smooth_img title=nilearn.image.smooth_img><code class="xref py py-func docutils literal notranslate"><span class=pre>smooth_img</span></code></a> returns a Nifti1Image object, which can then be readily passed to other nilearn functions.</p><p>In nilearn, we often use the term <em>“niimg”</em> as abbreviation that denotes either a file name or a <a class="reference external"href=http://nipy.org/nibabel/nibabel_images.html>NiftiImage object</a>.</p><p>Niimgs can be 3D or 4D. A 4D niimg may for instance represent a time series of 3D images. It can be <strong>a list of file names</strong>, if these contain 3D information:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=c1># dataset folder contains subject1.nii and subject2.nii</span>
<span class=gp>>>> </span><span class=kn>from</span> <span class=nn>nilearn.image</span> <span class=kn>import</span> <span class=n>smooth_img</span>
<span class=gp>>>> </span><span class=n>result_img</span> <span class=o>=</span> <span class=n>smooth_img</span><span class=p>([</span><span class=s1>'dataset/subject1.nii'</span><span class=p>,</span> <span class=s1>'dataset/subject2.nii'</span><span class=p>])</span>
</pre></div></div><p><code class="docutils literal notranslate"><span class=pre>result_img</span></code> is a 4D in-memory image, containing the data of both subjects.</p></div><div class=section id=file-name-matching-globbing-and-user-path-expansion><span id=filename-matching></span><h3><span class=section-number>6.1.1.2. </span>File name matching: “globbing” and user path expansion<a title="Permalink to this headline"class=headerlink href=#file-name-matching-globbing-and-user-path-expansion>¶</a></h3><p>You can specify files with <em>wildcard</em> matching patterns (as in Unix shell):</p><blockquote><div><ul><li><p><strong>Matching multiple files</strong>: suppose the dataset folder contains subject_01.nii, subject_03.nii, and subject_03.nii; <code class="docutils literal notranslate"><span class=pre>dataset/subject_*.nii</span></code> is a glob expression matching all filenames:</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=c1># Example with a smoothing process:</span>
<span class=gp>>>> </span><span class=kn>from</span> <span class=nn>nilearn.image</span> <span class=kn>import</span> <span class=n>smooth_img</span>
<span class=gp>>>> </span><span class=n>result_img</span> <span class=o>=</span> <span class=n>smooth_img</span><span class=p>(</span><span class=s2>"dataset/subject_*.nii"</span><span class=p>)</span>
</pre></div></div> <p>Note that the resulting is a 4D image.</p></li><li><p><strong>Expanding the home directory</strong> <code class="docutils literal notranslate"><span class=pre>~</span></code> is expanded to your home directory:</p> <div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=n>result_img</span> <span class=o>=</span> <span class=n>smooth_img</span><span class=p>(</span><span class=s2>"~/dataset/subject_01.nii"</span><span class=p>)</span>
</pre></div></div> <p>Using <code class="docutils literal notranslate"><span class=pre>~</span></code> rather than specifying the details of the path is good practice, as it will make it more likely that your script work on different computers.</p></li></ul></div></blockquote><div class=topic><p class=topic-title><strong>Python globbing</strong></p><p>For more complicated use cases, Python also provides functions to work with file paths, in particular, <a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/glob.html#glob.glob><code class="xref py py-func docutils literal notranslate"><span class=pre>glob.glob</span></code></a>.</p><div class="admonition warning"><p class=admonition-title>Warning</p><p>Unlike nilearn’s path expansion, the result of <a class="reference external"title="(in Python v3.8)"href=https://docs.python.org/3.8/library/glob.html#glob.glob><code class="xref py py-func docutils literal notranslate"><span class=pre>glob.glob</span></code></a> is not sorted and, depending on the computer you are running, they might not be in alphabetic order. We advise you to rely on nilearn’s path expansion.</p></div><p>To load data with globbing, we suggest that you use <a class="reference internal"href=../modules/generated/nilearn.image.load_img.html#nilearn.image.load_img title=nilearn.image.load_img><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.image.load_img</span></code></a>.</p></div></div></div><div class=section id=fetching-open-datasets-from-internet><span id=datasets></span><h2><a class=toc-backref href=#id2><span class=section-number>6.1.2. </span>Fetching open datasets from Internet</a><a title="Permalink to this headline"class=headerlink href=#fetching-open-datasets-from-internet>¶</a></h2><p>Nilearn provides dataset fetching function that automatically downloads reference datasets and atlases. They can be imported from <a class="reference internal"href=../modules/reference.html#module-nilearn.datasets title=nilearn.datasets><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.datasets</span></code></a>:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=kn>from</span> <span class=nn>nilearn</span> <span class=kn>import</span> <span class=n>datasets</span>
<span class=gp>>>> </span><span class=n>haxby_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>fetch_haxby</span><span class=p>()</span>
</pre></div></div><p>They return a data structure that contains different pieces of information on the retrieved dataset, including the file names on hard disk:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=c1># The different files</span>
<span class=gp>>>> </span><span class=nb>print</span><span class=p>(</span><span class=nb>sorted</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>haxby_dataset</span><span class=o>.</span><span class=n>keys</span><span class=p>())))</span>
<span class=go>['anat', 'description', 'func', 'mask', 'mask_face', 'mask_face_little',</span>
<span class=go>'mask_house', 'mask_house_little', 'mask_vt', 'session_target']</span>
<span class=gp>>>> </span><span class=c1># Path to first functional file</span>
<span class=gp>>>> </span><span class=nb>print</span><span class=p>(</span><span class=n>haxby_dataset</span><span class=o>.</span><span class=n>func</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
<span class=go>/.../nilearn_data/haxby2001/subj1/bold.nii.gz</span>
</pre></div></div><p>Explanation and further resources of the dataset at hand can be retrieved as follows:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=nb>print</span><span class=p>(</span><span class=n>haxby_dataset</span><span class=o>.</span><span class=n>description</span><span class=p>)</span>
<span class=go>Haxby 2001 results</span>


<span class=go>Notes</span>
<span class=go>-----</span>
<span class=go>Results from a classical fMRI study that...</span>
</pre></div></div><div class=line-block><div class=line><br></div></div><div class="admonition seealso"><p class=admonition-title>See also</p><p>For a list of all the data fetching functions in nilearn, see <a class="reference internal"href=../modules/reference.html#datasets-ref><span class="std std-ref">nilearn.datasets: Automatic Dataset Fetching</span></a>.</p></div><div class=line-block><div class=line><br></div></div><div class=topic><p class=topic-title><strong>nilearn_data: Where is the downloaded data stored?</strong></p><p>The fetching functions download the reference datasets to the disk. They save it locally for future use, in one of the following directories (in order of priority, if present):</p><blockquote><div><ul class=simple><li><p>the folder specified by <cite>data_dir</cite> parameter in the fetching function</p></li><li><p>the global environment variable <cite>NILEARN_SHARED_DATA</cite></p></li><li><p>the user environment variable <cite>NILEARN_DATA</cite></p></li><li><p>the <cite>nilearn_data</cite> folder in the user home folder</p></li></ul></div></blockquote><p>The two different environment variables (NILEARN_SHARED_DATA and NILEARN_DATA) are provided for multi-user systems, to distinguish a global dataset repository that may be read-only at the user-level. Note that you can copy that folder to another user’s computers to avoid the initial dataset download on the first fetching call.</p><p>You can check in which directory nilearn will store the data with the function <a class="reference internal"href=../modules/generated/nilearn.datasets.get_data_dirs.html#nilearn.datasets.get_data_dirs title=nilearn.datasets.get_data_dirs><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.datasets.get_data_dirs</span></code></a>.</p></div><div class=line-block><div class=line><br></div></div></div><div class=section id=understanding-neuroimaging-data><h2><a class=toc-backref href=#id3><span class=section-number>6.1.3. </span>Understanding neuroimaging data</a><a title="Permalink to this headline"class=headerlink href=#understanding-neuroimaging-data>¶</a></h2><div class=section id=nifti-and-analyze-data><h3><span class=section-number>6.1.3.1. </span>Nifti and Analyze data<a title="Permalink to this headline"class=headerlink href=#nifti-and-analyze-data>¶</a></h3><p>For volumetric data, nilearn works with data stored as in the Nifti structure (via the <a class="reference external"href=http://nipy.sourceforge.net/nibabel/>nibabel</a> package).</p><p>The <a class="reference external"href=http://nifti.nimh.nih.gov/>NifTi</a> data structure (also used in Analyze files) is the standard way of sharing data in neuroimaging research. Three main components are:</p><dl class="field-list simple"><dt class=field-odd>data</dt><dd class=field-odd><p>raw scans in form of a numpy array: <code class="docutils literal notranslate"><span class=pre>data</span> <span class=pre>=</span> <span class=pre>nilearn.image.get_data(img)</span></code></p></dd><dt class=field-even>affine</dt><dd class=field-even><p>returns the transformation matrix that maps from voxel indices of the numpy array to actual real-world locations of the brain: <code class="docutils literal notranslate"><span class=pre>affine</span> <span class=pre>=</span> <span class=pre>img.affine</span></code></p></dd><dt class=field-odd>header</dt><dd class=field-odd><p>low-level information about the data (slice duration, etc.): <code class="docutils literal notranslate"><span class=pre>header</span> <span class=pre>=</span> <span class=pre>img.header</span></code></p></dd></dl><p>If you need to load the data without using nilearn, read the <a class="reference external"href=http://nipy.sourceforge.net/nibabel/>nibabel</a> documentation.</p><p>Note: For older versions of <a class="reference external"href=http://nipy.sourceforge.net/nibabel/>nibabel</a>, affine and header can be retrieved with <code class="docutils literal notranslate"><span class=pre>get_affine()</span></code> and <code class="docutils literal notranslate"><span class=pre>get_header()</span></code>.</p><div class=topic><p class=topic-title><strong>Dataset formatting: data shape</strong></p><p>It is important to appreciate two main representations for storing and accessing more than one Nifti images, that is sets of MRI scans:</p><ul class=simple><li><p>a big 4D matrix representing (3D MRI + 1D for time), stored in a single Nifti file. <a class="reference external"href=http://www.fmrib.ox.ac.uk/fsl/>FSL</a> users tend to prefer this format.</p></li><li><p>several 3D matrices representing each time point (single 3D volume) of the session, stored in set of 3D Nifti or analyse files. <a class="reference external"href=http://www.fil.ion.ucl.ac.uk/spm/>SPM</a> users tend to prefer this format.</p></li></ul></div></div><div class=section id=niimg-like-objects><span id=niimg></span><h3><span class=section-number>6.1.3.2. </span>Niimg-like objects<a title="Permalink to this headline"class=headerlink href=#niimg-like-objects>¶</a></h3><p>Nilearn functions take as input argument what we call “Niimg-like objects”:</p><p><strong>Niimg:</strong> A Niimg-like object can be one of the following:</p><blockquote><div><ul class=simple><li><p>A string with a file path to a Nifti or Analyse image</p></li><li><p>An <code class="docutils literal notranslate"><span class=pre>SpatialImage</span></code> from nibabel, ie an object exposing <code class="docutils literal notranslate"><span class=pre>get_fdata()</span></code> method and <code class="docutils literal notranslate"><span class=pre>affine</span></code> attribute, typically a <code class="docutils literal notranslate"><span class=pre>Nifti1Image</span></code> from <a class="reference external"href=http://nipy.sourceforge.net/nibabel/>nibabel</a>.</p></li></ul></div></blockquote><p><strong>Niimg-4D:</strong> Similarly, some functions require 4D Nifti-like data, which we call Niimgs or Niimg-4D. Accepted input arguments are:</p><blockquote><div><ul class=simple><li><p>A path to a 4D Nifti image</p></li><li><p>List of paths to 3D Nifti images</p></li><li><p>4D Nifti-like object</p></li><li><p>List of 3D Nifti-like objects</p></li></ul></div></blockquote><div class=topic><p class=topic-title><strong>Image affines</strong></p><p>If you provide a sequence of Nifti images, all of them must have the same affine.</p></div><div class=topic><p class=topic-title><strong>Decreasing memory used when loading Nifti images</strong></p><p>When Nifti images are stored compressed (.nii.gz), loading them directly consumes more memory. As a result, large 4D images may raise “MemoryError”, especially on smaller computers and when using Nilearn routines that require intensive 4D matrix operations. One step to improve the situation may be to decompress the data onto disk as an initial step. If multiple images are loaded into memory sequentially, another solution may be to <a class="reference external"href=https://nipy.org/nibabel/images_and_memory.html#using-uncache>uncache</a> one before loading and performing operations on another.</p></div></div><div class=section id=text-files-phenotype-or-behavior><h3><span class=section-number>6.1.3.3. </span>Text files: phenotype or behavior<a title="Permalink to this headline"class=headerlink href=#text-files-phenotype-or-behavior>¶</a></h3><p>Phenotypic or behavioral data are often provided as text or CSV (Comma Separated Values) file. They can be loaded with <cite>pd.read_csv</cite> but you may have to specify some options (typically <cite>sep</cite> if fields aren’t delimited with a comma).</p><p>For the Haxby datasets, we can load the categories of the images presented to the subject:</p><div class="highlight-default notranslate"><div class=highlight><pre><span></span><span class=gp>>>> </span><span class=kn>from</span> <span class=nn>nilearn</span> <span class=kn>import</span> <span class=n>datasets</span>
<span class=gp>>>> </span><span class=n>haxby_dataset</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>fetch_haxby</span><span class=p>()</span>
<span class=gp>>>> </span><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
<span class=gp>>>> </span><span class=n>labels</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>haxby_dataset</span><span class=o>.</span><span class=n>session_target</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>sep</span><span class=o>=</span><span class=s2>" "</span><span class=p>)</span>
<span class=gp>>>> </span><span class=n>stimuli</span> <span class=o>=</span> <span class=n>labels</span><span class=p>[</span><span class=s1>'labels'</span><span class=p>]</span>
<span class=gp>>>> </span><span class=nb>print</span><span class=p>(</span><span class=n>stimuli</span><span class=o>.</span><span class=n>unique</span><span class=p>())</span>
<span class=go>['bottle' 'cat' 'chair' 'face' 'house' 'rest' 'scissors' 'scrambledpix'</span>
<span class=go> 'shoe']</span>
</pre></div></div><div class=topic><p class=topic-title><strong>Reading CSV with pandas</strong></p><p><a class="reference external"href=http://pandas.pydata.org/>Pandas</a> is a powerful package to read data from CSV files and manipulate them.</p></div><div class=line-block><div class=line><br></div></div></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>6.1. Input and output: neuroimaging data representation</a><ul><li><a class="reference internal"href=#inputing-data-file-names-or-image-objects>6.1.1. Inputing data: file names or image objects</a><ul><li><a class="reference internal"href=#file-names-and-objects-3d-and-4d-images>6.1.1.1. File names and objects, 3D and 4D images</a></li><li><a class="reference internal"href=#file-name-matching-globbing-and-user-path-expansion>6.1.1.2. File name matching: “globbing” and user path expansion</a></li></ul></li><li><a class="reference internal"href=#fetching-open-datasets-from-internet>6.1.2. Fetching open datasets from Internet</a></li><li><a class="reference internal"href=#understanding-neuroimaging-data>6.1.3. Understanding neuroimaging data</a><ul><li><a class="reference internal"href=#nifti-and-analyze-data>6.1.3.1. Nifti and Analyze data</a></li><li><a class="reference internal"href=#niimg-like-objects>6.1.3.2. Niimg-like objects</a></li><li><a class="reference internal"href=#text-files-phenotype-or-behavior>6.1.3.3. Text files: phenotype or behavior</a></li></ul></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=index.html><span class=section-number>6. </span>Manipulation brain volumes with nilearn</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=manipulating_images.html><span class=section-number>6.2. </span>Manipulating images: resampling, smoothing, masking, ROIs…</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../_sources/manipulating_images/input_output.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>