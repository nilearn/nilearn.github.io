<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="2.4. SpaceNet: decoding with spatial structure for better maps"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/decoding/space_net.html property=og:url><meta content=Nilearn property=og:site_name><meta content="The SpaceNet decoder: nilearn.decoding.SpaceNetRegressor and nilearn.decoding.SpaceNetClassifier implements spatial penalties which improve brain decoding power as well as decoder maps: penalty=”tv..."property=og:description><meta content=../_images/sphx_glr_plot_oasis_vbm_space_net_002.png property=og:image><meta content=Nilearn property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../_static/pygments.css rel=stylesheet><link href=../_static/nature.css rel=stylesheet><link href=../_static/copybutton.css rel=stylesheet><link href=../_static/sg_gallery.css rel=stylesheet><link href=../_static/sg_gallery-binder.css rel=stylesheet><link href=../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../ id=documentation_options src=../_static/documentation_options.js></script><script src=../_static/jquery.js></script><script src=../_static/underscore.js></script><script src=../_static/doctools.js></script><script src=../_static/clipboard.min.js></script><script src=../_static/copybutton.js></script><link rel="shortcut icon"href=../_static/favicon.ico><link href=../search.html rel=search title=Search><link title="2.5. Searchlight : finding voxels containing information"href=searchlight.html rel=next><link title="2.3. FREM: fast ensembling of regularized models for robust decoding"href=frem.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../index.html> <img alt="Nilearn logo"border=0 src=../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=searchlight.html>Searchlight</a></li><li><big><a href=../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../modules/reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../py-modindex.html>modules</a></li><li class=right><a title="2.5. Searchlight : finding voxels containing information"accesskey=N href=searchlight.html>next</a> |</li><li class=right><a title="2.3. FREM: fast ensembling of regularized models for robust decoding"accesskey=P href=frem.html>previous</a> |</li><li><a href=../index.html>Nilearn Home</a> | </li><li><a href=../user_guide.html>User Guide</a> | </li><li><a href=../auto_examples/index.html>Examples</a> | </li><li><a href=../modules/reference.html>Reference</a> | </li><li id=navbar-about><a href=../authors.html>About</a>| </li><li><a href=../glossary.html>Glossary</a>| </li><li><a href=../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=index.html><span class=section-number>2. </span>Decoding and MVPA: predicting from brain images</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class=section id=spacenet-decoding-with-spatial-structure-for-better-maps><span id=space-net></span><h1><span class=section-number>2.4. </span>SpaceNet: decoding with spatial structure for better maps<a title="Permalink to this headline"class=headerlink href=#spacenet-decoding-with-spatial-structure-for-better-maps>¶</a></h1><div class=section id=the-spacenet-decoder><h2><span class=section-number>2.4.1. </span>The SpaceNet decoder<a title="Permalink to this headline"class=headerlink href=#the-spacenet-decoder>¶</a></h2><p><a class="reference internal"href=../modules/generated/nilearn.decoding.SpaceNetRegressor.html#nilearn.decoding.SpaceNetRegressor title=nilearn.decoding.SpaceNetRegressor><code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.decoding.SpaceNetRegressor</span></code></a> and <a class="reference internal"href=../modules/generated/nilearn.decoding.SpaceNetClassifier.html#nilearn.decoding.SpaceNetClassifier title=nilearn.decoding.SpaceNetClassifier><code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.decoding.SpaceNetClassifier</span></code></a> implements spatial penalties which improve brain decoding power as well as decoder maps:</p><ul class=simple><li><p>penalty=”tvl1”: priors inspired from TV (Total Variation) [Michel <em>et al.</em> <a class="footnote-reference brackets"href=#michel-inria-00563468 id=id1>1</a>], TV-L1 [Baldassarre <em>et al.</em> <a class="footnote-reference brackets"href=#baldassarre2012 id=id2>2</a>], [Gramfort <em>et al.</em> <a class="footnote-reference brackets"href=#gramfort-hal-00839984 id=id3>3</a>].</p></li><li><p>penalty=”graph-net”: GraphNet prior [Grosenick <em>et al.</em> <a class="footnote-reference brackets"href=#grosenick2013304 id=id4>4</a>].</p></li></ul><p>These regularize <a class="reference internal"href=../glossary.html#term-classification><span class="xref std std-term">classification</span></a> and <a class="reference internal"href=../glossary.html#term-regression><span class="xref std std-term">regression</span></a> problems in brain imaging. The results are brain maps which are both sparse (i.e regression coefficients are zero everywhere, except at predictive <a class="reference internal"href=../glossary.html#term-voxel><span class="xref std std-term">voxels</span></a>) and structured (blobby). The superiority of TV-L1 over methods without structured priors like the Lasso, <a class="reference internal"href=../glossary.html#term-SVM><span class="xref std std-term">SVM</span></a>, <a class="reference internal"href=../glossary.html#term-ANOVA><span class="xref std std-term">ANOVA</span></a>, Ridge, etc. for yielding more interpretable maps and improved prediction scores is now well established [Baldassarre <em>et al.</em> <a class="footnote-reference brackets"href=#baldassarre2012 id=id5>2</a>], [Gramfort <em>et al.</em> <a class="footnote-reference brackets"href=#gramfort-hal-00839984 id=id6>3</a>], [Grosenick <em>et al.</em> <a class="footnote-reference brackets"href=#grosenick2013304 id=id7>4</a>].</p><p>Note that TV-L1 prior leads to a difficult optimization problem, and so can be slow to run. Under the hood, a few heuristics are used to make things a bit faster. These include:</p><ul class=simple><li><p>Feature preprocessing, where an F-test is used to eliminate non-predictive <a class="reference internal"href=../glossary.html#term-voxel><span class="xref std std-term">voxels</span></a>, thus reducing the size of the brain mask in a principled way.</p></li><li><p>Continuation is used along the regularization path, where the solution of the optimization problem for a given value of the regularization parameter <cite>alpha</cite> is used as initialization for the next regularization (smaller) value on the regularization grid.</p></li></ul><p><strong>Implementation:</strong> See [Dohmatob <em>et al.</em> <a class="footnote-reference brackets"href=#dohmatob-hal-01147731 id=id8>5</a>] and [Dohmatob <em>et al.</em> <a class="footnote-reference brackets"href=#dohmatob-hal-00991743 id=id9>6</a>] for technical details regarding the implementation of SpaceNet.</p></div><div class=section id=related-example><h2><span class=section-number>2.4.2. </span>Related example<a title="Permalink to this headline"class=headerlink href=#related-example>¶</a></h2><p><a class="reference internal"href=../auto_examples/02_decoding/plot_oasis_vbm_space_net.html#sphx-glr-auto-examples-02-decoding-plot-oasis-vbm-space-net-py><span class="std std-ref">Age prediction on OASIS dataset with SpaceNet</span></a>.</p><div class="figure align-default"><img alt=../_images/sphx_glr_plot_oasis_vbm_space_net_002.png src=../_images/sphx_glr_plot_oasis_vbm_space_net_002.png></div><div class="admonition note"><p class=admonition-title>Note</p><p>Empirical comparisons using this method have been removed from documentation in version 0.7 to keep its computational cost low. You can easily try SpaceNet instead of FREM in <a class="reference internal"href=../auto_examples/02_decoding/plot_mixed_gambles_frem.html#sphx-glr-auto-examples-02-decoding-plot-mixed-gambles-frem-py><span class="std std-ref">mixed gambles study</span></a> or <a class="reference internal"href=../auto_examples/02_decoding/plot_haxby_frem.html#sphx-glr-auto-examples-02-decoding-plot-haxby-frem-py><span class="std std-ref">Haxby study</span></a>.</p></div><div class="admonition seealso"><p class=admonition-title>See also</p><p><a class="reference internal"href=frem.html#frem><span class="std std-ref">FREM</span></a>, a pipeline ensembling many models that yields very good decoding performance at a lower computational cost.</p></div></div><div class=section id=references><h2><span class=section-number>2.4.3. </span>References<a title="Permalink to this headline"class=headerlink href=#references>¶</a></h2><p><dl class="footnote brackets"><dt class=label id=michel-inria-00563468><span class=brackets><a class=fn-backref href=#id1>1</a></span></dt><dd><p>Vincent Michel, Alexandre Gramfort, Gaël Varoquaux, Evelyn Eger, and Bertrand Thirion. Total variation regularization for fMRI-based prediction of behaviour. <em>IEEE Transactions on Medical Imaging</em>, 30(7):1328 – 1340, February 2011. URL: <a class="reference external"href=https://hal.inria.fr/inria-00563468>https://hal.inria.fr/inria-00563468</a>, <a class="reference external"href=https://doi.org/10.1109/TMI.2011.2113378>doi:10.1109/TMI.2011.2113378</a>.</p></dd><dt class=label id=baldassarre2012><span class=brackets>2</span><span class=fn-backref>(<a href=#id2>1</a>,<a href=#id5>2</a>)</span></dt><dd><p>Luca Baldassarre, Janaina Mourao-Miranda, and Massimiliano Pontil. Structured sparsity models for brain decoding from fmri data. In <em>2012 Second International Workshop on Pattern Recognition in NeuroImaging</em>, volume, 5–8. 2012. URL: <a class="reference external"href=http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/neurosparse_prni.pdf>http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/neurosparse_prni.pdf</a>, <a class="reference external"href=https://doi.org/10.1109/PRNI.2012.31>doi:10.1109/PRNI.2012.31</a>.</p></dd><dt class=label id=gramfort-hal-00839984><span class=brackets>3</span><span class=fn-backref>(<a href=#id3>1</a>,<a href=#id6>2</a>)</span></dt><dd><p>Alexandre Gramfort, Bertrand Thirion, and Gaël Varoquaux. Identifying predictive regions from fMRI with TV-L1 prior. In <em>Pattern Recognition in Neuroimaging (PRNI)</em>. Philadelphia, United States, June 2013. IEEE. URL: <a class="reference external"href=https://hal.inria.fr/hal-00839984>https://hal.inria.fr/hal-00839984</a>.</p></dd><dt class=label id=grosenick2013304><span class=brackets>4</span><span class=fn-backref>(<a href=#id4>1</a>,<a href=#id7>2</a>)</span></dt><dd><p>Logan Grosenick, Brad Klingenberg, Kiefer Katovich, Brian Knutson, and Jonathan E. Taylor. Interpretable whole-brain prediction analysis with graphnet. <em>NeuroImage</em>, 72:304–321, 2013. URL: <a class="reference external"href=https://www.sciencedirect.com/science/article/pii/S1053811912012487>https://www.sciencedirect.com/science/article/pii/S1053811912012487</a>, <a class="reference external"href=https://doi.org/https://doi.org/10.1016/j.neuroimage.2012.12.062>doi:https://doi.org/10.1016/j.neuroimage.2012.12.062</a>.</p></dd><dt class=label id=dohmatob-hal-01147731><span class=brackets><a class=fn-backref href=#id8>5</a></span></dt><dd><p>Elvis Dohmatob, Michael Eickenberg, Bertrand Thirion, and Gaël Varoquaux. Speeding-up model-selection in GraphNet via early-stopping and univariate feature-screening. In <em>PRNI</em>. Stanford, United States, June 2015. URL: <a class="reference external"href=https://hal.inria.fr/hal-01147731>https://hal.inria.fr/hal-01147731</a>.</p></dd><dt class=label id=dohmatob-hal-00991743><span class=brackets><a class=fn-backref href=#id9>6</a></span></dt><dd><p>Elvis Dohmatob, Alexandre Gramfort, Bertrand Thirion, and Gaël Varoquaux. Benchmarking solvers for TV-l1 least-squares and logistic regression in brain imaging. In <em>PRNI 2014 - 4th International Workshop on Pattern Recognition in NeuroImaging</em>. Tübingen, Germany, June 2014. IEEE. URL: <a class="reference external"href=https://hal.inria.fr/hal-00991743>https://hal.inria.fr/hal-00991743</a>.</p></dd></dl></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>2.4. SpaceNet: decoding with spatial structure for better maps</a><ul><li><a class="reference internal"href=#the-spacenet-decoder>2.4.1. The SpaceNet decoder</a></li><li><a class="reference internal"href=#related-example>2.4.2. Related example</a></li><li><a class="reference internal"href=#references>2.4.3. References</a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=frem.html><span class=section-number>2.3. </span>FREM: fast ensembling of regularized models for robust decoding</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=searchlight.html><span class=section-number>2.5. </span>Searchlight : finding voxels containing information</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../_sources/decoding/space_net.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>