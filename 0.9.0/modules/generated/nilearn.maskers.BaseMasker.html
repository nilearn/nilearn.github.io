<!doctypehtml><html lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1.0 name=viewport><meta content="8.7.1. nilearn.maskers.BaseMasker"property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/modules/generated/nilearn.maskers.BaseMasker.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Examples using nilearn.maskers.BaseMasker: The haxby dataset: different multi-class strategies The haxby dataset: different multi-class strategies, Searchlight analysis of face vs house recognition..."property=og:description><meta content=../../_images/sphx_glr_plot_haxby_multiclass_thumb.png property=og:image><meta content="The haxby dataset: different multi-class strategies"property=og:image:alt><title>Nilearn: Statistical Analysis for NeuroImaging in Python — Machine learning for NeuroImaging</title><link href=../../_static/pygments.css rel=stylesheet><link href=../../_static/nature.css rel=stylesheet><link href=../../_static/copybutton.css rel=stylesheet><link href=../../_static/sg_gallery.css rel=stylesheet><link href=../../_static/sg_gallery-binder.css rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css rel=stylesheet><script data-url_root=../../ id=documentation_options src=../../_static/documentation_options.js></script><script src=../../_static/jquery.js></script><script src=../../_static/underscore.js></script><script src=../../_static/doctools.js></script><script src=../../_static/clipboard.min.js></script><script src=../../_static/copybutton.js></script><link rel="shortcut icon"href=../../_static/favicon.ico><link href=../../search.html rel=search title=Search><link title="8.7.2. nilearn.maskers.NiftiMasker"href=nilearn.maskers.NiftiMasker.html rel=next><link title="8.6.3.1. nilearn.interfaces.fsl.get_design_from_fslmat"href=nilearn.interfaces.fsl.get_design_from_fslmat.html rel=prev><meta content=True name=HandheldFriendly><meta content=width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0 name=viewport><meta content="nilearn, neuroimaging, python, neuroscience, machinelearning"name=keywords><script>function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});</script><script>function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});</script><script>var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();</script><body><div id=logo-banner><div class=logo><a href=../../index.html> <img alt="Nilearn logo"border=0 src=../../_static/nilearn-logo.png> </a></div><div class=tags><ul><li><big><a href=../../auto_examples/decoding/plot_haxby_anova_svm.html>SVM</a></big></li><li><small><a href=../../connectivity/parcellating.html>Ward clustering</a></small></li><li><a href=../../decoding/searchlight.html>Searchlight</a></li><li><big><a href=../../connectivity/resting_state_networks.html>ICA</a></big></li><li><a href=../../manipulating_images/data_preparation.html>Nifti IO</a></li><li><a href=../reference.html#module-nilearn.datasets>Datasets</a></li></ul></div><div class=banner><h1>Nilearn:</h1><h2>Statistics for NeuroImaging in Python</h2></div><div class=search_form><div class=gcse-search id=cse style=width:100%></div><script>(function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();</script></div></div><div class=related-wrapper><div aria-label="related navigation"class=related role=navigation><h3>Navigation</h3><ul><li class=right style=margin-right:10px><a title="Python Module Index"href=../../py-modindex.html>modules</a></li><li class=right><a title="8.7.2. nilearn.maskers.NiftiMasker"accesskey=N href=nilearn.maskers.NiftiMasker.html>next</a> |</li><li class=right><a title="8.6.3.1. nilearn.interfaces.fsl.get_design_from_fslmat"accesskey=P href=nilearn.interfaces.fsl.get_design_from_fslmat.html>previous</a> |</li><li><a href=../../index.html>Nilearn Home</a> | </li><li><a href=../../user_guide.html>User Guide</a> | </li><li><a href=../../auto_examples/index.html>Examples</a> | </li><li><a href=../reference.html>Reference</a> | </li><li id=navbar-about><a href=../../authors.html>About</a>| </li><li><a href=../../glossary.html>Glossary</a>| </li><li><a href=../../bibliography.html>Bibliography</a>| </li><li id=navbar-ecosystem><a href=http://www.nipy.org/>Nipy ecosystem</a></li><li class="nav-item nav-item-1"><a href=../../user_guide.html>User guide: table of contents</a> »</li><li class="nav-item nav-item-2"><a accesskey=U href=../reference.html><span class=section-number>8. </span>Reference documentation: all nilearn functions</a> »</li><li class="nav-item nav-item-this"><a href>Nilearn: Statistical Analysis for NeuroImaging in Python</a></li></ul></div></div><div class=stable-banner>This is the <em>stable</em> documentation for the latest release of Nilearn, the current development version is available <a href=https://nilearn.github.io/dev/index.html>here</a>.</div><div class=document><div class=documentwrapper><div class=bodywrapper><div class=body role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the class signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><div class=section id=nilearn-maskers-basemasker><h1><span class=section-number>8.7.1. </span>nilearn.maskers.BaseMasker<a title="Permalink to this headline"class=headerlink href=#nilearn-maskers-basemasker>¶</a></h1><dl class="py class"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker><em class=property><span class=pre>class</span> </em><span class="sig-prename descclassname"><span class=pre>nilearn.maskers.</span></span><span class="sig-name descname"><span class=pre>BaseMasker</span></span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/72e810f01/nilearn/maskers/base_masker.py#L124><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker>¶</a></dt><dd><p>Base class for NiftiMaskers.</p> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.__init__><span class="sig-name descname"><span class=pre>__init__</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>*</span></span><span class=n><span class=pre>args</span></span></em>, <em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>kwargs</span></span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.__init__>¶</a></dt><dd><p>Initialize self. See help(type(self)) for accurate signature.</p></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.transform_single_imgs><em class=property><span class=pre>abstract</span> </em><span class="sig-name descname"><span class=pre>transform_single_imgs</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>copy</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/72e810f01/nilearn/maskers/base_masker.py#L127><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.transform_single_imgs>¶</a></dt><dd><p>Extract signals from a single 4D niimg.</p> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier>3D/4D Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Images to process. It must boil down to a 4D image with scans number as last dimension.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details. shape: (number of scans, number of confounds)</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>shape: (number of scans - number of volumes removed, ) Masks the niimgs along time/fourth dimension to perform scrubbing (remove volumes with high motion) and/or non-steady-state volumes. This parameter is passed to signal.clean.</p> <blockquote><div><div class=versionadded><p><span class="versionmodified added">New in version 0.8.0.</span></p></div></div></blockquote></dd><dt><strong>copy</strong><span class=classifier>Boolean, optional</span></dt><dd><p>Indicates whether a copy is returned or not. Default=True.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>region_signals</strong><span class=classifier>2D numpy.ndarray</span></dt><dd><p>Signal for each element. shape: (number of scans, number of elements)</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.transform><span class="sig-name descname"><span class=pre>transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/72e810f01/nilearn/maskers/base_masker.py#L164><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.transform>¶</a></dt><dd><p>Apply mask, spatial and temporal preprocessing</p> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier>3D/4D Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a> Images to process. It must boil down to a 4D image with scans number as last dimension.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details. shape: (number of scans, number of confounds)</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>shape: (number of scans - number of volumes removed, ) Masks the niimgs along time/fourth dimension to perform scrubbing (remove volumes with high motion) and/or non-steady-state volumes. This parameter is passed to signal.clean.</p> <blockquote><div><div class=versionadded><p><span class="versionmodified added">New in version 0.8.0.</span></p></div></div></blockquote></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>region_signals</strong><span class=classifier>2D numpy.ndarray</span></dt><dd><p>Signal for each element. shape: (number of scans, number of elements)</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.fit_transform><span class="sig-name descname"><span class=pre>fit_transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>X</span></span></em>, <em class=sig-param><span class=n><span class=pre>y</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>fit_params</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/72e810f01/nilearn/maskers/base_masker.py#L216><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.fit_transform>¶</a></dt><dd><p>Fit to data, then transform it</p> <dl class=field-list><dt class=field-odd>Parameters</dt><dd class=field-odd><dl><dt><strong>X</strong><span class=classifier>Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></dd><dt><strong>y</strong><span class=classifier>numpy array of shape [n_samples], optional</span></dt><dd><p>Target values.</p></dd><dt><strong>confounds</strong><span class=classifier>list of confounds, optional</span></dt><dd><p>List of confounds (2D arrays or filenames pointing to CSV files). Must be of same length than imgs_list.</p></dd><dt><strong>sample_mask</strong><span class=classifier>list of sample_mask, optional</span></dt><dd><p>List of sample_mask (1D arrays) if scrubbing motion outliers. Must be of same length than imgs_list.</p> <blockquote><div><div class=versionadded><p><span class="versionmodified added">New in version 0.8.0.</span></p></div></div></blockquote></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>X_new</strong><span class=classifier>numpy array of shape [n_samples, n_features_new]</span></dt><dd><p>Transformed array.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.inverse_transform><span class="sig-name descname"><span class=pre>inverse_transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>X</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/72e810f01/nilearn/maskers/base_masker.py#L272><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.inverse_transform>¶</a></dt><dd><p>Transform the 2D data matrix back to an image in brain space.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>X</strong><span class=classifier>Niimg-like object</span></dt><dd><p>See <a class="reference external"href=http://nilearn.github.io/manipulating_images/input_output.html>http://nilearn.github.io/manipulating_images/input_output.html</a></p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>img</strong><span class=classifier>Transformed image in brain space.</span></dt><dd></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.get_params><span class="sig-name descname"><span class=pre>get_params</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>deep</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.get_params>¶</a></dt><dd><p>Get parameters for this estimator.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>deep</strong><span class=classifier>bool, default=True</span></dt><dd><p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>params</strong><span class=classifier>dict</span></dt><dd><p>Parameter names mapped to their values.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.BaseMasker.set_params><span class="sig-name descname"><span class=pre>set_params</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>params</span></span></em><span class=sig-paren>)</span><a title="Permalink to this definition"class=headerlink href=#nilearn.maskers.BaseMasker.set_params>¶</a></dt><dd><p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference external"title="(in scikit-learn v1.0)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>). The latter have parameters of the form <code class="docutils literal notranslate"><span class=pre>&LTcomponent>__&LTparameter></span></code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"><dt class=field-odd>Parameters</dt><dd class=field-odd><dl class=simple><dt><strong>**params</strong><span class=classifier>dict</span></dt><dd><p>Estimator parameters.</p></dd></dl></dd><dt class=field-even>Returns</dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>estimator instance</span></dt><dd><p>Estimator instance.</p></dd></dl></dd></dl></dd></dl></dd></dl><div class=section id=examples-using-nilearn-maskers-basemasker><h2><span class=section-number>8.7.1.1. </span>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.maskers.BaseMasker</span></code><a title="Permalink to this headline"class=headerlink href=#examples-using-nilearn-maskers-basemasker>¶</a></h2><div tooltip="We compare one vs all and one vs one multi-class strategies: the overall cross-validated accura..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id1><img alt="The haxby dataset: different multi-class strategies"src=../../_images/sphx_glr_plot_haxby_multiclass_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_multiclass.html#sphx-glr-auto-examples-02-decoding-plot-haxby-multiclass-py><span class="std std-ref">The haxby dataset: different multi-class strategies</span></a></span><a title="Permalink to this image"class=headerlink href=#id1>¶</a></p></div></div><div tooltip="Searchlight analysis requires fitting a classifier a large amount of times. As a result, it is ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id2><img alt="Searchlight analysis of face vs house recognition"src=../../_images/sphx_glr_plot_haxby_searchlight_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight.html#sphx-glr-auto-examples-02-decoding-plot-haxby-searchlight-py><span class="std std-ref">Searchlight analysis of face vs house recognition</span></a></span><a title="Permalink to this image"class=headerlink href=#id2>¶</a></p></div></div><div tooltip='In this script we reproduce the data analysis conducted by Haxby et al. in "Distributed and Ove...'class=sphx-glr-thumbcontainer><div class="figure align-default"id=id3><img alt="ROI-based decoding analysis in Haxby et al. dataset"src=../../_images/sphx_glr_plot_haxby_full_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_full_analysis.html#sphx-glr-auto-examples-02-decoding-plot-haxby-full-analysis-py><span class="std std-ref">ROI-based decoding analysis in Haxby et al. dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id3>¶</a></p></div></div><div tooltip="This example uses Voxel-Based Morphometry (VBM) to study the relationship between aging and gra..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id4><img alt="Voxel-Based Morphometry on Oasis dataset"src=../../_images/sphx_glr_plot_oasis_vbm_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_oasis_vbm.html#sphx-glr-auto-examples-02-decoding-plot-oasis-vbm-py><span class="std std-ref">Voxel-Based Morphometry on Oasis dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id4>¶</a></p></div></div><div tooltip="This example partly reproduces the encoding model presented in     `Visual image reconstruction..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id5><img alt="Encoding models for visual stimuli from Miyawaki et al. 2008"src=../../_images/sphx_glr_plot_miyawaki_encoding_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_encoding.html#sphx-glr-auto-examples-02-decoding-plot-miyawaki-encoding-py><span class="std std-ref">Encoding models for visual stimuli from Miyawaki et al. 2008</span></a></span><a title="Permalink to this image"class=headerlink href=#id5>¶</a></p></div></div><div tooltip="This example reproduces the experiment presented in     `Visual image reconstruction from human..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id6><img alt="Reconstruction of visual stimuli from Miyawaki et al. 2008"src=../../_images/sphx_glr_plot_miyawaki_reconstruction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_reconstruction.html#sphx-glr-auto-examples-02-decoding-plot-miyawaki-reconstruction-py><span class="std std-ref">Reconstruction of visual stimuli from Miyawaki et al. 2008</span></a></span><a title="Permalink to this image"class=headerlink href=#id6>¶</a></p></div></div><div tooltip="This example extracts the signal on regions defined via a probabilistic atlas, to construct a f..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id7><img alt="Extracting signals of a probabilistic atlas of functional regions"src=../../_images/sphx_glr_plot_probabilistic_atlas_extraction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_probabilistic_atlas_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-probabilistic-atlas-extraction-py><span class="std std-ref">Extracting signals of a probabilistic atlas of functional regions</span></a></span><a title="Permalink to this image"class=headerlink href=#id7>¶</a></p></div></div><div tooltip="This example constructs a functional connectome using the sparse inverse covariance."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id8><img alt="Computing a connectome with sparse inverse covariance"src=../../_images/sphx_glr_plot_inverse_covariance_connectome_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_inverse_covariance_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-inverse-covariance-connectome-py><span class="std std-ref">Computing a connectome with sparse inverse covariance</span></a></span><a title="Permalink to this image"class=headerlink href=#id8>¶</a></p></div></div><div tooltip="This example shows how to produce seed-to-voxel correlation maps for a single subject based on ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id9><img alt="Producing single subject maps of seed-to-voxel correlation"src=../../_images/sphx_glr_plot_seed_to_voxel_correlation_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_seed_to_voxel_correlation.html#sphx-glr-auto-examples-03-connectivity-plot-seed-to-voxel-correlation-py><span class="std std-ref">Producing single subject maps of seed-to-voxel correlation</span></a></span><a title="Permalink to this image"class=headerlink href=#id9>¶</a></p></div></div><div tooltip="This example shows how to estimate a connectome on a group of subjects using the group sparse i..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id10><img alt="Group Sparse inverse covariance for multi-subject connectome"src=../../_images/sphx_glr_plot_multi_subject_connectome_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_multi_subject_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-multi-subject-connectome-py><span class="std std-ref">Group Sparse inverse covariance for multi-subject connectome</span></a></span><a title="Permalink to this image"class=headerlink href=#id10>¶</a></p></div></div><div tooltip="This example shows how to use nilearn.regions.RegionExtractor to extract spatially constrained ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id11><img alt="Regions extraction using :term:`Dictionary learning` and functional connectomes"src=../../_images/sphx_glr_plot_extract_regions_dictlearning_maps_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_extract_regions_dictlearning_maps.html#sphx-glr-auto-examples-03-connectivity-plot-extract-regions-dictlearning-maps-py><span class="std std-ref">Regions extraction using Dictionary learning and functional connectomes</span></a></span><a title="Permalink to this image"class=headerlink href=#id11>¶</a></p></div></div><div tooltip="This examples shows how to turn a parcellation into connectome for visualization. This requires..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id12><img alt="Comparing connectomes on different reference atlases"src=../../_images/sphx_glr_plot_atlas_comparison_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_atlas_comparison.html#sphx-glr-auto-examples-03-connectivity-plot-atlas-comparison-py><span class="std std-ref">Comparing connectomes on different reference atlases</span></a></span><a title="Permalink to this image"class=headerlink href=#id12>¶</a></p></div></div><div tooltip="This example compares different kinds of functional connectivity between regions of interest : ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id13><img alt="Classification of age groups using functional connectivity"src=../../_images/sphx_glr_plot_group_level_connectivity_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_group_level_connectivity.html#sphx-glr-auto-examples-03-connectivity-plot-group-level-connectivity-py><span class="std std-ref">Classification of age groups using functional connectivity</span></a></span><a title="Permalink to this image"class=headerlink href=#id13>¶</a></p></div></div><div tooltip="Here we show how to extract signals from a brain parcellation and compute a correlation matrix."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id14><img alt="Extracting signals from a brain parcellation"src=../../_images/sphx_glr_plot_signal_extraction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_signal_extraction.html#sphx-glr-auto-examples-03-connectivity-plot-signal-extraction-py><span class="std std-ref">Extracting signals from a brain parcellation</span></a></span><a title="Permalink to this image"class=headerlink href=#id14>¶</a></p></div></div><div tooltip="This example shows how to extract signals from spherical regions. We show how to build spheres ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id15><img alt="Extract signals on spheres and plot a connectome"src=../../_images/sphx_glr_plot_sphere_based_connectome_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/03_connectivity/plot_sphere_based_connectome.html#sphx-glr-auto-examples-03-connectivity-plot-sphere-based-connectome-py><span class="std std-ref">Extract signals on spheres and plot a connectome</span></a></span><a title="Permalink to this image"class=headerlink href=#id15>¶</a></p></div></div><div tooltip="This example shows a full step-by-step workflow of fitting a GLM to data extracted from a seed ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id16><img alt="Default Mode Network extraction of AHDH dataset"src=../../_images/sphx_glr_plot_adhd_dmn_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_adhd_dmn.html#sphx-glr-auto-examples-04-glm-first-level-plot-adhd-dmn-py><span class="std std-ref">Default Mode Network extraction of AHDH dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id16>¶</a></p></div></div><div tooltip="Here we fit a First Level GLM with the minimize_memory-argument set to False. By doing so, the ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id17><img alt="Predicted time series and residuals"src=../../_images/sphx_glr_plot_predictions_residuals_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_predictions_residuals.html#sphx-glr-auto-examples-04-glm-first-level-plot-predictions-residuals-py><span class="std std-ref">Predicted time series and residuals</span></a></span><a title="Permalink to this image"class=headerlink href=#id17>¶</a></p></div></div><div tooltip="This simple example shows how to extract regions from Smith atlas resting state networks."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id18><img alt="Regions Extraction of Default Mode Networks using Smith Atlas"src=../../_images/sphx_glr_plot_extract_rois_smith_atlas_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_extract_rois_smith_atlas.html#sphx-glr-auto-examples-06-manipulating-images-plot-extract-rois-smith-atlas-py><span class="std std-ref">Regions Extraction of Default Mode Networks using Smith Atlas</span></a></span><a title="Permalink to this image"class=headerlink href=#id18>¶</a></p></div></div><div tooltip="Here is a simple example of automatic mask computation using the nifti masker. The mask is comp..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id19><img alt="Simple example of NiftiMasker use"src=../../_images/sphx_glr_plot_nifti_simple_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_nifti_simple.html#sphx-glr-auto-examples-06-manipulating-images-plot-nifti-simple-py><span class="std std-ref">Simple example of NiftiMasker use</span></a></span><a title="Permalink to this image"class=headerlink href=#id19>¶</a></p></div></div><div tooltip="This simple example shows how to extract signals from functional fmri data and brain regions de..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id20><img alt="Extracting signals from brain regions using the NiftiLabelsMasker"src=../../_images/sphx_glr_plot_nifti_labels_simple_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_nifti_labels_simple.html#sphx-glr-auto-examples-06-manipulating-images-plot-nifti-labels-simple-py><span class="std std-ref">Extracting signals from brain regions using the NiftiLabelsMasker</span></a></span><a title="Permalink to this image"class=headerlink href=#id20>¶</a></p></div></div><div tooltip="In this example, the Nifti masker is used to automatically compute a mask."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id21><img alt="Understanding NiftiMasker and mask computation"src=../../_images/sphx_glr_plot_mask_computation_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_mask_computation.html#sphx-glr-auto-examples-06-manipulating-images-plot-mask-computation-py><span class="std std-ref">Understanding NiftiMasker and mask computation</span></a></span><a title="Permalink to this image"class=headerlink href=#id21>¶</a></p></div></div><div tooltip="This example shows manual steps to create and further modify an ROI spatial mask. They represen..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id22><img alt="Computing a Region of Interest (ROI) mask manually"src=../../_images/sphx_glr_plot_roi_extraction_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_roi_extraction.html#sphx-glr-auto-examples-06-manipulating-images-plot-roi-extraction-py><span class="std std-ref">Computing a Region of Interest (ROI) mask manually</span></a></span><a title="Permalink to this image"class=headerlink href=#id22>¶</a></p></div></div><div tooltip=" This example is meant to demonstrate nilearn as a low-level tools used to combine feature extr..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id23><img alt="Multivariate decompositions: Independent component analysis of fMRI"src=../../_images/sphx_glr_plot_ica_resting_state_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_resting_state.html#sphx-glr-auto-examples-07-advanced-plot-ica-resting-state-py><span class="std std-ref">Multivariate decompositions: Independent component analysis of fMRI</span></a></span><a title="Permalink to this image"class=headerlink href=#id23>¶</a></p></div></div><div tooltip="This example shows how to use the Localizer dataset in a basic analysis. A standard Anova is pe..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id24><img alt="Massively univariate analysis of a calculation task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_simple_analysis_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_simple_analysis.html#sphx-glr-auto-examples-07-advanced-plot-localizer-simple-analysis-py><span class="std std-ref">Massively univariate analysis of a calculation task from the Localizer dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id24>¶</a></p></div></div><div tooltip="This example compares different kinds of functional connectivity between regions of interest : ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id25><img alt="Functional connectivity predicts age group"src=../../_images/sphx_glr_plot_age_group_prediction_cross_val_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_age_group_prediction_cross_val.html#sphx-glr-auto-examples-07-advanced-plot-age-group-prediction-cross-val-py><span class="std std-ref">Functional connectivity predicts age group</span></a></span><a title="Permalink to this image"class=headerlink href=#id25>¶</a></p></div></div><div tooltip="This example shows the results obtained in a massively univariate analysis performed at the int..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id26><img alt="Massively univariate analysis of a motor task from the Localizer dataset"src=../../_images/sphx_glr_plot_localizer_mass_univariate_methods_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html#sphx-glr-auto-examples-07-advanced-plot-localizer-mass-univariate-methods-py><span class="std std-ref">Massively univariate analysis of a motor task from the Localizer dataset</span></a></span><a title="Permalink to this image"class=headerlink href=#id26>¶</a></p></div></div><div tooltip="This example shows how to download statistical maps from NeuroVault, label them with NeuroSynth..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id27><img alt="NeuroVault cross-study ICA maps."src=../../_images/sphx_glr_plot_ica_neurovault_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_neurovault.html#sphx-glr-auto-examples-07-advanced-plot-ica-neurovault-py><span class="std std-ref">NeuroVault cross-study ICA maps.</span></a></span><a title="Permalink to this image"class=headerlink href=#id27>¶</a></p></div></div><div tooltip="A permuted Ordinary Least Squares algorithm is run at each voxel in order to determine whether ..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id28><img alt="Massively univariate analysis of face vs house recognition"src=../../_images/sphx_glr_plot_haxby_mass_univariate_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_haxby_mass_univariate.html#sphx-glr-auto-examples-07-advanced-plot-haxby-mass-univariate-py><span class="std std-ref">Massively univariate analysis of face vs house recognition</span></a></span><a title="Permalink to this image"class=headerlink href=#id28>¶</a></p></div></div><div tooltip="This tutorial opens the box of decoding pipelines to bridge integrated functionalities provided..."class=sphx-glr-thumbcontainer><div class="figure align-default"id=id29><img alt="Advanced decoding using scikit learn"src=../../_images/sphx_glr_plot_advanced_decoding_scikit_thumb.png><p class=caption><span class=caption-text><a class="reference internal"href=../../auto_examples/07_advanced/plot_advanced_decoding_scikit.html#sphx-glr-auto-examples-07-advanced-plot-advanced-decoding-scikit-py><span class="std std-ref">Advanced decoding using scikit learn</span></a></span><a title="Permalink to this image"class=headerlink href=#id29>¶</a></p></div></div><div style=clear:both></div></div></div><div class=clearer></div></div></div></div><div aria-label="main navigation"class=sphinxsidebar role=navigation><div class=sphinxsidebarwrapper><h4>Giving credit</h4><ul class=simple><li><p>Please consider <a href=../../authors.html#citing>citing the papers</a>.</p></li></ul><h3><a href=../../index.html>Table of Contents</a></h3><ul><li><a class="reference internal"href=#>8.7.1. nilearn.maskers.BaseMasker</a><ul><li><a class="reference internal"href=#examples-using-nilearn-maskers-basemasker>8.7.1.1. Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.maskers.BaseMasker</span></code></a></li></ul></li></ul><h4>Previous topic</h4><p class=topless><a title="previous chapter"href=nilearn.interfaces.fsl.get_design_from_fslmat.html><span class=section-number>8.6.3.1. </span>nilearn.interfaces.fsl.get_design_from_fslmat</a></p><h4>Next topic</h4><p class=topless><a title="next chapter"href=nilearn.maskers.NiftiMasker.html><span class=section-number>8.7.2. </span>nilearn.maskers.NiftiMasker</a></p><div id=searchbox role=search style=display:none><h3 id=searchlabel>Quick search</h3><div class=searchformwrapper><form action=../../search.html class=search><input aria-labelledby=searchlabel name=q><input type=submit value=Go></form></div></div><script>$('#searchbox').show(0);</script></div></div><div class=clearer></div></div><div class=footer>© The nilearn developers 2010-2022. Created using <a href=http://sphinx.pocoo.org/>Sphinx</a> 4.0.2. <span style=padding-left:5ex> <a href=../../_sources/modules/generated/nilearn.maskers.BaseMasker.rst.txt rel=nofollow>Show this page source</a> </span></div></body></html>