<!doctypehtml><html class=no-js data-content_root=../../ lang=en><meta charset=utf-8><meta content=width=device-width,initial-scale=1 name=viewport><meta content="light dark"name=color-scheme><meta content=width=device-width,initial-scale=1 name=viewport><meta content=nilearn.maskers.MultiNiftiMapsMasker property=og:title><meta content=website property=og:type><meta content=https://nilearn.github.io/modules/generated/nilearn.maskers.MultiNiftiMapsMasker.html property=og:url><meta content=Nilearn property=og:site_name><meta content="Examples using nilearn.maskers.MultiNiftiMapsMasker: Comparing connectomes on different reference atlases"property=og:description><meta content=https://nilearn.github.io/_images/sphx_glr_plot_atlas_comparison_thumb.png property=og:image><meta property=og:image:alt><meta content="Examples using nilearn.maskers.MultiNiftiMapsMasker: Comparing connectomes on different reference atlases"name=description><link href=../../search.html rel=search title=Search><link href=nilearn.maskers.NiftiSpheresMasker.html rel=next title=nilearn.maskers.NiftiSpheresMasker><link href=nilearn.maskers.NiftiMapsMasker.html rel=prev title=nilearn.maskers.NiftiMapsMasker><link rel="shortcut icon"href=../../_static/favicon.ico><title>nilearn.maskers.MultiNiftiMapsMasker - Nilearn</title><link href=../../_static/pygments.css?v=045299b1 rel=stylesheet><link href=../../_static/styles/furo.css?v=135e06be rel=stylesheet><link href=../../_static/copybutton.css?v=76b2166b rel=stylesheet><link href=../../_static/sg_gallery.css?v=61a4c737 rel=stylesheet><link href=../../_static/sg_gallery-binder.css?v=f4aeca0c rel=stylesheet><link href=../../_static/sg_gallery-dataframe.css?v=2082cf3c rel=stylesheet><link href=../../_static/sg_gallery-rendered-html.css?v=1277b6f3 rel=stylesheet><link href=../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7 rel=stylesheet><link href=../../_static/styles/furo-extensions.css?v=36a5483c rel=stylesheet><link href=../../_static/custom.css?v=486f7390 rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/fontawesome.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/solid.min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/brands.min.css rel=stylesheet><style>body{--color-code-background:#fff;--color-code-foreground:black;--admonition-font-size:100%;--admonition-title-font-size:100%;--color-announcement-background:#fbb360;--color-announcement-text:#111418;--color-admonition-title--note:#448aff;--color-admonition-title-background--note:#448aff10}@media not print{body[data-theme=dark]{--color-code-background:#232629;--color-code-foreground:#ccc;--color-announcement-background:#935610;--color-announcement-text:#fff}@media (prefers-color-scheme:dark){body:not([data-theme=light]){--color-code-background:#232629;--color-code-foreground:#ccc;--color-announcement-background:#935610;--color-announcement-text:#fff}}}</style><body><script>document.body.dataset.theme = localStorage.getItem("theme") || "auto";</script><svg style=display:none xmlns=http://www.w3.org/2000/svg><symbol viewbox="0 0 24 24"id=svg-toc><title>Contents</title><svg viewbox="0 0 1024 1024"fill=currentColor stroke=currentColor stroke-width=0><path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/></svg></symbol><symbol viewbox="0 0 24 24"id=svg-menu><title>Menu</title><svg viewbox="0 0 24 24"class=feather-menu fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 xmlns=http://www.w3.org/2000/svg><line x1=3 x2=21 y1=12 y2=12></line><line x1=3 x2=21 y1=6 y2=6></line><line x1=3 x2=21 y1=18 y2=18></line></svg></symbol><symbol viewbox="0 0 24 24"id=svg-arrow-right><title>Expand</title><svg viewbox="0 0 24 24"class=feather-chevron-right fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=2 xmlns=http://www.w3.org/2000/svg><polyline points="9 18 15 12 9 6"></polyline></svg></symbol><symbol viewbox="0 0 24 24"id=svg-sun><title>Light mode</title><svg viewbox="0 0 24 24"class=feather-sun fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1.5 xmlns=http://www.w3.org/2000/svg><circle cx=12 cy=12 r=5></circle><line x1=12 x2=12 y1=1 y2=3></line><line x1=12 x2=12 y1=21 y2=23></line><line x1=4.22 x2=5.64 y1=4.22 y2=5.64></line><line x1=18.36 x2=19.78 y1=18.36 y2=19.78></line><line x1=1 x2=3 y1=12 y2=12></line><line x1=21 x2=23 y1=12 y2=12></line><line x1=4.22 x2=5.64 y1=19.78 y2=18.36></line><line x1=18.36 x2=19.78 y1=5.64 y2=4.22></line></svg></symbol><symbol viewbox="0 0 24 24"id=svg-moon><title>Dark mode</title><svg viewbox="0 0 24 24"class=icon-tabler-moon fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1.5 xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0z"fill=none stroke=none /><path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z"/></svg></symbol><symbol viewbox="0 0 24 24"id=svg-sun-half><title>Auto light/dark mode</title><svg viewbox="0 0 24 24"class=icon-tabler-shadow fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1.5 xmlns=http://www.w3.org/2000/svg><path d="M0 0h24v24H0z"fill=none stroke=none /><circle cx=12 cy=12 r=9 /><path d="M13 12h5"/><path d="M13 15h4"/><path d="M13 18h1"/><path d="M13 9h4"/><path d="M13 6h1"/></svg></symbol></svg><input class=sidebar-toggle id=__navigation name=__navigation type=checkbox><input class=sidebar-toggle id=__toc name=__toc type=checkbox><label class="overlay sidebar-overlay"for=__navigation><div class=visually-hidden>Hide navigation sidebar</div></label><label class="overlay toc-overlay"for=__toc><div class=visually-hidden>Hide table of contents sidebar</div></label><div class=page><header class=mobile-header><div class=header-left><label class=nav-overlay-icon for=__navigation><div class=visually-hidden>Toggle site navigation sidebar</div> <i class=icon><svg><use href=#svg-menu></use></svg></i></label></div><div class=header-center><a href=../../index.html><div class=brand>Nilearn</div></a></div><div class=header-right><div class="theme-toggle-container theme-toggle-header"><button class=theme-toggle><div class=visually-hidden>Toggle Light / Dark / Auto color theme</div> <svg class=theme-icon-when-auto><use href=#svg-sun-half></use></svg> <svg class=theme-icon-when-dark><use href=#svg-moon></use></svg> <svg class=theme-icon-when-light><use href=#svg-sun></use></svg></button></div><label class="toc-overlay-icon toc-header-icon"for=__toc><div class=visually-hidden>Toggle table of contents sidebar</div> <i class=icon><svg><use href=#svg-toc></use></svg></i></label></div></header><aside class=sidebar-drawer><div class=sidebar-container><div class=sidebar-sticky><a class=sidebar-brand href=../../index.html> <div class=sidebar-logo-container><img alt=Logo class=sidebar-logo src=../../_static/nilearn-transparent.png></div> <span class=sidebar-brand-text>Nilearn</span> </a><form action=../../search.html class=sidebar-search-container role=search><input aria-label=Search class=sidebar-search name=q placeholder=Search><input name=check_keywords type=hidden value=yes><input name=area type=hidden value=default></form><div id=searchbox></div><div class=sidebar-scroll><div class=sidebar-tree><ul class=current><li class=toctree-l1><a class="reference internal"href=../../quickstart.html>Quickstart</a></li><li class="toctree-l1 has-children"><a class="reference internal"href=../../auto_examples/index.html>Examples</a><input class=toctree-checkbox id=toctree-checkbox-1 name=toctree-checkbox-1 role=switch type=checkbox><label for=toctree-checkbox-1><div class=visually-hidden>Toggle navigation of Examples</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/00_tutorials/index.html>Basic tutorials</a><input class=toctree-checkbox id=toctree-checkbox-2 name=toctree-checkbox-2 role=switch type=checkbox><label for=toctree-checkbox-2><div class=visually-hidden>Toggle navigation of Basic tutorials</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/00_tutorials/plot_python_101.html>Basic numerics and plotting with Python</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/00_tutorials/plot_3d_and_4d_niimg.html>3D and 4D niimgs: handling and visualizing</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/00_tutorials/plot_nilearn_101.html>Basic nilearn example: manipulating and looking at data</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/00_tutorials/plot_decoding_tutorial.html>A introduction tutorial to fMRI decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/00_tutorials/plot_single_subject_single_run.html>Intro to GLM Analysis: a single-run, single-subject fMRI dataset</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/01_plotting/index.html>Visualization of brain images</a><input class=toctree-checkbox id=toctree-checkbox-3 name=toctree-checkbox-3 role=switch type=checkbox><label for=toctree-checkbox-3><div class=visually-hidden>Toggle navigation of Visualization of brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_demo_glass_brain.html>Glass brain plotting in nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_visualize_megatrawls_netmats.html>Visualizing Megatrawls Network Matrices from Human Connectome Project</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_prob_atlas.html>Visualizing 4D probabilistic atlas maps</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_atlas.html>Basic Atlas plotting</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_overlay.html>Visualizing a probabilistic atlas: the default mode in the MSDL atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_dim_plotting.html>Controlling the contrast of the background when plotting</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_multiscale_parcellations.html>Visualizing multiscale functional brain parcellations</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_colormaps.html>Matplotlib colormaps in Nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_visualization.html>NeuroImaging volumes visualization</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_carpet.html>Visualizing global patterns with a carpet plot</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_haxby_masks.html>Plot Haxby masks</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_surface_projection_strategies.html>Technical point: Illustration of the volume to surface sampling schemes</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_demo_plotting.html>Plotting tools in nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_surf_atlas.html>Loading and plotting of a cortical surface atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_3d_map_to_surface_projection.html>Making a surface plot of a 3D statistical map</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_demo_more_plotting.html>More plotting tools from nilearn</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_demo_glass_brain_extensive.html>Glass brain plotting in nilearn (all options)</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/01_plotting/plot_surf_stat_map.html>Seed-based connectivity on the surface</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/02_decoding/index.html>Decoding and predicting from brain images</a><input class=toctree-checkbox id=toctree-checkbox-4 name=toctree-checkbox-4 role=switch type=checkbox><label for=toctree-checkbox-4><div class=visually-hidden>Toggle navigation of Decoding and predicting from brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_stimuli.html>Show stimuli of Haxby et al. dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_mixed_gambles_frem.html>FREM on Jimura et al “mixed gambles” dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_oasis_vbm_space_net.html>Voxel-Based Morphometry on Oasis dataset with Space-Net prior</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight_surface.html>Cortical surface-based searchlight decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_frem.html>Decoding with FREM: face vs house vs chair object recognition</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_anova_svm.html>Decoding with ANOVA + SVM: face vs house in the Haxby dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_multiclass.html>The haxby dataset: different multi-class strategies</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_searchlight.html>Searchlight analysis of face vs house recognition</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_glm_decoding.html>Decoding of a dataset after GLM fit for signal extraction</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_full_analysis.html>ROI-based decoding analysis in Haxby et al. dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_grid_search.html>Setting a parameter by cross-validation</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_oasis_vbm.html>Voxel-Based Morphometry on Oasis dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_haxby_different_estimators.html>Different classifiers in decoding the Haxby dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_simulated_data.html>Example of pattern recognition on simulated data</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_encoding.html>Encoding models for visual stimuli from Miyawaki et al. 2008</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/02_decoding/plot_miyawaki_reconstruction.html>Reconstruction of visual stimuli from Miyawaki et al. 2008</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/03_connectivity/index.html>Functional connectivity</a><input class=toctree-checkbox id=toctree-checkbox-5 name=toctree-checkbox-5 role=switch type=checkbox><label for=toctree-checkbox-5><div class=visually-hidden>Toggle navigation of Functional connectivity</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_inverse_covariance_connectome.html>Computing a connectome with sparse inverse covariance</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_probabilistic_atlas_extraction.html>Extracting signals of a probabilistic atlas of functional regions</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_simulated_connectome.html>Connectivity structure estimation on simulated data</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_multi_subject_connectome.html>Group Sparse inverse covariance for multi-subject connectome</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_seed_to_voxel_correlation.html>Producing single subject maps of seed-to-voxel correlation</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_compare_decomposition.html>Deriving spatial maps from group fMRI data using ICA and Dictionary Learning</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_extract_regions_dictlearning_maps.html>Regions extraction using dictionary learning and functional connectomes</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_atlas_comparison.html>Comparing connectomes on different reference atlases</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_group_level_connectivity.html>Classification of age groups using functional connectivity</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_signal_extraction.html>Extracting signals from a brain parcellation</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_sphere_based_connectome.html>Extract signals on spheres and plot a connectome</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/03_connectivity/plot_data_driven_parcellations.html>Clustering methods to learn a brain parcellation from fMRI</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/04_glm_first_level/index.html>GLM: First level analysis</a><input class=toctree-checkbox id=toctree-checkbox-6 name=toctree-checkbox-6 role=switch type=checkbox><label for=toctree-checkbox-6><div class=visually-hidden>Toggle navigation of GLM: First level analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_design_matrix.html>Examples of design matrices</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_write_events_file.html>Generate an events.tsv file for the NeuroSpin localizer task</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_adhd_dmn.html>Default Mode Network extraction of ADHD dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_fir_model.html>Analysis of an fMRI dataset with a Finite Impule Response (FIR) model</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_spm_multimodal_faces.html>Single-subject data (two runs) in native space</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_hrf.html>Example of MRI response functions</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_predictions_residuals.html>Predicted time series and residuals</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_bids_features.html>First level analysis of a complete BIDS dataset from openneuro</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html>Example of surface-based first-level analysis</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_two_runs_model.html>Simple example of two-runs fMRI model fitting</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/04_glm_first_level/plot_first_level_details.html>Understanding parameters of the first-level model</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/05_glm_second_level/index.html>GLM: Second level analysis</a><input class=toctree-checkbox id=toctree-checkbox-7 name=toctree-checkbox-7 role=switch type=checkbox><label for=toctree-checkbox-7><div class=visually-hidden>Toggle navigation of GLM: Second level analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_design_matrix.html>Example of second level design matrix</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_proportion_activated_voxels.html>Second-level fMRI model: true positive proportion in clusters</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_thresholding.html>Statistical testing of a second-level analysis</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_oasis.html>Voxel-Based Morphometry on OASIS dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_two_sample_test.html>Second-level fMRI model: two-sample test, unpaired and paired</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_one_sample_test.html>Second-level fMRI model: one sample test</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/05_glm_second_level/plot_second_level_association_test.html>Example of generic design in second-level models</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/06_manipulating_images/index.html>Manipulating brain image volumes</a><input class=toctree-checkbox id=toctree-checkbox-8 name=toctree-checkbox-8 role=switch type=checkbox><label for=toctree-checkbox-8><div class=visually-hidden>Toggle navigation of Manipulating brain image volumes</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_negate_image.html>Negating an image with math_img</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_compare_mean_image.html>Comparing the means of 2 images</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_smooth_mean_image.html>Smoothing an image</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_extract_regions_labels_image.html>Breaking an atlas of labels in separated regions</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_resample_to_template.html>Resample an image to a template</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_extract_rois_smith_atlas.html>Regions Extraction of Default Mode Networks using Smith Atlas</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_nifti_simple.html>Simple example of NiftiMasker use</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_extract_rois_statistical_maps.html>Region Extraction using a t-statistical map (3D)</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_nifti_labels_simple.html>Extracting signals from brain regions using the NiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_mask_computation.html>Understanding NiftiMasker and mask computation</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_affine_transformation.html>Visualization of affine resamplings</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/06_manipulating_images/plot_roi_extraction.html>Computing a Region of Interest (ROI) mask manually</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/07_advanced/index.html>Advanced statistical analysis of brain images</a><input class=toctree-checkbox id=toctree-checkbox-9 name=toctree-checkbox-9 role=switch type=checkbox><label for=toctree-checkbox-9><div class=visually-hidden>Toggle navigation of Advanced statistical analysis of brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_resting_state.html>Multivariate decompositions: Independent component analysis of fMRI</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_simple_analysis.html>Massively univariate analysis of a calculation task from the Localizer dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_bids_analysis.html>BIDS dataset first and second level analysis</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_neurovault_meta_analysis.html>NeuroVault meta-analysis of stop-go paradigm studies</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_age_group_prediction_cross_val.html>Functional connectivity predicts age group</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_surface_bids_analysis.html>Surface-based dataset first and second level analysis of a dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_localizer_mass_univariate_methods.html>Massively univariate analysis of a motor task from the Localizer dataset</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_ica_neurovault.html>NeuroVault cross-study ICA maps</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_haxby_mass_univariate.html>Massively univariate analysis of face vs house recognition</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_advanced_decoding_scikit.html>Advanced decoding using scikit learn</a></li><li class=toctree-l3><a class="reference internal"href=../../auto_examples/07_advanced/plot_beta_series.html>Beta-Series Modeling for Task-Based Functional Connectivity and Decoding</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../auto_examples/08_experimental/index.html>Examples for experimental modules</a><input class=toctree-checkbox id=toctree-checkbox-10 name=toctree-checkbox-10 role=switch type=checkbox><label for=toctree-checkbox-10><div class=visually-hidden>Toggle navigation of Examples for experimental modules</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../auto_examples/08_experimental/plot_surface_image_and_maskers.html>A short demo of the surface images & maskers</a></li></ul></li></ul></li><li class="toctree-l1 has-children"><a class="reference internal"href=../../user_guide.html>User guide</a><input class=toctree-checkbox id=toctree-checkbox-11 name=toctree-checkbox-11 role=switch type=checkbox><label for=toctree-checkbox-11><div class=visually-hidden>Toggle navigation of User guide</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l2><a class="reference internal"href=../../introduction.html>1. Introduction</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#what-is-nilearn>2. What is <code class="docutils literal notranslate"><span class=pre>nilearn</span></code>?</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#using-nilearn-for-the-first-time>3. Using <code class="docutils literal notranslate"><span class=pre>nilearn</span></code> for the first time</a></li><li class=toctree-l2><a class="reference internal"href=../../introduction.html#machine-learning-applications-to-neuroimaging>4. Machine learning applications to Neuroimaging</a></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../decoding/index.html>5. Decoding and MVPA: predicting from brain images</a><input class=toctree-checkbox id=toctree-checkbox-12 name=toctree-checkbox-12 role=switch type=checkbox><label for=toctree-checkbox-12><div class=visually-hidden>Toggle navigation of 5. Decoding and MVPA: predicting from brain images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../decoding/decoding_intro.html>5.1. An introduction to decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/estimator_choice.html>5.2. Choosing the right predictive model for neuroimaging</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/frem.html>5.3. FREM: fast ensembling of regularized models for robust decoding</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/space_net.html>5.4. SpaceNet: decoding with spatial structure for better maps</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/searchlight.html>5.5. Searchlight : finding voxels containing information</a></li><li class=toctree-l3><a class="reference internal"href=../../decoding/going_further.html>5.6. Running scikit-learn functions for more control on the analysis</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../connectivity/index.html>6. Functional connectivity and resting state</a><input class=toctree-checkbox id=toctree-checkbox-13 name=toctree-checkbox-13 role=switch type=checkbox><label for=toctree-checkbox-13><div class=visually-hidden>Toggle navigation of 6. Functional connectivity and resting state</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../connectivity/functional_connectomes.html>6.1. Extracting times series to build a functional connectome</a></li><li class="toctree-l3 has-children"><a class="reference internal"href=../../connectivity/connectome_extraction.html>6.2. Connectome extraction: inverse covariance for direct connections</a><input class=toctree-checkbox id=toctree-checkbox-14 name=toctree-checkbox-14 role=switch type=checkbox><label for=toctree-checkbox-14><div class=visually-hidden>Toggle navigation of 6.2. Connectome extraction: inverse covariance for direct connections</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l4><a class="reference internal"href=../../developers/group_sparse_covariance.html>6.2.3.1. Group-sparse covariance estimation</a></li></ul></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/resting_state_networks.html>6.3. Extracting functional brain networks: ICA and related</a></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/region_extraction.html>6.4. Region Extraction for better brain parcellations</a></li><li class=toctree-l3><a class="reference internal"href=../../connectivity/parcellating.html>6.5. Clustering to parcellate the brain in regions</a></li></ul></li><li class=toctree-l2><a class="reference internal"href=../../plotting/index.html>7. Plotting brain images</a></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../glm/index.html>8. Analyzing fMRI using GLMs</a><input class=toctree-checkbox id=toctree-checkbox-15 name=toctree-checkbox-15 role=switch type=checkbox><label for=toctree-checkbox-15><div class=visually-hidden>Toggle navigation of 8. Analyzing fMRI using GLMs</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../glm/glm_intro.html>8.1. An introduction to GLMs in fMRI statistical analysis</a></li><li class=toctree-l3><a class="reference internal"href=../../glm/first_level_model.html>8.2. First level models</a></li><li class=toctree-l3><a class="reference internal"href=../../glm/second_level_model.html>8.3. Second level models</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../manipulating_images/index.html>9. Manipulation brain volumes with nilearn</a><input class=toctree-checkbox id=toctree-checkbox-16 name=toctree-checkbox-16 role=switch type=checkbox><label for=toctree-checkbox-16><div class=visually-hidden>Toggle navigation of 9. Manipulation brain volumes with nilearn</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/input_output.html>9.1. Input and output: neuroimaging data representation</a></li><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/manipulating_images.html>9.2. Manipulating images: resampling, smoothing, masking, ROIs…</a></li><li class=toctree-l3><a class="reference internal"href=../../manipulating_images/masker_objects.html>9.3. From neuroimaging volumes to data matrices: the masker objects</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../../building_blocks/index.html>10. Advanced usage: manual pipelines and scaling up</a><input class=toctree-checkbox id=toctree-checkbox-17 name=toctree-checkbox-17 role=switch type=checkbox><label for=toctree-checkbox-17><div class=visually-hidden>Toggle navigation of 10. Advanced usage: manual pipelines and scaling up</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=../../building_blocks/manual_pipeline.html>10.1. Building your own neuroimaging machine-learning pipeline</a></li><li class=toctree-l3><a class="reference internal"href=../../building_blocks/neurovault.html>10.2. Downloading statistical maps from the Neurovault repository</a></li></ul></li></ul></li><li class="toctree-l1 current has-children"><a class="reference internal"href=../index.html>API References</a><input checked class=toctree-checkbox id=toctree-checkbox-18 name=toctree-checkbox-18 role=switch type=checkbox><label for=toctree-checkbox-18><div class=visually-hidden>Toggle navigation of API References</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul class=current><li class="toctree-l2 has-children"><a class="reference internal"href=../connectome.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.connectome</span></code>: Functional Connectivity</a><input class=toctree-checkbox id=toctree-checkbox-19 name=toctree-checkbox-19 role=switch type=checkbox><label for=toctree-checkbox-19><div class=visually-hidden>Toggle navigation of nilearn.connectome: Functional Connectivity</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.ConnectivityMeasure.html>nilearn.connectome.ConnectivityMeasure</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.GroupSparseCovariance.html>nilearn.connectome.GroupSparseCovariance</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.GroupSparseCovarianceCV.html>nilearn.connectome.GroupSparseCovarianceCV</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.sym_matrix_to_vec.html>nilearn.connectome.sym_matrix_to_vec</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.vec_to_sym_matrix.html>nilearn.connectome.vec_to_sym_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.group_sparse_covariance.html>nilearn.connectome.group_sparse_covariance</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.cov_to_corr.html>nilearn.connectome.cov_to_corr</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.connectome.prec_to_partial.html>nilearn.connectome.prec_to_partial</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../datasets.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.datasets</span></code>: Automatic Dataset Fetching</a><input class=toctree-checkbox id=toctree-checkbox-20 name=toctree-checkbox-20 role=switch type=checkbox><label for=toctree-checkbox-20><div class=visually-hidden>Toggle navigation of nilearn.datasets: Automatic Dataset Fetching</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_icbm152_2009.html>nilearn.datasets.fetch_icbm152_2009</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_icbm152_brain_gm_mask.html>nilearn.datasets.fetch_icbm152_brain_gm_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_surf_fsaverage.html>nilearn.datasets.fetch_surf_fsaverage</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_brain_mask.html>nilearn.datasets.load_mni152_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_gm_mask.html>nilearn.datasets.load_mni152_gm_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_gm_template.html>nilearn.datasets.load_mni152_gm_template</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_template.html>nilearn.datasets.load_mni152_template</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_wm_mask.html>nilearn.datasets.load_mni152_wm_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_mni152_wm_template.html>nilearn.datasets.load_mni152_wm_template</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_aal.html>nilearn.datasets.fetch_atlas_aal</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_allen_2011.html>nilearn.datasets.fetch_atlas_allen_2011</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_basc_multiscale_2015.html>nilearn.datasets.fetch_atlas_basc_multiscale_2015</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_craddock_2012.html>nilearn.datasets.fetch_atlas_craddock_2012</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_destrieux_2009.html>nilearn.datasets.fetch_atlas_destrieux_2009</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_difumo.html>nilearn.datasets.fetch_atlas_difumo</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_harvard_oxford.html>nilearn.datasets.fetch_atlas_harvard_oxford</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_juelich.html>nilearn.datasets.fetch_atlas_juelich</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_msdl.html>nilearn.datasets.fetch_atlas_msdl</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_pauli_2017.html>nilearn.datasets.fetch_atlas_pauli_2017</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_schaefer_2018.html>nilearn.datasets.fetch_atlas_schaefer_2018</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_smith_2009.html>nilearn.datasets.fetch_atlas_smith_2009</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_surf_destrieux.html>nilearn.datasets.fetch_atlas_surf_destrieux</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_talairach.html>nilearn.datasets.fetch_atlas_talairach</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_atlas_yeo_2011.html>nilearn.datasets.fetch_atlas_yeo_2011</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_coords_dosenbach_2010.html>nilearn.datasets.fetch_coords_dosenbach_2010</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_coords_power_2011.html>nilearn.datasets.fetch_coords_power_2011</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_coords_seitzman_2018.html>nilearn.datasets.fetch_coords_seitzman_2018</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_abide_pcp.html>nilearn.datasets.fetch_abide_pcp</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_adhd.html>nilearn.datasets.fetch_adhd</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_bids_langloc_dataset.html>nilearn.datasets.fetch_bids_langloc_dataset</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_development_fmri.html>nilearn.datasets.fetch_development_fmri</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_ds000030_urls.html>nilearn.datasets.fetch_ds000030_urls</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_fiac_first_level.html>nilearn.datasets.fetch_fiac_first_level</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_haxby.html>nilearn.datasets.fetch_haxby</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_language_localizer_demo_dataset.html>nilearn.datasets.fetch_language_localizer_demo_dataset</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_localizer_first_level.html>nilearn.datasets.fetch_localizer_first_level</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_miyawaki2008.html>nilearn.datasets.fetch_miyawaki2008</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_openneuro_dataset_index.html>nilearn.datasets.fetch_openneuro_dataset_index</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_spm_auditory.html>nilearn.datasets.fetch_spm_auditory</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_spm_multimodal_fmri.html>nilearn.datasets.fetch_spm_multimodal_fmri</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_surf_nki_enhanced.html>nilearn.datasets.fetch_surf_nki_enhanced</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_localizer_button_task.html>nilearn.datasets.fetch_localizer_button_task</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_localizer_calculation_task.html>nilearn.datasets.fetch_localizer_calculation_task</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_localizer_contrasts.html>nilearn.datasets.fetch_localizer_contrasts</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_megatrawls_netmats.html>nilearn.datasets.fetch_megatrawls_netmats</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_mixed_gambles.html>nilearn.datasets.fetch_mixed_gambles</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_oasis_vbm.html>nilearn.datasets.fetch_oasis_vbm</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_neurovault_auditory_computation_task.html>nilearn.datasets.fetch_neurovault_auditory_computation_task</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_neurovault_motor_task.html>nilearn.datasets.fetch_neurovault_motor_task</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_neurovault.html>nilearn.datasets.fetch_neurovault</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_neurovault_ids.html>nilearn.datasets.fetch_neurovault_ids</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.fetch_openneuro_dataset.html>nilearn.datasets.fetch_openneuro_dataset</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.get_data_dirs.html>nilearn.datasets.get_data_dirs</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.patch_openneuro_dataset.html>nilearn.datasets.patch_openneuro_dataset</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.select_from_index.html>nilearn.datasets.select_from_index</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.datasets.load_sample_motor_activation_image.html>nilearn.datasets.load_sample_motor_activation_image</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../decoding.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.decoding</span></code>: Decoding</a><input class=toctree-checkbox id=toctree-checkbox-21 name=toctree-checkbox-21 role=switch type=checkbox><label for=toctree-checkbox-21><div class=visually-hidden>Toggle navigation of nilearn.decoding: Decoding</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.Decoder.html>nilearn.decoding.Decoder</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.DecoderRegressor.html>nilearn.decoding.DecoderRegressor</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.FREMClassifier.html>nilearn.decoding.FREMClassifier</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.FREMRegressor.html>nilearn.decoding.FREMRegressor</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.SpaceNetClassifier.html>nilearn.decoding.SpaceNetClassifier</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.SpaceNetRegressor.html>nilearn.decoding.SpaceNetRegressor</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decoding.SearchLight.html>nilearn.decoding.SearchLight</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../decomposition.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.decomposition</span></code>: Multivariate Decompositions</a><input class=toctree-checkbox id=toctree-checkbox-22 name=toctree-checkbox-22 role=switch type=checkbox><label for=toctree-checkbox-22><div class=visually-hidden>Toggle navigation of nilearn.decomposition: Multivariate Decompositions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.decomposition.CanICA.html>nilearn.decomposition.CanICA</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.decomposition.DictLearning.html>nilearn.decomposition.DictLearning</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../experimental.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.experimental</span></code>: Experimental Modules</a><input class=toctree-checkbox id=toctree-checkbox-23 name=toctree-checkbox-23 role=switch type=checkbox><label for=toctree-checkbox-23><div class=visually-hidden>Toggle navigation of nilearn.experimental: Experimental Modules</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.FileMesh.html>nilearn.experimental.surface.FileMesh</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.InMemoryMesh.html>nilearn.experimental.surface.InMemoryMesh</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.Mesh.html>nilearn.experimental.surface.Mesh</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.PolyMesh.html>nilearn.experimental.surface.PolyMesh</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.SurfaceImage.html>nilearn.experimental.surface.SurfaceImage</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.SurfaceLabelsMasker.html>nilearn.experimental.surface.SurfaceLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.SurfaceMasker.html>nilearn.experimental.surface.SurfaceMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.fetch_destrieux.html>nilearn.experimental.surface.fetch_destrieux</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.fetch_nki.html>nilearn.experimental.surface.fetch_nki</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.experimental.surface.load_fsaverage.html>nilearn.experimental.surface.load_fsaverage</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../glm.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.glm</span></code>: Generalized Linear Models</a><input class=toctree-checkbox id=toctree-checkbox-24 name=toctree-checkbox-24 role=switch type=checkbox><label for=toctree-checkbox-24><div class=visually-hidden>Toggle navigation of nilearn.glm: Generalized Linear Models</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.glm.Contrast.html>nilearn.glm.Contrast</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.FContrastResults.html>nilearn.glm.FContrastResults</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.TContrastResults.html>nilearn.glm.TContrastResults</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.ARModel.html>nilearn.glm.ARModel</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.OLSModel.html>nilearn.glm.OLSModel</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.LikelihoodModelResults.html>nilearn.glm.LikelihoodModelResults</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.RegressionResults.html>nilearn.glm.RegressionResults</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.SimpleRegressionResults.html>nilearn.glm.SimpleRegressionResults</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.compute_contrast.html>nilearn.glm.compute_contrast</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.compute_fixed_effects.html>nilearn.glm.compute_fixed_effects</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.expression_to_contrast_vector.html>nilearn.glm.expression_to_contrast_vector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.fdr_threshold.html>nilearn.glm.fdr_threshold</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.cluster_level_inference.html>nilearn.glm.cluster_level_inference</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.threshold_stats_img.html>nilearn.glm.threshold_stats_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.FirstLevelModel.html>nilearn.glm.first_level.FirstLevelModel</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.check_design_matrix.html>nilearn.glm.first_level.check_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.compute_regressor.html>nilearn.glm.first_level.compute_regressor</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.first_level_from_bids.html>nilearn.glm.first_level.first_level_from_bids</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.glover_dispersion_derivative.html>nilearn.glm.first_level.glover_dispersion_derivative</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.glover_hrf.html>nilearn.glm.first_level.glover_hrf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.glover_time_derivative.html>nilearn.glm.first_level.glover_time_derivative</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.make_first_level_design_matrix.html>nilearn.glm.first_level.make_first_level_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.mean_scaling.html>nilearn.glm.first_level.mean_scaling</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.run_glm.html>nilearn.glm.first_level.run_glm</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.spm_dispersion_derivative.html>nilearn.glm.first_level.spm_dispersion_derivative</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.spm_hrf.html>nilearn.glm.first_level.spm_hrf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.first_level.spm_time_derivative.html>nilearn.glm.first_level.spm_time_derivative</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.second_level.SecondLevelModel.html>nilearn.glm.second_level.SecondLevelModel</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.second_level.make_second_level_design_matrix.html>nilearn.glm.second_level.make_second_level_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.glm.second_level.non_parametric_inference.html>nilearn.glm.second_level.non_parametric_inference</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../image.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.image</span></code>: Image Processing and Resampling Utilities</a><input class=toctree-checkbox id=toctree-checkbox-25 name=toctree-checkbox-25 role=switch type=checkbox><label for=toctree-checkbox-25><div class=visually-hidden>Toggle navigation of nilearn.image: Image Processing and Resampling Utilities</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.image.binarize_img.html>nilearn.image.binarize_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.clean_img.html>nilearn.image.clean_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.concat_imgs.html>nilearn.image.concat_imgs</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.coord_transform.html>nilearn.image.coord_transform</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.copy_img.html>nilearn.image.copy_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.crop_img.html>nilearn.image.crop_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.get_data.html>nilearn.image.get_data</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.high_variance_confounds.html>nilearn.image.high_variance_confounds</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.index_img.html>nilearn.image.index_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.iter_img.html>nilearn.image.iter_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.largest_connected_component_img.html>nilearn.image.largest_connected_component_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.load_img.html>nilearn.image.load_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.math_img.html>nilearn.image.math_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.mean_img.html>nilearn.image.mean_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.new_img_like.html>nilearn.image.new_img_like</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.resample_img.html>nilearn.image.resample_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.resample_to_img.html>nilearn.image.resample_to_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.reorder_img.html>nilearn.image.reorder_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.smooth_img.html>nilearn.image.smooth_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.swap_img_hemispheres.html>nilearn.image.swap_img_hemispheres</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.image.threshold_img.html>nilearn.image.threshold_img</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../interfaces.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.interfaces</span></code>: Loading components from interfaces</a><input class=toctree-checkbox id=toctree-checkbox-26 name=toctree-checkbox-26 role=switch type=checkbox><label for=toctree-checkbox-26><div class=visually-hidden>Toggle navigation of nilearn.interfaces: Loading components from interfaces</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.bids.get_bids_files.html>nilearn.interfaces.bids.get_bids_files</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.bids.parse_bids_filename.html>nilearn.interfaces.bids.parse_bids_filename</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.bids.save_glm_to_bids.html>nilearn.interfaces.bids.save_glm_to_bids</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.fmriprep.load_confounds.html>nilearn.interfaces.fmriprep.load_confounds</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.fmriprep.load_confounds_strategy.html>nilearn.interfaces.fmriprep.load_confounds_strategy</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.interfaces.fsl.get_design_from_fslmat.html>nilearn.interfaces.fsl.get_design_from_fslmat</a></li></ul></li><li class="toctree-l2 current has-children"><a class="reference internal"href=../maskers.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.maskers</span></code>: Extracting Signals from Brain Images</a><input checked class=toctree-checkbox id=toctree-checkbox-27 name=toctree-checkbox-27 role=switch type=checkbox><label for=toctree-checkbox-27><div class=visually-hidden>Toggle navigation of nilearn.maskers: Extracting Signals from Brain Images</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul class=current><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.BaseMasker.html>nilearn.maskers.BaseMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.NiftiMasker.html>nilearn.maskers.NiftiMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.MultiNiftiMasker.html>nilearn.maskers.MultiNiftiMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.NiftiLabelsMasker.html>nilearn.maskers.NiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.MultiNiftiLabelsMasker.html>nilearn.maskers.MultiNiftiLabelsMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.NiftiMapsMasker.html>nilearn.maskers.NiftiMapsMasker</a></li><li class="toctree-l3 current current-page"><a class="current reference internal"href=#>nilearn.maskers.MultiNiftiMapsMasker</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.maskers.NiftiSpheresMasker.html>nilearn.maskers.NiftiSpheresMasker</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../masking.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.masking</span></code>: Data Masking Utilities</a><input class=toctree-checkbox id=toctree-checkbox-28 name=toctree-checkbox-28 role=switch type=checkbox><label for=toctree-checkbox-28><div class=visually-hidden>Toggle navigation of nilearn.masking: Data Masking Utilities</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_epi_mask.html>nilearn.masking.compute_epi_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_multi_epi_mask.html>nilearn.masking.compute_multi_epi_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_brain_mask.html>nilearn.masking.compute_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_multi_brain_mask.html>nilearn.masking.compute_multi_brain_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_background_mask.html>nilearn.masking.compute_background_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.compute_multi_background_mask.html>nilearn.masking.compute_multi_background_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.intersect_masks.html>nilearn.masking.intersect_masks</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.apply_mask.html>nilearn.masking.apply_mask</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.masking.unmask.html>nilearn.masking.unmask</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../mass_univariate.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.mass_univariate</span></code>: Mass-Univariate Analysis</a><input class=toctree-checkbox id=toctree-checkbox-29 name=toctree-checkbox-29 role=switch type=checkbox><label for=toctree-checkbox-29><div class=visually-hidden>Toggle navigation of nilearn.mass_univariate: Mass-Univariate Analysis</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.mass_univariate.permuted_ols.html>nilearn.mass_univariate.permuted_ols</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../plotting.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.plotting</span></code>: Plotting Brain Data</a><input class=toctree-checkbox id=toctree-checkbox-30 name=toctree-checkbox-30 role=switch type=checkbox><label for=toctree-checkbox-30><div class=visually-hidden>Toggle navigation of nilearn.plotting: Plotting Brain Data</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.find_cut_slices.html>nilearn.plotting.find_cut_slices</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.find_xyz_cut_coords.html>nilearn.plotting.find_xyz_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.find_parcellation_cut_coords.html>nilearn.plotting.find_parcellation_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.find_probabilistic_atlas_cut_coords.html>nilearn.plotting.find_probabilistic_atlas_cut_coords</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_anat.html>nilearn.plotting.plot_anat</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_img.html>nilearn.plotting.plot_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_epi.html>nilearn.plotting.plot_epi</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_matrix.html>nilearn.plotting.plot_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_roi.html>nilearn.plotting.plot_roi</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_stat_map.html>nilearn.plotting.plot_stat_map</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_glass_brain.html>nilearn.plotting.plot_glass_brain</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_connectome.html>nilearn.plotting.plot_connectome</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_markers.html>nilearn.plotting.plot_markers</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_prob_atlas.html>nilearn.plotting.plot_prob_atlas</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_carpet.html>nilearn.plotting.plot_carpet</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_surf.html>nilearn.plotting.plot_surf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_surf_roi.html>nilearn.plotting.plot_surf_roi</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_surf_contours.html>nilearn.plotting.plot_surf_contours</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_surf_stat_map.html>nilearn.plotting.plot_surf_stat_map</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_img_on_surf.html>nilearn.plotting.plot_img_on_surf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_img_comparison.html>nilearn.plotting.plot_img_comparison</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_design_matrix.html>nilearn.plotting.plot_design_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_event.html>nilearn.plotting.plot_event</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.plot_contrast_matrix.html>nilearn.plotting.plot_contrast_matrix</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.view_surf.html>nilearn.plotting.view_surf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.view_img_on_surf.html>nilearn.plotting.view_img_on_surf</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.view_connectome.html>nilearn.plotting.view_connectome</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.view_markers.html>nilearn.plotting.view_markers</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.view_img.html>nilearn.plotting.view_img</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.show.html>nilearn.plotting.show</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.get_projector.html>nilearn.plotting.displays.get_projector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.get_slicer.html>nilearn.plotting.displays.get_slicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.OrthoProjector.html>nilearn.plotting.displays.OrthoProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.XZProjector.html>nilearn.plotting.displays.XZProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YZProjector.html>nilearn.plotting.displays.YZProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YXProjector.html>nilearn.plotting.displays.YXProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.XProjector.html>nilearn.plotting.displays.XProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YProjector.html>nilearn.plotting.displays.YProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.ZProjector.html>nilearn.plotting.displays.ZProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LZRYProjector.html>nilearn.plotting.displays.LZRYProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LYRZProjector.html>nilearn.plotting.displays.LYRZProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LYRProjector.html>nilearn.plotting.displays.LYRProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LZRProjector.html>nilearn.plotting.displays.LZRProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LRProjector.html>nilearn.plotting.displays.LRProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.LProjector.html>nilearn.plotting.displays.LProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.RProjector.html>nilearn.plotting.displays.RProjector</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.BaseAxes.html>nilearn.plotting.displays.BaseAxes</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.CutAxes.html>nilearn.plotting.displays.CutAxes</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.GlassBrainAxes.html>nilearn.plotting.displays.GlassBrainAxes</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.BaseSlicer.html>nilearn.plotting.displays.BaseSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.OrthoSlicer.html>nilearn.plotting.displays.OrthoSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.PlotlySurfaceFigure.html>nilearn.plotting.displays.PlotlySurfaceFigure</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.TiledSlicer.html>nilearn.plotting.displays.TiledSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.MosaicSlicer.html>nilearn.plotting.displays.MosaicSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.XZSlicer.html>nilearn.plotting.displays.XZSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YZSlicer.html>nilearn.plotting.displays.YZSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YXSlicer.html>nilearn.plotting.displays.YXSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.XSlicer.html>nilearn.plotting.displays.XSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.YSlicer.html>nilearn.plotting.displays.YSlicer</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.plotting.displays.ZSlicer.html>nilearn.plotting.displays.ZSlicer</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../regions.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.regions</span></code>: Operating on Regions</a><input class=toctree-checkbox id=toctree-checkbox-31 name=toctree-checkbox-31 role=switch type=checkbox><label for=toctree-checkbox-31><div class=visually-hidden>Toggle navigation of nilearn.regions: Operating on Regions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.regions.connected_regions.html>nilearn.regions.connected_regions</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.connected_label_regions.html>nilearn.regions.connected_label_regions</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.img_to_signals_labels.html>nilearn.regions.img_to_signals_labels</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.signals_to_img_labels.html>nilearn.regions.signals_to_img_labels</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.img_to_signals_maps.html>nilearn.regions.img_to_signals_maps</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.signals_to_img_maps.html>nilearn.regions.signals_to_img_maps</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.recursive_neighbor_agglomeration.html>nilearn.regions.recursive_neighbor_agglomeration</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.RegionExtractor.html>nilearn.regions.RegionExtractor</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.Parcellations.html>nilearn.regions.Parcellations</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.ReNA.html>nilearn.regions.ReNA</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.regions.HierarchicalKMeans.html>nilearn.regions.HierarchicalKMeans</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../reporting.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.reporting</span></code>: Reporting Functions</a><input class=toctree-checkbox id=toctree-checkbox-32 name=toctree-checkbox-32 role=switch type=checkbox><label for=toctree-checkbox-32><div class=visually-hidden>Toggle navigation of nilearn.reporting: Reporting Functions</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.reporting.HTMLReport.html>nilearn.reporting.HTMLReport</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.reporting.get_clusters_table.html>nilearn.reporting.get_clusters_table</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.reporting.make_glm_report.html>nilearn.reporting.make_glm_report</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../signal.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.signal</span></code>: Preprocessing Time Series</a><input class=toctree-checkbox id=toctree-checkbox-33 name=toctree-checkbox-33 role=switch type=checkbox><label for=toctree-checkbox-33><div class=visually-hidden>Toggle navigation of nilearn.signal: Preprocessing Time Series</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.signal.butterworth.html>nilearn.signal.butterworth</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.signal.clean.html>nilearn.signal.clean</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.signal.high_variance_confounds.html>nilearn.signal.high_variance_confounds</a></li></ul></li><li class="toctree-l2 has-children"><a class="reference internal"href=../surface.html><code class="xref py py-mod docutils literal notranslate"><span class=pre>nilearn.surface</span></code>: Manipulating Surface Data</a><input class=toctree-checkbox id=toctree-checkbox-34 name=toctree-checkbox-34 role=switch type=checkbox><label for=toctree-checkbox-34><div class=visually-hidden>Toggle navigation of nilearn.surface: Manipulating Surface Data</div><i class=icon><svg><use href=#svg-arrow-right></use></svg></i></label><ul><li class=toctree-l3><a class="reference internal"href=nilearn.surface.load_surf_data.html>nilearn.surface.load_surf_data</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.surface.load_surf_mesh.html>nilearn.surface.load_surf_mesh</a></li><li class=toctree-l3><a class="reference internal"href=nilearn.surface.vol_to_surf.html>nilearn.surface.vol_to_surf</a></li></ul></li></ul></li><li class=toctree-l1><a class="reference internal"href=../../glossary.html>Glossary</a></li></ul><p class=caption role=heading><span class=caption-text>Development</span></p><ul><li class=toctree-l1><a class="reference internal"href=../../development.html>Contributing</a></li><li class=toctree-l1><a class="reference internal"href=../../maintenance.html>Maintenance</a></li><li class=toctree-l1><a class="reference internal"href=../../changes/whats_new.html>What’s new</a></li><li class=toctree-l1><a class="reference internal"href=../../authors.html>Team</a></li><li class=toctree-l1><a class="reference external"href=https://github.com/nilearn/nilearn>GitHub Repository</a></li></ul></div></div></div></div></aside><div class=main><div class=content><div class=article-container><a class="back-to-top muted-link"href=#> <svg viewbox="0 0 24 24"xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path></svg> <span>Back to top</span> </a><div class=content-icon-container><div class=edit-this-page><a title="Edit this page"class=muted-link href=https://github.com/nilearn/nilearn/edit/main/doc/modules/generated/nilearn.maskers.MultiNiftiMapsMasker.rst target=_blank> <svg viewbox="0 0 24 24"aria-hidden=true fill=none stroke=currentColor stroke-linecap=round stroke-linejoin=round stroke-width=1.5><path d="M0 0h24v24H0z"fill=none stroke=none /><path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4"/><line x1=13.5 x2=17.5 y1=6.5 y2=10.5 /></svg> <span class=visually-hidden>Edit this page</span> </a></div><div class="theme-toggle-container theme-toggle-content"><button class=theme-toggle><div class=visually-hidden>Toggle Light / Dark / Auto color theme</div> <svg class=theme-icon-when-auto><use href=#svg-sun-half></use></svg> <svg class=theme-icon-when-dark><use href=#svg-moon></use></svg> <svg class=theme-icon-when-light><use href=#svg-sun></use></svg></button></div><label class="toc-overlay-icon toc-content-icon"for=__toc><div class=visually-hidden>Toggle table of contents sidebar</div> <i class=icon><svg><use href=#svg-toc></use></svg></i></label></div><article role=main><div class="admonition note"><p class=admonition-title>Note</p><p>This page is a reference documentation. It only explains the class signature, and not how to use it. Please refer to the <a class="reference internal"href=../../user_guide.html#user-guide><span class="std std-ref">user guide</span></a> for the big picture.</p></div><section id=nilearn-maskers-multiniftimapsmasker><h1>nilearn.maskers.MultiNiftiMapsMasker<a title="Link to this heading"class=headerlink href=#nilearn-maskers-multiniftimapsmasker>#</a></h1><dl class="py class"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker><em class=property><span class=pre>class</span><span class=w> </span></em><span class="sig-prename descclassname"><span class=pre>nilearn.maskers.</span></span><span class="sig-name descname"><span class=pre>MultiNiftiMapsMasker</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>maps_img</span></span></em>, <em class=sig-param><span class=n><span class=pre>mask_img</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>allow_overlap</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>smoothing_fwhm</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>standardize</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>standardize_confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>high_variance_confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>detrend</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>low_pass</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>high_pass</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>t_r</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>dtype</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>resampling_target</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'data'</span></span></em>, <em class=sig-param><span class=n><span class=pre>memory</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>Memory(location=None)</span></span></em>, <em class=sig-param><span class=n><span class=pre>memory_level</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>0</span></span></em>, <em class=sig-param><span class=n><span class=pre>verbose</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>0</span></span></em>, <em class=sig-param><span class=n><span class=pre>reports</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>n_jobs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>1</span></span></em>, <em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>kwargs</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/multi_nifti_maps_masker.py#L12><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker>#</a></dt><dd><p>Class for masking of Niimg-like objects.</p> <p>MultiNiftiMapsMasker is useful when data from overlapping volumes and from different subjects should be extracted (contrary to <a class="reference internal"href=nilearn.maskers.NiftiMapsMasker.html#nilearn.maskers.NiftiMapsMasker title=nilearn.maskers.NiftiMapsMasker><code class="xref py py-class docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiMapsMasker</span></code></a>).</p> <div class="admonition note"><p class=admonition-title>Note</p><p>Inf or NaN present in the given input images are automatically put to zero rather than considered as missing data.</p></div> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>maps_img</strong><span class=classifier>4D niimg-like object</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Set of continuous maps. One representative time course per map is extracted using least square regression.</p></dd><dt><strong>mask_img</strong><span class=classifier>3D niimg-like object, optional</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Mask to apply to regions before extracting signals.</p></dd><dt><strong>allow_overlap</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, default=True</span></dt><dd><p>If False, an error is raised if the maps overlaps (ie at least two maps have a non-zero value for the same voxel).</p></dd><dt><strong>smoothing_fwhm</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#float><code class="xref py py-obj docutils literal notranslate"><span class=pre>float</span></code></a>, optional.</span></dt><dd><p>If <cite>smoothing_fwhm</cite> is not <cite>None</cite>, it gives the <a class="reference internal"href=../../glossary.html#term-FWHM><span class="xref std std-term">full-width at half maximum</span></a> in millimeters of the spatial smoothing to apply to the signal.</p></dd><dt><strong>standardize</strong><span class=classifier>{‘zscore_sample’, ‘zscore’, ‘psc’, True, False}, default=False</span></dt><dd><p>Strategy to standardize the signal:</p> <blockquote><div><ul class=simple><li><p>‘zscore_sample’: The signal is z-scored. Timeseries are shifted to zero mean and scaled to unit variance. Uses sample std.</p></li><li><p>‘zscore’: The signal is z-scored. Timeseries are shifted to zero mean and scaled to unit variance. Uses population std by calling default <a class="reference external"title="(in NumPy v1.26)"href=https://numpy.org/doc/stable/reference/generated/numpy.std.html#numpy.std><code class="xref py py-obj docutils literal notranslate"><span class=pre>numpy.std</span></code></a> with N - <code class="docutils literal notranslate"><span class=pre>ddof=0</span></code>.</p></li><li><p>‘psc’: Timeseries are shifted to zero mean value and scaled to percent signal change (as compared to original mean signal).</p></li><li><p>True: The signal is z-scored (same as option <cite>zscore</cite>). Timeseries are shifted to zero mean and scaled to unit variance.</p></li><li><p>False: Do not standardize the data.</p></li></ul></div></blockquote></dd><dt><strong>standardize_confounds</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, default=True</span></dt><dd><p>If set to <cite>True</cite>, the confounds are z-scored: their mean is put to 0 and their variance to 1 in the time dimension.</p></dd><dt><strong>high_variance_confounds</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, default=False</span></dt><dd><p>If True, high variance confounds are computed on provided image with <a class="reference internal"href=nilearn.image.high_variance_confounds.html#nilearn.image.high_variance_confounds title=nilearn.image.high_variance_confounds><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.image.high_variance_confounds</span></code></a> and default parameters and regressed out.</p></dd><dt><strong>detrend</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, optional</span></dt><dd><p>Whether to detrend signals or not.</p></dd><dt><strong>low_pass</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#float><code class="xref py py-obj docutils literal notranslate"><span class=pre>float</span></code></a> or None, default=None</span></dt><dd><p>Low cutoff frequency in Hertz. If specified, signals above this frequency will be filtered out. If <cite>None</cite>, no low-pass filtering will be performed.</p></dd><dt><strong>high_pass</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#float><code class="xref py py-obj docutils literal notranslate"><span class=pre>float</span></code></a>, default=None</span></dt><dd><p>High cutoff frequency in Hertz. If specified, signals below this frequency will be filtered out.</p></dd><dt><strong>t_r</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#float><code class="xref py py-obj docutils literal notranslate"><span class=pre>float</span></code></a> or None, default=None</span></dt><dd><p><a class="reference internal"href=../../glossary.html#term-TR><span class="xref std std-term">Repetition time</span></a>, in seconds (sampling period). Set to <cite>None</cite> if not provided.</p></dd><dt><strong>dtype</strong><span class=classifier>{dtype, “auto”}, optional</span></dt><dd><p>Data type toward which the data should be converted. If “auto”, the data will be converted to int32 if dtype is discrete and float32 if it is continuous.</p></dd><dt><strong>resampling_target</strong><span class=classifier>{“data”, “mask”, “maps”, None}, default=”data”</span></dt><dd><p>Gives which image gives the final shape/size:</p> <blockquote><div><ul class=simple><li><p>“data” means the atlas is resampled to the shape of the data if needed</p></li><li><p>“mask” means the maps_img and images provided to fit() are resampled to the shape and affine of mask_img</p></li><li><p>“maps” means the mask_img and images provided to fit() are resampled to the shape and affine of maps_img</p></li><li><p>None means no resampling: if shapes and affines do not match, a ValueError is raised.</p></li></ul></div></blockquote></dd><dt><strong>memory</strong><span class=classifier>instance of <a class="reference external"title="(in joblib v1.4.dev0)"href=https://joblib.readthedocs.io/en/latest/generated/joblib.Memory.html#joblib.Memory><code class="xref py py-class docutils literal notranslate"><span class=pre>joblib.Memory</span></code></a>, <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#str><code class="xref py py-obj docutils literal notranslate"><span class=pre>str</span></code></a>, or <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/pathlib.html#pathlib.Path><code class="xref py py-class docutils literal notranslate"><span class=pre>pathlib.Path</span></code></a></span></dt><dd><p>Used to cache the masking process. By default, no caching is done. If a <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#str><code class="xref py py-obj docutils literal notranslate"><span class=pre>str</span></code></a> is given, it is the path to the caching directory.</p></dd><dt><strong>memory_level</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>, default=0</span></dt><dd><p>Rough estimator of the amount of memory used by caching. Higher value means more memory for caching. Zero means no caching.</p></dd><dt><strong>n_jobs</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>, default=1</span></dt><dd><p>The number of CPUs to use to do the computation. <cite>-1</cite> means ‘all CPUs’.</p></dd><dt><strong>verbose</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>, default=0</span></dt><dd><p>Verbosity level (<cite>0</cite> means no message).</p></dd><dt><strong>reports</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#bool><code class="xref py py-obj docutils literal notranslate"><span class=pre>bool</span></code></a>, default=True</span></dt><dd><p>If set to True, data is saved in order to produce a report.</p></dd><dt><strong>kwargs</strong><span class=classifier>dict</span></dt><dd><p>Keyword arguments to be passed to functions called within the masker. Kwargs prefixed with <cite>‘clean__’</cite> will be passed to <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>clean</span></code></a>. Within <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>clean</span></code></a>, kwargs prefixed with <cite>‘butterworth__’</cite> will be passed to the Butterworth filter (i.e., <cite>clean__butterworth__</cite>).</p></dd></dl></dd></dl> <div class="admonition seealso"><p class=admonition-title>See also</p><dl class=simple><dt><a class="reference internal"href=nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker title=nilearn.maskers.NiftiMasker><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiMasker</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.maskers.NiftiLabelsMasker.html#nilearn.maskers.NiftiLabelsMasker title=nilearn.maskers.NiftiLabelsMasker><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiLabelsMasker</span></code></a></dt><dd></dd><dt><a class="reference internal"href=nilearn.maskers.NiftiMapsMasker.html#nilearn.maskers.NiftiMapsMasker title=nilearn.maskers.NiftiMapsMasker><code class="xref py py-obj docutils literal notranslate"><span class=pre>nilearn.maskers.NiftiMapsMasker</span></code></a></dt><dd></dd></dl></div> <p class=rubric>Notes</p> <p>If resampling_target is set to “maps”, every 3D image processed by transform() will be resampled to the shape of maps_img. It may lead to a very large memory consumption if the voxel number in maps_img is large.</p> <dl class=field-list><dt class=field-odd>Attributes<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>maps_img_</strong><span class=classifier><a class="reference external"title="(in NiBabel v5.3.0.dev5+g773e3c40)"href=https://nipy.org/nibabel/reference/nibabel.nifti1.html#nibabel.nifti1.Nifti1Image><code class="xref py py-obj docutils literal notranslate"><span class=pre>nibabel.nifti1.Nifti1Image</span></code></a></span></dt><dd><p>The maps mask of the data.</p></dd><dt><strong>n_elements_</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a></span></dt><dd><p>The number of overlapping maps in the mask. This is equivalent to the number of volumes in the mask image.</p> <div class=versionadded><p><span class="versionmodified added">New in version 0.9.2.</span></p></div></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.__init__><span class="sig-name descname"><span class=pre>__init__</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>maps_img</span></span></em>, <em class=sig-param><span class=n><span class=pre>mask_img</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>allow_overlap</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>smoothing_fwhm</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>standardize</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>standardize_confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>high_variance_confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>detrend</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>False</span></span></em>, <em class=sig-param><span class=n><span class=pre>low_pass</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>high_pass</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>t_r</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>dtype</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>resampling_target</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'data'</span></span></em>, <em class=sig-param><span class=n><span class=pre>memory</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>Memory(location=None)</span></span></em>, <em class=sig-param><span class=n><span class=pre>memory_level</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>0</span></span></em>, <em class=sig-param><span class=n><span class=pre>verbose</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>0</span></span></em>, <em class=sig-param><span class=n><span class=pre>reports</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em>, <em class=sig-param><span class=n><span class=pre>n_jobs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>1</span></span></em>, <em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>kwargs</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/multi_nifti_maps_masker.py#L102><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.__init__>#</a></dt><dd></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.transform_imgs><span class="sig-name descname"><span class=pre>transform_imgs</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs_list</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>n_jobs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>1</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/multi_nifti_maps_masker.py#L146><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.transform_imgs>#</a></dt><dd><p>Extract signals from a list of 4D niimgs.</p> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#list><code class="xref py py-obj docutils literal notranslate"><span class=pre>list</span></code></a> of Niimg-like objects</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Images to process. Each element of the list is a 4D image.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>. Please see the related documentation for details. shape: list of (number of scans, number of confounds)</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>shape: (number of scans - number of volumes removed, ) Masks the niimgs along time/fourth dimension to perform scrubbing (remove volumes with high motion) and/or non-steady-state volumes. This parameter is passed to <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl><dt><strong>region_signals</strong><span class=classifier>list of 2D <a class="reference external"title="(in NumPy v1.26)"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray><code class="xref py py-obj docutils literal notranslate"><span class=pre>numpy.ndarray</span></code></a></span></dt><dd><p>List of signals for each map per subject. shape: list of (number of scans, number of maps)</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.transform><span class="sig-name descname"><span class=pre>transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/multi_nifti_maps_masker.py#L195><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.transform>#</a></dt><dd><p>Apply mask, spatial and temporal preprocessing.</p> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#list><code class="xref py py-obj docutils literal notranslate"><span class=pre>list</span></code></a> of Niimg-like objects</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Images to process. Each element of the list is a 4D image.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>. Please see the related documentation for details. shape: list of (number of scans, number of confounds)</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>shape: (number of scans - number of volumes removed, ) Masks the niimgs along time/fourth dimension to perform scrubbing (remove volumes with high motion) and/or non-steady-state volumes. This parameter is passed to <a class="reference internal"href=nilearn.signal.clean.html#nilearn.signal.clean title=nilearn.signal.clean><code class="xref py py-func docutils literal notranslate"><span class=pre>nilearn.signal.clean</span></code></a>.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl><dt><strong>region_signals</strong><span class=classifier>list of 2D <a class="reference external"title="(in NumPy v1.26)"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray><code class="xref py py-obj docutils literal notranslate"><span class=pre>numpy.ndarray</span></code></a></span></dt><dd><p>List of signals for each map per subject. shape: list of (number of scans, number of maps)</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.fit><span class="sig-name descname"><span class=pre>fit</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>y</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/nifti_maps_masker.py#L359><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.fit>#</a></dt><dd><p>Prepare signal extraction from regions.</p> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#list><code class="xref py py-obj docutils literal notranslate"><span class=pre>list</span></code></a> of Niimg-like objects</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Image data passed to the reporter.</p></dd><dt><strong>y</strong><span class=classifier>None</span></dt><dd><p>This parameter is unused. It is solely included for scikit-learn compatibility.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.fit_transform><span class="sig-name descname"><span class=pre>fit_transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/nifti_maps_masker.py#L455><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.fit_transform>#</a></dt><dd><p>Prepare and perform signal extraction.</p></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.generate_report><span class="sig-name descname"><span class=pre>generate_report</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>displayed_maps</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>10</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/nifti_maps_masker.py#L196><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.generate_report>#</a></dt><dd><p>Generate an HTML report for the current <code class="docutils literal notranslate"><span class=pre>NiftiMapsMasker</span></code> object.</p> <div class="admonition note"><p class=admonition-title>Note</p><p>This functionality requires to have <code class="docutils literal notranslate"><span class=pre>Matplotlib</span></code> installed.</p></div> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>displayed_maps</strong><span class=classifier><a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>, or <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#list><code class="xref py py-obj docutils literal notranslate"><span class=pre>list</span></code></a>, or <a class="reference external"title="(in NumPy v1.26)"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray><code class="xref py py-class docutils literal notranslate"><span class=pre>ndarray</span></code></a>, or “all”, default=10</span></dt><dd><p>Indicates which maps will be displayed in the HTML report.</p> <blockquote><div><ul class=simple><li><p>If “all”: All maps will be displayed in the report.</p></li></ul><div class="highlight-python notranslate"><div class=highlight><pre><span></span><span class=n>masker</span><span class=o>.</span><span class=n>generate_report</span><span class=p>(</span><span class=s2>"all"</span><span class=p>)</span>
</pre></div></div><ul class=simple><li><p>If a <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/stdtypes.html#list><code class="xref py py-obj docutils literal notranslate"><span class=pre>list</span></code></a> or <a class="reference external"title="(in NumPy v1.26)"href=https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray><code class="xref py py-class docutils literal notranslate"><span class=pre>ndarray</span></code></a>: This indicates the indices of the maps to be displayed in the report. For example, the following code will generate a report with maps 6, 3, and 12, displayed in this specific order:</p></li></ul><div class="highlight-python notranslate"><div class=highlight><pre><span></span><span class=n>masker</span><span class=o>.</span><span class=n>generate_report</span><span class=p>([</span><span class=mi>6</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>12</span><span class=p>])</span>
</pre></div></div><ul class=simple><li><p>If an <a class="reference external"title="(in Python v3.9)"href=https://docs.python.org/3.9/library/functions.html#int><code class="xref py py-obj docutils literal notranslate"><span class=pre>int</span></code></a>: This will only display the first n maps, n being the value of the parameter. By default, the report will only contain the first 10 maps. Example to display the first 16 maps:</p></li></ul><div class="highlight-python notranslate"><div class=highlight><pre><span></span><span class=n>masker</span><span class=o>.</span><span class=n>generate_report</span><span class=p>(</span><span class=mi>16</span><span class=p>)</span>
</pre></div></div></div></blockquote></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl><dt><strong>report</strong><span class=classifier><cite>nilearn.reporting.html_report.HTMLReport</cite></span></dt><dd><p>HTML report for the masker.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.get_metadata_routing><span class="sig-name descname"><span class=pre>get_metadata_routing</span></span><span class=sig-paren>(</span><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.get_metadata_routing>#</a></dt><dd><p>Get metadata routing of this object.</p> <p>Please check <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/metadata_routing.html#metadata-routing><span class="xref std std-ref">User Guide</span></a> on how the routing mechanism works.</p> <dl class="field-list simple"><dt class=field-odd>Returns<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>routing</strong><span class=classifier>MetadataRequest</span></dt><dd><p>A <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.utils.metadata_routing.MetadataRequest.html#sklearn.utils.metadata_routing.MetadataRequest><code class="xref py py-class docutils literal notranslate"><span class=pre>MetadataRequest</span></code></a> encapsulating routing information.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.get_params><span class="sig-name descname"><span class=pre>get_params</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>deep</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>True</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.get_params>#</a></dt><dd><p>Get parameters for this estimator.</p> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>deep</strong><span class=classifier>bool, default=True</span></dt><dd><p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>params</strong><span class=classifier>dict</span></dt><dd><p>Parameter names mapped to their values.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.inverse_transform><span class="sig-name descname"><span class=pre>inverse_transform</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>region_signals</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/nifti_maps_masker.py#L615><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.inverse_transform>#</a></dt><dd><p>Compute <a class="reference internal"href=../../glossary.html#term-voxel><span class="xref std std-term">voxel</span></a> signals from region signals.</p> <p>Any mask given at initialization is taken into account.</p> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>region_signals</strong><span class=classifier>1D/2D numpy.ndarray</span></dt><dd><p>Signal for each region. If a 1D array is provided, then the shape should be (number of elements,), and a 3D img will be returned. If a 2D array is provided, then the shape should be (number of scans, number of elements), and a 4D img will be returned.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>voxel_signals</strong><span class=classifier>nibabel.Nifti1Image</span></dt><dd><p>Signal for each voxel. shape: that of maps.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.set_fit_request><span class="sig-name descname"><span class=pre>set_fit_request</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>*</span></span></em>, <em class=sig-param><span class=n><span class=pre>imgs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'$UNCHANGED$'</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.set_fit_request>#</a></dt><dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class=pre>fit</span></code> method.</p> <p>Note that this method is only relevant if <code class="docutils literal notranslate"><span class=pre>enable_metadata_routing=True</span></code> (see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config><code class="xref py py-func docutils literal notranslate"><span class=pre>sklearn.set_config</span></code></a>). Please see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/metadata_routing.html#metadata-routing><span class="xref std std-ref">User Guide</span></a> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul class=simple><li><p><code class="docutils literal notranslate"><span class=pre>True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class=pre>fit</span></code> if provided. The request is ignored if metadata is not provided.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class=pre>fit</span></code>.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li></ul> <p>The default (<code class="docutils literal notranslate"><span class=pre>sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <div class=versionadded><p><span class="versionmodified added">New in version 1.3.</span></p></div> <div class="admonition note"><p class=admonition-title>Note</p><p>This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>. Otherwise it has no effect.</p></div> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>imgs</strong><span class=classifier>str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class=pre>imgs</span></code> parameter in <code class="docutils literal notranslate"><span class=pre>fit</span></code>.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>object</span></dt><dd><p>The updated object.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.set_inverse_transform_request><span class="sig-name descname"><span class=pre>set_inverse_transform_request</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>*</span></span></em>, <em class=sig-param><span class=n><span class=pre>region_signals</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'$UNCHANGED$'</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.set_inverse_transform_request>#</a></dt><dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class=pre>inverse_transform</span></code> method.</p> <p>Note that this method is only relevant if <code class="docutils literal notranslate"><span class=pre>enable_metadata_routing=True</span></code> (see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config><code class="xref py py-func docutils literal notranslate"><span class=pre>sklearn.set_config</span></code></a>). Please see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/metadata_routing.html#metadata-routing><span class="xref std std-ref">User Guide</span></a> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul class=simple><li><p><code class="docutils literal notranslate"><span class=pre>True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class=pre>inverse_transform</span></code> if provided. The request is ignored if metadata is not provided.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class=pre>inverse_transform</span></code>.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li></ul> <p>The default (<code class="docutils literal notranslate"><span class=pre>sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <div class=versionadded><p><span class="versionmodified added">New in version 1.3.</span></p></div> <div class="admonition note"><p class=admonition-title>Note</p><p>This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>. Otherwise it has no effect.</p></div> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>region_signals</strong><span class=classifier>str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class=pre>region_signals</span></code> parameter in <code class="docutils literal notranslate"><span class=pre>inverse_transform</span></code>.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>object</span></dt><dd><p>The updated object.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.set_output><span class="sig-name descname"><span class=pre>set_output</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>*</span></span></em>, <em class=sig-param><span class=n><span class=pre>transform</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.set_output>#</a></dt><dd><p>Set output container.</p> <p>See <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html#sphx-glr-auto-examples-miscellaneous-plot-set-output-py><span>Introducing the set_output API</span></a> for an example on how to use the API.</p> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>transform</strong><span class=classifier>{“default”, “pandas”}, default=None</span></dt><dd><p>Configure output of <cite>transform</cite> and <cite>fit_transform</cite>.</p> <ul class=simple><li><p><cite>“default”</cite>: Default output format of a transformer</p></li><li><p><cite>“pandas”</cite>: DataFrame output</p></li><li><p><cite>“polars”</cite>: Polars output</p></li><li><p><cite>None</cite>: Transform configuration is unchanged</p></li></ul> <div class=versionadded><p><span class="versionmodified added">New in version 1.4: </span><cite>“polars”</cite> option was added.</p></div></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>estimator instance</span></dt><dd><p>Estimator instance.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.set_params><span class="sig-name descname"><span class=pre>set_params</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>**</span></span><span class=n><span class=pre>params</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.set_params>#</a></dt><dd><p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>). The latter have parameters of the form <code class="docutils literal notranslate"><span class=pre>&LTcomponent>__&LTparameter></span></code> so that it’s possible to update each component of a nested object.</p> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>**params</strong><span class=classifier>dict</span></dt><dd><p>Estimator parameters.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>estimator instance</span></dt><dd><p>Estimator instance.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.set_transform_request><span class="sig-name descname"><span class=pre>set_transform_request</span></span><span class=sig-paren>(</span><em class=sig-param><span class=o><span class=pre>*</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'$UNCHANGED$'</span></span></em>, <em class=sig-param><span class=n><span class=pre>imgs</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'$UNCHANGED$'</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>'$UNCHANGED$'</span></span></em><span class=sig-paren>)</span><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.set_transform_request>#</a></dt><dd><p>Request metadata passed to the <code class="docutils literal notranslate"><span class=pre>transform</span></code> method.</p> <p>Note that this method is only relevant if <code class="docutils literal notranslate"><span class=pre>enable_metadata_routing=True</span></code> (see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.set_config.html#sklearn.set_config><code class="xref py py-func docutils literal notranslate"><span class=pre>sklearn.set_config</span></code></a>). Please see <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/metadata_routing.html#metadata-routing><span class="xref std std-ref">User Guide</span></a> on how the routing mechanism works.</p> <p>The options for each parameter are:</p> <ul class=simple><li><p><code class="docutils literal notranslate"><span class=pre>True</span></code>: metadata is requested, and passed to <code class="docutils literal notranslate"><span class=pre>transform</span></code> if provided. The request is ignored if metadata is not provided.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>False</span></code>: metadata is not requested and the meta-estimator will not pass it to <code class="docutils literal notranslate"><span class=pre>transform</span></code>.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>None</span></code>: metadata is not requested, and the meta-estimator will raise an error if the user provides it.</p></li><li><p><code class="docutils literal notranslate"><span class=pre>str</span></code>: metadata should be passed to the meta-estimator with this given alias instead of the original name.</p></li></ul> <p>The default (<code class="docutils literal notranslate"><span class=pre>sklearn.utils.metadata_routing.UNCHANGED</span></code>) retains the existing request. This allows you to change the request for some parameters and not others.</p> <div class=versionadded><p><span class="versionmodified added">New in version 1.3.</span></p></div> <div class="admonition note"><p class=admonition-title>Note</p><p>This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a <a class="reference external"title="(in scikit-learn v1.4)"href=https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline><code class="xref py py-class docutils literal notranslate"><span class=pre>Pipeline</span></code></a>. Otherwise it has no effect.</p></div> <dl class="field-list simple"><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt><strong>confounds</strong><span class=classifier>str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class=pre>confounds</span></code> parameter in <code class="docutils literal notranslate"><span class=pre>transform</span></code>.</p></dd><dt><strong>imgs</strong><span class=classifier>str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class=pre>imgs</span></code> parameter in <code class="docutils literal notranslate"><span class=pre>transform</span></code>.</p></dd><dt><strong>sample_mask</strong><span class=classifier>str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED</span></dt><dd><p>Metadata routing for <code class="docutils literal notranslate"><span class=pre>sample_mask</span></code> parameter in <code class="docutils literal notranslate"><span class=pre>transform</span></code>.</p></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>self</strong><span class=classifier>object</span></dt><dd><p>The updated object.</p></dd></dl></dd></dl></dd></dl> <dl class="py method"><dt class="sig sig-object py"id=nilearn.maskers.MultiNiftiMapsMasker.transform_single_imgs><span class="sig-name descname"><span class=pre>transform_single_imgs</span></span><span class=sig-paren>(</span><em class=sig-param><span class=n><span class=pre>imgs</span></span></em>, <em class=sig-param><span class=n><span class=pre>confounds</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em>, <em class=sig-param><span class=n><span class=pre>sample_mask</span></span><span class=o><span class=pre>=</span></span><span class=default_value><span class=pre>None</span></span></em><span class=sig-paren>)</span><a class="reference external"href=https://github.com/nilearn/nilearn/blob/a950195e8/nilearn/maskers/nifti_maps_masker.py#L461><span class=viewcode-link><span class=pre>[source]</span></span></a><a title="Link to this definition"class=headerlink href=#nilearn.maskers.MultiNiftiMapsMasker.transform_single_imgs>#</a></dt><dd><p>Extract signals from a single 4D niimg.</p> <dl class=field-list><dt class=field-odd>Parameters<span class=colon>:</span></dt><dd class=field-odd><dl><dt><strong>imgs</strong><span class=classifier>3D/4D Niimg-like object</span></dt><dd><p>See <a class="reference internal"href=../../manipulating_images/input_output.html#extracting-data><span class="std std-ref">Input and output: neuroimaging data representation</span></a>. Images to process. If a 3D niimg is provided, a singleton dimension will be added to the output to represent the single scan in the niimg.</p></dd><dt><strong>confounds</strong><span class=classifier>CSV file or array-like, optional</span></dt><dd><p>This parameter is passed to signal.clean. Please see the related documentation for details. shape: (number of scans, number of confounds)</p></dd><dt><strong>sample_mask</strong><span class=classifier>Any type compatible with numpy-array indexing, optional</span></dt><dd><p>shape: (number of scans - number of volumes removed, ) Masks the niimgs along time/fourth dimension to perform scrubbing (remove volumes with high motion) and/or non-steady-state volumes. This parameter is passed to signal.clean.</p> <blockquote><div><div class=versionadded><p><span class="versionmodified added">New in version 0.8.0.</span></p></div></div></blockquote></dd></dl></dd><dt class=field-even>Returns<span class=colon>:</span></dt><dd class=field-even><dl class=simple><dt><strong>region_signals</strong><span class=classifier>2D numpy.ndarray</span></dt><dd><p>Signal for each map. shape: (number of scans, number of maps)</p></dd></dl></dd><dt class=field-odd>Warns<span class=colon>:</span></dt><dd class=field-odd><dl class=simple><dt>DeprecationWarning</dt><dd><p>If a 3D niimg input is provided, the current behavior (adding a singleton dimension to produce a 2D array) is deprecated. Starting in version 0.12, a 1D array will be returned for 3D inputs.</p></dd></dl></dd></dl></dd></dl></dd></dl><section id=examples-using-nilearn-maskers-multiniftimapsmasker><h2>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.maskers.MultiNiftiMapsMasker</span></code><a title="Link to this heading"class=headerlink href=#examples-using-nilearn-maskers-multiniftimapsmasker>#</a></h2><div class=sphx-glr-thumbnails><div tooltip="This examples shows how to turn a parcellation into connectome for visualization. This requires..."class=sphx-glr-thumbcontainer><img alt src=../../_images/sphx_glr_plot_atlas_comparison_thumb.png><p><a class="reference internal"href=../../auto_examples/03_connectivity/plot_atlas_comparison.html#sphx-glr-auto-examples-03-connectivity-plot-atlas-comparison-py><span class="std std-ref">Comparing connectomes on different reference atlases</span></a></p><div class=sphx-glr-thumbnail-title>Comparing connectomes on different reference atlases</div></div></div><div style=clear:both></div></section></section></article></div><footer><div class=related-pages><a class=next-page href=nilearn.maskers.NiftiSpheresMasker.html> <div class=page-info><div class=context><span>Next</span></div><div class=title>nilearn.maskers.NiftiSpheresMasker</div></div> <svg class=furo-related-icon><use href=#svg-arrow-right></use></svg> </a><a class=prev-page href=nilearn.maskers.NiftiMapsMasker.html> <svg class=furo-related-icon><use href=#svg-arrow-right></use></svg> <div class=page-info><div class=context><span>Previous</span></div><div class=title>nilearn.maskers.NiftiMapsMasker</div></div> </a></div><div class=bottom-of-page><div class=left-details><div class=copyright>Copyright © The nilearn developers 2010-2023</div> Made with <a href=https://www.sphinx-doc.org/>Sphinx</a> and <a class=muted-link href=https://pradyunsg.me>@pradyunsg</a>'s <a href=https://github.com/pradyunsg/furo>Furo</a></div><div class=right-details><div class=icons><a class="muted-link fa-brands fa-solid fa-github fa-2x"aria-label=GitHub href=https://github.com/nilearn/nilearn></a><a class="muted-link fa-brands fa-solid fa-twitter fa-2x"aria-label=Twitter href=https://twitter.com/nilearn></a><a class="muted-link fa-brands fa-solid fa-mastodon fa-2x"aria-label=Mastodon href=https://fosstodon.org/@nilearn></a><a class="muted-link fa-brands fa-solid fa-discord fa-2x"aria-label=Discord href=https://discord.gg/SsQABEJHkZ></a></div></div></div></footer></div><aside class=toc-drawer><div class="toc-sticky toc-scroll"><div class=toc-title-container><span class=toc-title> On this page </span></div><div class=toc-tree-container><div class=toc-tree><ul><li><a class="reference internal"href=#>nilearn.maskers.MultiNiftiMapsMasker</a><ul><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker</span></code></a><ul><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.__init__><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.__init__</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.transform_imgs><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.transform_imgs</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.transform><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.transform</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.fit><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.fit</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.fit_transform><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.fit_transform</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.generate_report><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.generate_report</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.get_metadata_routing><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.get_metadata_routing</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.get_params><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.get_params</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.inverse_transform><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.inverse_transform</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.set_fit_request><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.set_fit_request</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.set_inverse_transform_request><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.set_inverse_transform_request</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.set_output><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.set_output</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.set_params><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.set_params</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.set_transform_request><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.set_transform_request</span></code></a></li><li><a class="reference internal"href=#nilearn.maskers.MultiNiftiMapsMasker.transform_single_imgs><code class="docutils literal notranslate"><span class=pre>MultiNiftiMapsMasker.transform_single_imgs</span></code></a></li></ul></li><li><a class="reference internal"href=#examples-using-nilearn-maskers-multiniftimapsmasker>Examples using <code class="docutils literal notranslate"><span class=pre>nilearn.maskers.MultiNiftiMapsMasker</span></code></a></li></ul></li></ul></div></div></div></aside></div></div><script src=../../_static/documentation_options.js?v=5929fcd5></script><script src=../../_static/doctools.js?v=888ff710></script><script src=../../_static/sphinx_highlight.js?v=dc90522c></script><script src=../../_static/scripts/furo.js?v=32e29ea5></script><script src=../../_static/clipboard.min.js?v=a7894cd8></script><script src=../../_static/copybutton.js?v=4ea706d9></script><script src=../../_static/design-tabs.js?v=36754332></script></body></html>