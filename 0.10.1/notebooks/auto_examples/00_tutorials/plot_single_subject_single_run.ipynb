{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Intro to GLM Analysis: a single-session, single-subject fMRI dataset\n\nIn this tutorial, we use a General Linear Model (:term:`GLM`) to compare the\n:term:`fMRI` signal during periods of auditory stimulation\nversus periods of rest.\n\nThe analyse described here is performed in the native space, directly on the\noriginal :term:`EPI` scans without any spatial or temporal preprocessing.\n(More sensitive results would likely be obtained on the corrected,\nspatially normalized and smoothed images).\n\n## The data\n\nThe dataset comes from an experiment conducted at the FIL by Geraint Rees\nunder the direction of Karl Friston. It is provided by FIL methods\ngroup which develops the SPM software.\n\nAccording to SPM documentation, 96 scans were acquired (repetition time\n:term:`TR` = 7s) in one session. The paradigm consisted of alternating periods\nof stimulation and rest, lasting 42s each (that is, for 6 scans). The session\nstarted with a rest block.  Auditory stimulation consisted of bi-syllabic words\npresented binaurally at a rate of 60 per minute.\nThe functional data starts at scan number 4,\nthat is the image file ``fM00223_004``.\n\nThe whole brain :term:`BOLD`/:term:`EPI` images were acquired on a 2T Siemens\nMAGNETOM Vision system. Each scan consisted of 64 contiguous slices (64x64x64\n3mm x 3mm x 3mm :term:`voxels<voxel>`).\nAcquisition of one scan took 6.05s, with the scan to scan repeat time\n(:term:`TR`) set arbitrarily to 7s.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieving the data\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial, we load the data using a data downloading\n          function. To input your own data, you will need to provide\n          a list of paths to your own files in the ``subject_data`` variable.\n          These should abide to the Brain Imaging Data Structure\n          (:term:`BIDS`) organization.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_spm_auditory\n\nsubject_data = fetch_spm_auditory()\nsubject_data.func  # print the list of names of functional images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can display the first functional image and the subject's anatomy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_anat, plot_img, plot_stat_map\n\nplot_img(subject_data.func[0], colorbar=True, cbar_tick_format=\"%i\")\nplot_anat(subject_data.anat, colorbar=True, cbar_tick_format=\"%i\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we concatenate all the 3D :term:`EPI` image into a single 4D image,\nthen we average them in order to create a background\nimage that will be used to display the activations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import concat_imgs, mean_img\n\nfmri_img = concat_imgs(subject_data.func)\nmean_img = mean_img(fmri_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specifying the experimental paradigm\n\nWe must now provide a description of the experiment, that is, define the\ntiming of the auditory stimulation and rest periods. This is typically\nprovided in an events.tsv file. The path of this file is\nprovided in the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nevents = pd.read_table(subject_data[\"events\"])\nevents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performing the GLM analysis\n\nIt is now time to create and estimate a ``FirstLevelModel`` object,\nthat will generate the *design matrix*\nusing the information provided by the ``events`` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import FirstLevelModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters of the first-level model\n\n* t_r=7(s) is the time of repetition of acquisitions\n* noise_model='ar1' specifies the noise covariance model: a lag-1 dependence\n* standardize=False means that we do not want\n  to rescale the time series to mean 0, variance 1\n* hrf_model='spm' means that we rely on the SPM \"canonical hrf\" model\n  (without time or dispersion derivatives)\n* drift_model='cosine' means that we model the signal drifts\n  as slow oscillating time functions\n* high_pass=0.01(Hz) defines the cutoff frequency\n  (inverse of the time period).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmri_glm = FirstLevelModel(\n    t_r=7,\n    noise_model=\"ar1\",\n    standardize=False,\n    hrf_model=\"spm\",\n    drift_model=\"cosine\",\n    high_pass=0.01,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have specified the model, we can run it on the :term:`fMRI` image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmri_glm = fmri_glm.fit(fmri_img, events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can inspect the design matrix (rows represent time, and\ncolumns contain the predictors).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = fmri_glm.design_matrices_[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Formally, we have taken the first design matrix, because the model is\nimplictily meant to for multiple runs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom nilearn.plotting import plot_design_matrix\n\nplot_design_matrix(design_matrix)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the design matrix image to disk\nfirst create a directory where you want to write the images\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\noutdir = \"results\"\nif not os.path.exists(outdir):\n    os.mkdir(outdir)\n\nfrom os.path import join\n\nplot_design_matrix(\n    design_matrix, output_file=join(outdir, \"design_matrix.png\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first column contains the expected response profile of regions which are\nsensitive to the auditory stimulation.\nLet's plot this first column\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.plot(design_matrix[\"active\"])\nplt.xlabel(\"scan\")\nplt.title(\"Expected Auditory Response\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting voxels with significant effects\n\nTo access the estimated coefficients (Betas of the :term:`GLM` model), we\ncreated :term:`contrast` with a single '1' in each of the columns: The role\nof the :term:`contrast` is to select some columns of the model --and\npotentially weight them-- to study the associated statistics. So in\na nutshell, a contrast is a weighted combination of the estimated\neffects.  Here we can define canonical contrasts that just consider\nthe two effects in isolation ---let's call them \"conditions\"---\nthen a :term:`contrast` that makes the difference between these conditions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nconditions = {\"active\": np.zeros(16), \"rest\": np.zeros(16)}\nconditions[\"active\"][0] = 1\nconditions[\"rest\"][1] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then compare the two conditions 'active' and 'rest' by\ndefining the corresponding :term:`contrast`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "active_minus_rest = conditions[\"active\"] - conditions[\"rest\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at it: plot the coefficients of the :term:`contrast`, indexed by\nthe names of the columns of the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_contrast_matrix\n\nplot_contrast_matrix(active_minus_rest, design_matrix=design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we compute the :term:`'estimated effect'<Parameter Estimate>`.\nIt is in :term:`BOLD` signal unit, but has no statistical guarantees,\nbecause it does not take into account the associated variance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eff_map = fmri_glm.compute_contrast(\n    active_minus_rest, output_type=\"effect_size\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to get statistical significance, we form a t-statistic, and\ndirectly convert it into z-scale. The z-scale means that the values\nare scaled to match a standard Gaussian distribution (mean=0,\nvariance=1), across voxels, if there were no effects in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_map = fmri_glm.compute_contrast(active_minus_rest, output_type=\"z_score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot thresholded z scores map\n\nWe display it on top of the average\nfunctional image of the series (could be the anatomical image of the\nsubject).  We use arbitrarily a threshold of 3.0 in z-scale. We'll\nsee later how to use corrected thresholds. We will show 3\naxial views, with display_mode='z' and cut_coords=3.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=3.0,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (Z>3)\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistical significance testing. One should worry about the\nstatistical validity of the procedure: here we used an arbitrary\nthreshold of 3.0 but the threshold should provide some guarantees on\nthe risk of false detections (aka type-1 errors in statistics).\nOne suggestion is to control the false positive rate\n(:term:`fpr<FPR correction>`, denoted by alpha)\nat a certain level, e.g. 0.001: this means that there is 0.1% chance\nof declaring an inactive :term:`voxel`, active.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm import threshold_stats_img\n\n_, threshold = threshold_stats_img(z_map, alpha=0.001, height_control=\"fpr\")\nprint(f\"Uncorrected p<0.001 threshold: {threshold:.3f}\")\nplot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (p<0.001)\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The problem is that with this you expect 0.001 * n_voxels to show up\nwhile they're not active --- tens to hundreds of voxels. A more\nconservative solution is to control the family wise error rate,\ni.e. the probability of making only one false detection, say at\n5%. For that we use the so-called Bonferroni correction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, threshold = threshold_stats_img(\n    z_map, alpha=0.05, height_control=\"bonferroni\"\n)\nprint(f\"Bonferroni-corrected, p<0.05 threshold: {threshold:.3f}\")\nplot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (p<0.05, corrected)\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is quite conservative indeed!  A popular alternative is to\ncontrol the expected proportion of\nfalse discoveries among detections. This is called the False\ndiscovery rate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, threshold = threshold_stats_img(z_map, alpha=0.05, height_control=\"fdr\")\nprint(f\"False Discovery rate = 0.05 threshold: {threshold:.3f}\")\nplot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (fdr=0.05)\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally people like to discard isolated voxels (aka \"small\nclusters\") from these images. It is possible to generate a\nthresholded map with small clusters removed by providing a\ncluster_threshold argument. Here clusters smaller than 10 voxels\nwill be discarded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clean_map, threshold = threshold_stats_img(\n    z_map, alpha=0.05, height_control=\"fdr\", cluster_threshold=10\n)\nplot_stat_map(\n    clean_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (fdr=0.05), clusters > 10 voxels\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save the effect and zscore maps to the disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_map.to_filename(join(outdir, \"active_vs_rest_z_map.nii.gz\"))\neff_map.to_filename(join(outdir, \"active_vs_rest_eff_map.nii.gz\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can furthermore extract and report the found positions in a table.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.reporting import get_clusters_table\n\ntable = get_clusters_table(\n    z_map, stat_threshold=threshold, cluster_threshold=20\n)\ntable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table can be saved for future use.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "table.to_csv(join(outdir, \"table.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performing an F-test\n\n\"active vs rest\" is a typical t test: condition versus\nbaseline. Another popular type of test is an F test in which one\nseeks whether a certain combination of conditions (possibly two-,\nthree- or higher-dimensional) explains a significant proportion of\nthe signal.  Here one might for instance test which voxels are well\nexplained by the combination of the active and rest condition.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specify the contrast and compute the corresponding map. Actually, the\ncontrast specification is done exactly the same way as for t-\ncontrasts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "effects_of_interest = np.vstack((conditions[\"active\"], conditions[\"rest\"]))\nplot_contrast_matrix(effects_of_interest, design_matrix)\nplt.show()\n\nz_map = fmri_glm.compute_contrast(effects_of_interest, output_type=\"z_score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the statistic has been converted to a z-variable, which\nmakes it easier to represent it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clean_map, threshold = threshold_stats_img(\n    z_map, alpha=0.05, height_control=\"fdr\", cluster_threshold=10\n)\nplot_stat_map(\n    clean_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Effects of interest (fdr=0.05), clusters > 10 voxels\",\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oops, there is a lot of non-neural signal in there (ventricles, arteries)...\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
