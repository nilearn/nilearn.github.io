{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Different classifiers in decoding the Haxby dataset\n\nHere we compare different classifiers on a visual object recognition\ndecoding task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We start by loading data using nilearn dataset fetcher\nfrom nilearn import datasets\nfrom nilearn.image import get_data\n\n# by default 2nd subject data will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint(\n    \"First subject anatomical nifti image (3D) located is \"\n    f\"at: {haxby_dataset.anat[0]}\"\n)\nprint(\n    \"First subject functional nifti image (4D) is located \"\n    f\"at: {haxby_dataset.func[0]}\"\n)\n\n# load labels\nimport numpy as np\nimport pandas as pd\n\nlabels = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\nstimuli = labels[\"labels\"]\n\n# identify resting state (baseline) labels in order to be able to remove them\nresting_state = stimuli == \"rest\"\n\n# extract the indices of the images corresponding to some condition or task\ntask_mask = np.logical_not(resting_state)\n\n# find names of remaining active labels\ncategories = stimuli[task_mask].unique()\n\n# extract tags indicating to which acquisition run a tag belongs\nsession_labels = labels[\"chunks\"][task_mask]\n\n\n# Load the fMRI data\n# For decoding, standardizing is often very important\nmask_filename = haxby_dataset.mask_vt[0]\nfunc_filename = haxby_dataset.func[0]\n\n\n# Because the data is in one single large 4D image, we need to use\n# index_img to do the split easily.\nfrom nilearn.image import index_img\n\nfmri_niimgs = index_img(func_filename, task_mask)\nclassification_target = stimuli[task_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the decoder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Then we define the various classifiers that we use\nclassifiers = [\n    \"svc_l2\",\n    \"svc_l1\",\n    \"logistic_l1\",\n    \"logistic_l2\",\n    \"ridge_classifier\",\n]\n\n# Here we compute prediction scores and run time for all these\n# classifiers\nimport time\n\nfrom nilearn.decoding import Decoder\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ncv = LeaveOneGroupOut()\nclassifiers_data = {}\n\nfor classifier_name in sorted(classifiers):\n    print(70 * \"_\")\n\n    # The decoder has as default score the `roc_auc`\n    decoder = Decoder(\n        estimator=classifier_name,\n        mask=mask_filename,\n        standardize=\"zscore_sample\",\n        cv=cv,\n    )\n    t0 = time.time()\n    decoder.fit(fmri_niimgs, classification_target, groups=session_labels)\n\n    classifiers_data[classifier_name] = {\"score\": decoder.cv_scores_}\n    print(f\"{classifier_name:10}: {time.time() - t0:.2f}s\")\n    for category in categories:\n        mean = np.mean(classifiers_data[classifier_name][\"score\"][category])\n        std = np.std(classifiers_data[classifier_name][\"score\"][category])\n        print(f\"    {category:14} vs all -- AUC: {mean:1.2f} +- {std:1.2f}\")\n\n    # Adding the average performance per estimator\n    scores = classifiers_data[classifier_name][\"score\"]\n    scores[\"AVERAGE\"] = np.mean(list(scores.values()), axis=0)\n    classifiers_data[classifier_name][\"score\"] = scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Then we make a rudimentary diagram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 6))\n\nall_categories = np.sort(np.hstack([categories, \"AVERAGE\"]))\ntick_position = np.arange(len(all_categories))\nplt.yticks(tick_position + 0.25, all_categories)\nheight = 0.1\n\nfor color, classifier_name in zip([\"b\", \"m\", \"k\", \"r\", \"g\"], classifiers):\n    score_means = [\n        np.mean(classifiers_data[classifier_name][\"score\"][category])\n        for category in all_categories\n    ]\n\n    plt.barh(\n        tick_position,\n        score_means,\n        label=classifier_name.replace(\"_\", \" \"),\n        height=height,\n        color=color,\n    )\n    tick_position = tick_position + height\n\nplt.xlabel(\"Classification accuracy (AUC score)\")\nplt.ylabel(\"Visual stimuli category\")\nplt.xlim(xmin=0.5)\nplt.legend(loc=\"lower left\", ncol=1)\nplt.title(\n    \"Category-specific classification accuracy for different classifiers\"\n)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that for a fixed penalty the results are similar between the svc\nand the logistic regression. The main difference relies on the penalty\n($\\ell_1$ and $\\ell_2$). The sparse penalty works better because we are in\nan intra-subject setting.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the face vs house map\n\nRestrict the decoding to face vs house\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "condition_mask = np.logical_or(stimuli == \"face\", stimuli == \"house\")\nstimuli = stimuli[condition_mask]\nassert len(stimuli) == 216\nfmri_niimgs_condition = index_img(func_filename, condition_mask)\nsession_labels = labels[\"chunks\"][condition_mask]\ncategories = stimuli.unique()\nassert len(categories) == 2\n\nfor classifier_name in sorted(classifiers):\n    decoder = Decoder(\n        estimator=classifier_name,\n        mask=mask_filename,\n        standardize=\"zscore_sample\",\n        cv=cv,\n    )\n    decoder.fit(fmri_niimgs_condition, stimuli, groups=session_labels)\n    classifiers_data[classifier_name] = {}\n    classifiers_data[classifier_name][\"score\"] = decoder.cv_scores_\n    classifiers_data[classifier_name][\"map\"] = decoder.coef_img_[\"face\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the face vs house map for the different classifiers\nUse the average EPI as a background\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import mean_img\nfrom nilearn.plotting import plot_stat_map, show\n\nmean_epi_img = mean_img(func_filename)\n\nfor classifier_name in sorted(classifiers):\n    coef_img = classifiers_data[classifier_name][\"map\"]\n    threshold = np.max(np.abs(get_data(coef_img))) * 1e-3\n    plot_stat_map(\n        coef_img,\n        bg_img=mean_epi_img,\n        display_mode=\"z\",\n        cut_coords=[-15],\n        threshold=threshold,\n        title=f\"{classifier_name.replace('_', ' ')}: face vs house\",\n    )\n\nshow()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
