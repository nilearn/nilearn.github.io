{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nSetting a parameter by cross-validation\n=======================================================\n\nHere we set the number of features selected in an Anova-SVC approach to\nmaximize the cross-validation score.\n\nAfter separating 2 sessions for validation, we vary that parameter and\nmeasure the cross-validation score. We also measure the prediction score\non the left-out validation data. As we can see, the two scores vary by a\nsignificant amount: this is due to sampling noise in cross validation,\nand choosing the parameter k to maximize the cross-validation score,\nmight not maximize the score on left-out data.\n\nThus using data to maximize a cross-validation score computed on that\nsame data is likely to optimistic and lead to an overfit.\n\nThe proper appraoch is known as a \"nested cross-validation\". It consists\nin doing cross-validation loops to set the model parameters inside the\ncross-validation loop used to judge the prediction performance: the\nparameters are set separately on each fold, never using the data used to\nmeasure performance.\n\nIn scikit-learn, this can be done using the GridSearchCV object, that\nwill automatically select the best parameters of an estimator from a\ngrid of parameter values.\n\nOne difficulty here is that we are working with a composite estimator: a\npipeline of feature selection followed by SVC. Thus to give the name\nof the parameter that we want to tune we need to give the name of the\nstep in the pipeline, followed by the name of the parameter, with '__' as\na separator.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Load the Haxby dataset\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn import datasets\nimport numpy as np\n# by default 2nd subject data will be fetched on which we run our analysis\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('Mask nifti image (3D) is located at: %s' % haxby_dataset.mask)\nprint('Functional nifti image (4D) are located at: %s' % haxby_dataset.func[0])\n\n# Load the behavioral data\nlabels = np.recfromcsv(haxby_dataset.session_target[0], delimiter=\" \")\ny = labels['labels']\nsession = labels['chunks']\n\n# Keep only data corresponding to shoes or bottles\ncondition_mask = np.logical_or(y == b'shoe', y == b'bottle')\ny = y[condition_mask]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Prepare the data with the NiftiMasker\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from nilearn.input_data import NiftiMasker\n\nmask_filename = haxby_dataset.mask\n# For decoding, standardizing is often very important\nnifti_masker = NiftiMasker(mask_img=mask_filename, sessions=session,\n                           smoothing_fwhm=4, standardize=True,\n                           memory=\"nilearn_cache\", memory_level=1)\nfunc_filename = haxby_dataset.func[0]\nX = nifti_masker.fit_transform(func_filename)\n# Restrict to non rest data\nX = X[condition_mask]\nsession = session[condition_mask]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Build the decoder that we will use\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Define the prediction function to be used.\n# Here we use a Support Vector Classification, with a linear kernel\nfrom sklearn.svm import SVC\nsvc = SVC(kernel='linear')\n\n\n# Define the dimension reduction to be used.\n# Here we use a classical univariate feature selection based on F-test,\n# namely Anova. We set the number of features to be selected to 500\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfeature_selection = SelectKBest(f_classif, k=500)\n\n# We have our classifier (SVC), our feature selection (SelectKBest), and now,\n# we can plug them together in a *pipeline* that performs the two operations\n# successively:\nfrom sklearn.pipeline import Pipeline\nanova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Compute prediction scores using cross-validation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "anova_svc.fit(X, y)\ny_pred = anova_svc.predict(X)\n\nfrom sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\ncv = LeaveOneLabelOut(session[session < 10])\n\nk_range = [10, 15, 30, 50, 150, 300, 500, 1000, 1500, 3000, 5000]\ncv_scores = []\nscores_validation = []\n\nfor k in k_range:\n    feature_selection.k = k\n    cv_scores.append(np.mean(\n        cross_val_score(anova_svc, X[session < 10], y[session < 10])))\n    print(\"CV score: %.4f\" % cv_scores[-1])\n\n    anova_svc.fit(X[session < 10], y[session < 10])\n    y_pred = anova_svc.predict(X[session == 10])\n    scores_validation.append(np.mean(y_pred == y[session == 10]))\n    print(\"score validation: %.4f\" % scores_validation[-1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Nested cross-validation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.grid_search import GridSearchCV\n# We are going to tune the parameter 'k' of the step called 'anova' in\n# the pipeline. Thus we need to address it as 'anova__k'.\n\n# Note that GridSearchCV takes an n_jobs argument that can make it go\n# much faster\ngrid = GridSearchCV(anova_svc, param_grid={'anova__k': k_range}, verbose=1)\nnested_cv_scores = cross_val_score(grid, X, y)\n\nprint(\"Nested CV score: %.4f\" % np.mean(nested_cv_scores))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Plot the prediction scores using matplotlib\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\nplt.figure(figsize=(6, 4))\nplt.plot(cv_scores, label='Cross validation scores')\nplt.plot(scores_validation, label='Left-out validation data scores')\nplt.xticks(np.arange(len(k_range)), k_range)\nplt.axis('tight')\nplt.xlabel('k')\n\nplt.axhline(np.mean(nested_cv_scores),\n            label='Nested cross-validation',\n            color='r')\n\nplt.legend(loc='best', frameon=False)\nplt.show()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.6", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}