
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/04_glm_first_level/plot_bids_features.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_04_glm_first_level_plot_bids_features.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_04_glm_first_level_plot_bids_features.py:


First level analysis of a complete BIDS dataset from openneuro
==============================================================

Full step-by-step example of fitting a :term:`GLM`
to perform a first level analysis in an openneuro :term:`BIDS` dataset.
We demonstrate how :term:`BIDS`
derivatives can be exploited to perform a simple one subject analysis with
minimal code. Details about the :term:`BIDS` standard are available at
`https://bids.neuroimaging.io/ <https://bids.neuroimaging.io/>`_.
We also demonstrate how to download individual groups of files from the
Openneuro s3 bucket.

More specifically:

1. Download an :term:`fMRI` :term:`BIDS` dataset
   with derivatives from openneuro.
2. Extract first level model objects automatically
   from the :term:`BIDS` dataset.
3. Demonstrate Quality assurance of Nilearn estimation against available FSL.
   estimation in the openneuro dataset.
4. Display contrast plot and uncorrected first level statistics table report.

.. GENERATED FROM PYTHON SOURCE LINES 26-33

Fetch openneuro :term:`BIDS` dataset
------------------------------------
We download one subject from the stopsignal task
in the ds000030 V4 :term:`BIDS` dataset available in openneuro.
This dataset contains the necessary information to run a statistical analysis
using Nilearn. The dataset also contains statistical results from a previous
FSL analysis that we can employ for comparison with the Nilearn estimation.

.. GENERATED FROM PYTHON SOURCE LINES 33-62

.. code-block:: Python

    from nilearn.datasets import (
        fetch_ds000030_urls,
        fetch_openneuro_dataset,
        select_from_index,
    )

    _, urls = fetch_ds000030_urls()

    exclusion_patterns = [
        "*group*",
        "*phenotype*",
        "*mriqc*",
        "*parameter_plots*",
        "*physio_plots*",
        "*space-fsaverage*",
        "*space-T1w*",
        "*dwi*",
        "*beh*",
        "*task-bart*",
        "*task-rest*",
        "*task-scap*",
        "*task-task*",
    ]
    urls = select_from_index(
        urls, exclusion_filters=exclusion_patterns, n_subjects=1
    )

    data_dir, _ = fetch_openneuro_dataset(urls=urls)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_ds000030_urls] Dataset found in 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed
    [fetch_openneuro_dataset] Dataset found in 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed




.. GENERATED FROM PYTHON SOURCE LINES 63-75

Obtain FirstLevelModel objects automatically and fit arguments
--------------------------------------------------------------
From the dataset directory we automatically obtain FirstLevelModel objects
with their subject_id filled from the :term:`BIDS` dataset.
Moreover we obtain,
for each model, the list of run images and their respective events and
confound regressors. Those are inferred from the confounds.tsv files
available in the :term:`BIDS` dataset.
To get the first level models we have to specify the dataset directory,
the task_label and the space_label as specified in the file names.
We also have to provide the folder with the desired derivatives, that in this
case were produced by the :term:`fMRIPrep` :term:`BIDS` app.

.. GENERATED FROM PYTHON SOURCE LINES 75-95

.. code-block:: Python

    from nilearn.glm.first_level import first_level_from_bids

    task_label = "stopsignal"
    space_label = "MNI152NLin2009cAsym"
    derivatives_folder = "derivatives/fmriprep"
    (
        models,
        models_run_imgs,
        models_events,
        models_confounds,
    ) = first_level_from_bids(
        data_dir,
        task_label,
        space_label,
        smoothing_fwhm=5.0,
        derivatives_folder=derivatives_folder,
        n_jobs=2,
        verbose=1,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/04_glm_first_level/plot_bids_features.py:85: UserWarning: 
    No bold.json found in BIDS folder in:
     /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/derivatives/fmriprep.
      ) = first_level_from_bids(
    /home/runner/work/nilearn/nilearn/examples/04_glm_first_level/plot_bids_features.py:85: UserWarning: 'slice_time_ref' not provided and cannot be inferred from metadata.
    It will be assumed that the slice timing reference is 0.0 percent of the repetition time.
    If it is not the case it will need to be set manually in the generated list of models.
      ) = first_level_from_bids(
    [first_level_from_bids] 
    Found the following 1 preprocessed BOLD files
    - for subject 10159
    - for filter: [('task', 'stopsignal'), ('space', 'MNI152NLin2009cAsym')]:
            - 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/derivatives/fmri
    prep/sub-10159/func/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_desc-pre
    proc_bold.nii.gz

    [first_level_from_bids] 
    Found the following 1 events files
    - for subject 10159
    - for filter: [('task', 'stopsignal')]:
            - 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/sub-10159/func/s
    ub-10159_task-stopsignal_events.tsv

    [first_level_from_bids] 
    Found the following 1 confounds files
    - for subject 10159
    - for filter: [('task', 'stopsignal')]:
            - 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/derivatives/fmri
    prep/sub-10159/func/sub-10159_task-stopsignal_desc-confounds_regressors.tsv





.. GENERATED FROM PYTHON SOURCE LINES 96-97

Access the model and model arguments of the subject and process events.

.. GENERATED FROM PYTHON SOURCE LINES 97-123

.. code-block:: Python


    model, imgs, events, confounds = (
        models[0],
        models_run_imgs[0],
        models_events[0],
        models_confounds[0],
    )
    subject = f"sub-{model.subject_label}"
    model.minimize_memory = False  # override default

    from pathlib import Path

    from nilearn.interfaces.fsl import get_design_from_fslmat

    fsl_design_matrix_path = (
        Path(data_dir)
        / "derivatives"
        / "task"
        / subject
        / "stopsignal.feat"
        / "design.mat"
    )
    design_matrix = get_design_from_fslmat(
        fsl_design_matrix_path, column_names=None
    )








.. GENERATED FROM PYTHON SOURCE LINES 124-127

We identify the columns of the Go and StopSuccess conditions of the
design matrix inferred from the FSL file, to use them later for contrast
definition.

.. GENERATED FROM PYTHON SOURCE LINES 127-134

.. code-block:: Python

    design_columns = [
        f"cond_{int(i):02}" for i in range(len(design_matrix.columns))
    ]
    design_columns[0] = "Go"
    design_columns[4] = "StopSuccess"
    design_matrix.columns = design_columns








.. GENERATED FROM PYTHON SOURCE LINES 135-138

First level model estimation (one subject)
------------------------------------------
We fit the first level model for one subject.

.. GENERATED FROM PYTHON SOURCE LINES 138-140

.. code-block:: Python

    model.fit(imgs, design_matrices=[design_matrix])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/04_glm_first_level/plot_bids_features.py:138: UserWarning: If design matrices are supplied, [t_r] will be ignored.
      model.fit(imgs, design_matrices=[design_matrix])
    [FirstLevelModel.fit] Loading data from 
    /home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/derivatives/fmri
    prep/sub-10159/func/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_desc-pre
    proc_bold.nii.gz
    [FirstLevelModel.fit] Computing the mask
    [FirstLevelModel.fit] Resampling mask
    [FirstLevelModel.fit] Finished fit
    [FirstLevelModel.fit] Computing run 1 out of 1 runs (go take a coffee, a big 
    one).
    [FirstLevelModel.fit] Performing mask computation.
    [FirstLevelModel.fit] Loading data from 
    Nifti1Image('/home/runner/nilearn_data/ds000030/ds000030_R1.0.4/uncompressed/der
    ivatives/fmriprep/sub-10159/func/sub-10159_task-stopsignal_space-MNI152NLin2009c
    Asym_desc-preproc_bold.nii.gz')
    [FirstLevelModel.fit] Smoothing images
    [FirstLevelModel.fit] Extracting region signals
    [FirstLevelModel.fit] Cleaning extracted signals
    [FirstLevelModel.fit] Masking took 2 seconds.
    [FirstLevelModel.fit] Performing GLM computation.
    [FirstLevelModel.fit] GLM took 2 seconds.
    [FirstLevelModel.fit] Computation of 1 runs done in 5 seconds.


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-8 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-8 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-8 pre {
      padding: 0;
    }

    #sk-container-id-8 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-8 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-8 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-8 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-8 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-8 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-8 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-8 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-8 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-8 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-8 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-8 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-8 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-8 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "▸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-8 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-8 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-8 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-8 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "▾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-8 div.sk-label label.sk-toggleable__label,
    #sk-container-id-8 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-8 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-8 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-8 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-8 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-8 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-8 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-8 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-8 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-8 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-8 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-8" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>First Level Model</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" checked><label for="sk-estimator-id-8" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;FirstLevelModel<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>First Level Model</pre></div> </div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 141-143

Then we compute the StopSuccess - Go contrast. We can use the column names
of the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 143-145

.. code-block:: Python

    z_map = model.compute_contrast("StopSuccess - Go")








.. GENERATED FROM PYTHON SOURCE LINES 146-150

Visualize results
-----------------
Let's have a look at the Nilearn estimation
and the FSL estimation available in the dataset.

.. GENERATED FROM PYTHON SOURCE LINES 150-181

.. code-block:: Python

    import matplotlib.pyplot as plt
    import nibabel as nib
    from scipy.stats import norm

    from nilearn.plotting import plot_glass_brain, show

    fsl_z_map = nib.load(
        Path(data_dir)
        / "derivatives"
        / "task"
        / subject
        / "stopsignal.feat"
        / "stats"
        / "zstat12.nii.gz"
    )

    plot_glass_brain(
        z_map,
        threshold=norm.isf(0.001),
        title='Nilearn Z map of "StopSuccess - Go" (unc p<0.001)',
        plot_abs=False,
        display_mode="ortho",
    )
    plot_glass_brain(
        fsl_z_map,
        threshold=norm.isf(0.001),
        title='FSL Z map of "StopSuccess - Go" (unc p<0.001)',
        plot_abs=False,
        display_mode="ortho",
    )




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_001.png
         :alt: plot bids features
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_002.png
         :alt: plot bids features
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <nilearn.plotting.displays._projectors.OrthoProjector object at 0x7f8eef613100>



.. GENERATED FROM PYTHON SOURCE LINES 182-183

We show the agreement between the 2 estimations.

.. GENERATED FROM PYTHON SOURCE LINES 183-196

.. code-block:: Python


    from nilearn.plotting import plot_bland_altman, plot_img_comparison

    plot_img_comparison(
        z_map, fsl_z_map, model.masker_, ref_label="Nilearn", src_label="FSL"
    )

    plot_bland_altman(
        z_map, fsl_z_map, model.masker_, ref_label="Nilearn", src_label="FSL"
    )

    show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_003.png
         :alt: Pearson's R: 0.97, Histogram of imgs values
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_004.png
         :alt: plot bids features
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_004.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [NiftiMasker.wrapped] Loading data from Nifti1Image(
    shape=(65, 77, 49, 1),
    affine=array([[   3.,    0.,   -0.,  -96.],
           [   0.,    3.,   -0., -132.],
           [   0.,    0.,    4.,  -78.],
           [   0.,    0.,    0.,    1.]])
    )
    [NiftiMasker.wrapped] Smoothing images
    [NiftiMasker.wrapped] Extracting region signals
    [NiftiMasker.wrapped] Cleaning extracted signals
    [NiftiMasker.wrapped] Loading data from Nifti1Image(
    shape=(65, 77, 49, 1),
    affine=array([[   3.,    0.,    0.,  -96.],
           [   0.,    3.,    0., -132.],
           [   0.,    0.,    4.,  -78.],
           [   0.,    0.,    0.,    1.]])
    )
    [NiftiMasker.wrapped] Smoothing images
    [NiftiMasker.wrapped] Extracting region signals
    [NiftiMasker.wrapped] Cleaning extracted signals
    [NiftiMasker.wrapped] Loading data from Nifti1Image(
    shape=(65, 77, 49, 1),
    affine=array([[   3.,    0.,   -0.,  -96.],
           [   0.,    3.,   -0., -132.],
           [   0.,    0.,    4.,  -78.],
           [   0.,    0.,    0.,    1.]])
    )
    [NiftiMasker.wrapped] Smoothing images
    [NiftiMasker.wrapped] Extracting region signals
    [NiftiMasker.wrapped] Cleaning extracted signals
    [NiftiMasker.wrapped] Loading data from Nifti1Image(
    shape=(65, 77, 49, 1),
    affine=array([[   3.,    0.,    0.,  -96.],
           [   0.,    3.,    0., -132.],
           [   0.,    0.,    4.,  -78.],
           [   0.,    0.,    0.,    1.]])
    )
    [NiftiMasker.wrapped] Smoothing images
    [NiftiMasker.wrapped] Extracting region signals
    [NiftiMasker.wrapped] Cleaning extracted signals




.. GENERATED FROM PYTHON SOURCE LINES 197-203

Saving model outputs to disk
----------------------------

We can now easily save the main results,
the model metadata and an HTML report to the disk.


.. GENERATED FROM PYTHON SOURCE LINES 203-219

.. code-block:: Python

    from nilearn.interfaces.bids import save_glm_to_bids

    output_dir = Path.cwd() / "results" / "plot_bids_features"
    output_dir.mkdir(exist_ok=True, parents=True)

    stat_threshold = norm.isf(0.001)

    save_glm_to_bids(
        model,
        contrasts="StopSuccess - Go",
        contrast_types={"StopSuccess - Go": "t"},
        out_dir=output_dir / "derivatives" / "nilearn_glm",
        threshold=stat_threshold,
        cluster_threshold=10,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/04_glm_first_level/plot_bids_features.py:210: UserWarning: Contrast name "StopSuccess - Go" changed to "stopsuccessMinusGo"
      save_glm_to_bids(
    [save_glm_to_bids] Saving mask...
    [save_glm_to_bids] Generating design matrices figures...
    [save_glm_to_bids] Generating contrast matrices figures...
    [save_glm_to_bids] Saving contrast-level statistical maps...
    [save_glm_to_bids] Saving model level statistical maps...
    [save_glm_to_bids] Generating HTML...


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-9 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-9 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-9 pre {
      padding: 0;
    }

    #sk-container-id-9 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-9 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-9 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-9 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-9 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-9 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-9 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-9 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-9 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-9 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-9 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-9 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-9 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-9 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "▸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-9 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-9 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-9 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-9 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "▾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-9 div.sk-label label.sk-toggleable__label,
    #sk-container-id-9 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-9 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-9 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-9 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-9 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-9 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-9 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-9 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-9 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-9 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-9 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-9" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>First Level Model</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" checked><label for="sk-estimator-id-9" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;FirstLevelModel<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>First Level Model</pre></div> </div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 220-221

View the generated files

.. GENERATED FROM PYTHON SOURCE LINES 221-224

.. code-block:: Python

    files = sorted((output_dir / "derivatives" / "nilearn_glm").glob("**/*"))
    print("\n".join([str(x.relative_to(output_dir)) for x in files]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivatives/nilearn_glm/dataset_description.json
    derivatives/nilearn_glm/sub-10159
    derivatives/nilearn_glm/sub-10159/report.html
    derivatives/nilearn_glm/sub-10159/statmap.json
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_clusters.json
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_clusters.tsv
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_design.png
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_stat-effect_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_stat-p_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_stat-t_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_stat-variance_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_contrast-stopsuccessMinusGo_stat-z_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_corrdesign.png
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_design.json
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_design.png
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_design.tsv
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_mask.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_stat-errorts_statmap.nii.gz
    derivatives/nilearn_glm/sub-10159/sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_stat-rsquared_statmap.nii.gz




.. GENERATED FROM PYTHON SOURCE LINES 225-231

Simple statistical report of thresholded contrast
-------------------------------------------------
We display the :term:`contrast` plot and table with cluster information.

Here we will the image directly from the results saved to disk.


.. GENERATED FROM PYTHON SOURCE LINES 231-255

.. code-block:: Python

    from nilearn.plotting import plot_contrast_matrix

    plot_contrast_matrix("StopSuccess - Go", design_matrix)

    z_map = (
        output_dir
        / "derivatives"
        / "nilearn_glm"
        / "sub-10159"
        / (
            "sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_"
            "contrast-stopsuccessMinusGo_stat-z_statmap.nii.gz"
        )
    )

    plot_glass_brain(
        z_map,
        threshold=stat_threshold,
        plot_abs=False,
        display_mode="z",
        figure=plt.figure(figsize=(4, 4)),
    )
    show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_005.png
         :alt: plot bids features
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_006.png
         :alt: plot bids features
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_bids_features_006.png
         :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 256-282

The saved results include a table of activated clusters.

.. note::

    This table can also be generated by using the function
    :func:`nilearn.reporting.get_clusters_table`.

    .. code-block:: python

       from nilearn.reporting import get_clusters_table

       table = get_clusters_table(
           z_map,
           stat_threshold=norm.isf(0.001),
           cluster_threshold=10
       )

.. seealso::

    The restults saved to disk and the output of get_clusters_table
    do not contain the anatomical location of the clusters.
    To get the names of the location of the clusters
    according to one or several atlases,
    we recommend using
    the `atlasreader package <https://github.com/miykael/atlasreader>`_.


.. GENERATED FROM PYTHON SOURCE LINES 282-299

.. code-block:: Python

    import pandas as pd

    table_file = (
        output_dir
        / "derivatives"
        / "nilearn_glm"
        / "sub-10159"
        / (
            "sub-10159_task-stopsignal_space-MNI152NLin2009cAsym_"
            "contrast-stopsuccessMinusGo_clusters.tsv"
        )
    )

    table = pd.read_csv(table_file, sep="\t")

    table






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Cluster ID</th>
          <th>X</th>
          <th>Y</th>
          <th>Z</th>
          <th>Peak Stat</th>
          <th>Cluster Size (mm3)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>1</td>
          <td>15.0</td>
          <td>-93.0</td>
          <td>-18.0</td>
          <td>122.516815</td>
          <td>27900.0</td>
        </tr>
        <tr>
          <th>1</th>
          <td>1a</td>
          <td>6.0</td>
          <td>-87.0</td>
          <td>-18.0</td>
          <td>70.342086</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>2</th>
          <td>1b</td>
          <td>30.0</td>
          <td>-87.0</td>
          <td>-22.0</td>
          <td>69.308056</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>3</th>
          <td>1c</td>
          <td>-3.0</td>
          <td>-87.0</td>
          <td>-18.0</td>
          <td>61.796424</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>4</th>
          <td>2</td>
          <td>9.0</td>
          <td>69.0</td>
          <td>-6.0</td>
          <td>92.734506</td>
          <td>64908.0</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>78</th>
          <td>34</td>
          <td>-3.0</td>
          <td>42.0</td>
          <td>26.0</td>
          <td>6.714833</td>
          <td>468.0</td>
        </tr>
        <tr>
          <th>79</th>
          <td>34a</td>
          <td>-6.0</td>
          <td>42.0</td>
          <td>18.0</td>
          <td>4.408391</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>80</th>
          <td>35</td>
          <td>-3.0</td>
          <td>12.0</td>
          <td>54.0</td>
          <td>5.883419</td>
          <td>396.0</td>
        </tr>
        <tr>
          <th>81</th>
          <td>35a</td>
          <td>3.0</td>
          <td>18.0</td>
          <td>54.0</td>
          <td>5.321565</td>
          <td>NaN</td>
        </tr>
        <tr>
          <th>82</th>
          <td>36</td>
          <td>-3.0</td>
          <td>9.0</td>
          <td>6.0</td>
          <td>5.772909</td>
          <td>468.0</td>
        </tr>
      </tbody>
    </table>
    <p>83 rows × 6 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 300-311

We can get a latex table from a Pandas Dataframe
for display and publication purposes.

.. note::

  This requires to have jinja2 installed:

    .. code-block:: bash

       pip install jinja2


.. GENERATED FROM PYTHON SOURCE LINES 311-313

.. code-block:: Python

    print(table.to_latex())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    \begin{tabular}{llrrrrr}
    \toprule
     & Cluster ID & X & Y & Z & Peak Stat & Cluster Size (mm3) \\
    \midrule
    0 & 1 & 15.000000 & -93.000000 & -18.000000 & 122.516815 & 27900.000000 \\
    1 & 1a & 6.000000 & -87.000000 & -18.000000 & 70.342086 & NaN \\
    2 & 1b & 30.000000 & -87.000000 & -22.000000 & 69.308056 & NaN \\
    3 & 1c & -3.000000 & -87.000000 & -18.000000 & 61.796424 & NaN \\
    4 & 2 & 9.000000 & 69.000000 & -6.000000 & 92.734506 & 64908.000000 \\
    5 & 2a & 12.000000 & 57.000000 & -22.000000 & 62.999189 & NaN \\
    6 & 2b & -15.000000 & 15.000000 & 74.000000 & 54.897183 & NaN \\
    7 & 2c & 15.000000 & 63.000000 & -10.000000 & 53.770578 & NaN \\
    8 & 3 & 0.000000 & -3.000000 & -18.000000 & 80.143288 & 22788.000000 \\
    9 & 3a & 27.000000 & 9.000000 & -26.000000 & 46.564125 & NaN \\
    10 & 3b & 0.000000 & -15.000000 & -42.000000 & 40.157605 & NaN \\
    11 & 3c & 3.000000 & -18.000000 & -50.000000 & 34.929461 & NaN \\
    12 & 4 & -54.000000 & 42.000000 & 14.000000 & 36.477846 & 4896.000000 \\
    13 & 4a & -45.000000 & 51.000000 & 22.000000 & 19.736986 & NaN \\
    14 & 4b & -33.000000 & 51.000000 & 38.000000 & 17.196909 & NaN \\
    15 & 4c & -27.000000 & 60.000000 & 34.000000 & 16.980278 & NaN \\
    16 & 5 & -3.000000 & 21.000000 & -10.000000 & 26.752451 & 2736.000000 \\
    17 & 5a & 0.000000 & 30.000000 & -6.000000 & 17.541399 & NaN \\
    18 & 5b & -12.000000 & 18.000000 & -14.000000 & 9.857585 & NaN \\
    19 & 5c & 3.000000 & 18.000000 & 2.000000 & 5.270521 & NaN \\
    20 & 6 & -63.000000 & 9.000000 & 2.000000 & 24.590403 & 6552.000000 \\
    21 & 6a & -57.000000 & 18.000000 & -6.000000 & 22.502379 & NaN \\
    22 & 6b & -63.000000 & 27.000000 & 10.000000 & 21.651576 & NaN \\
    23 & 6c & -63.000000 & -3.000000 & 6.000000 & 19.091516 & NaN \\
    24 & 7 & 39.000000 & 6.000000 & 66.000000 & 24.417132 & 3096.000000 \\
    25 & 7a & 51.000000 & 21.000000 & 50.000000 & 14.872285 & NaN \\
    26 & 7b & 45.000000 & 6.000000 & 62.000000 & 14.719380 & NaN \\
    27 & 7c & 51.000000 & 9.000000 & 54.000000 & 11.575969 & NaN \\
    28 & 8 & 12.000000 & 36.000000 & -26.000000 & 20.716392 & 396.000000 \\
    29 & 8a & 18.000000 & 30.000000 & -26.000000 & 14.789520 & NaN \\
    30 & 9 & -45.000000 & 6.000000 & 62.000000 & 20.216765 & 576.000000 \\
    31 & 9a & -51.000000 & 9.000000 & 54.000000 & 17.445403 & NaN \\
    32 & 10 & -39.000000 & -60.000000 & 62.000000 & 18.839163 & 1224.000000 \\
    33 & 10a & -48.000000 & -48.000000 & 62.000000 & 17.127426 & NaN \\
    34 & 10b & -30.000000 & -63.000000 & 66.000000 & 7.893538 & NaN \\
    35 & 10c & -57.000000 & -33.000000 & 58.000000 & 7.533043 & NaN \\
    36 & 11 & -3.000000 & -87.000000 & 46.000000 & 18.639460 & 3132.000000 \\
    37 & 11a & -39.000000 & -75.000000 & 50.000000 & 15.066584 & NaN \\
    38 & 11b & -18.000000 & -81.000000 & 54.000000 & 14.665675 & NaN \\
    39 & 11c & -27.000000 & -81.000000 & 50.000000 & 10.536638 & NaN \\
    40 & 12 & 12.000000 & -105.000000 & -2.000000 & 18.465759 & 864.000000 \\
    41 & 12a & 12.000000 & -93.000000 & -2.000000 & 6.515125 & NaN \\
    42 & 12b & 21.000000 & -105.000000 & -2.000000 & 5.758570 & NaN \\
    43 & 13 & 21.000000 & -93.000000 & 34.000000 & 17.517443 & 792.000000 \\
    44 & 14 & 63.000000 & 12.000000 & 10.000000 & 16.758346 & 828.000000 \\
    45 & 14a & 63.000000 & 21.000000 & 14.000000 & 8.923981 & NaN \\
    46 & 14b & 60.000000 & 12.000000 & -2.000000 & 7.959472 & NaN \\
    47 & 15 & -18.000000 & -60.000000 & -58.000000 & 16.093286 & 504.000000 \\
    48 & 15a & -18.000000 & -72.000000 & -58.000000 & 6.096305 & NaN \\
    49 & 16 & -9.000000 & 33.000000 & -26.000000 & 15.756994 & 900.000000 \\
    50 & 16a & -9.000000 & 42.000000 & -22.000000 & 11.028668 & NaN \\
    51 & 17 & -15.000000 & -108.000000 & -2.000000 & 14.652072 & 936.000000 \\
    52 & 18 & -9.000000 & -48.000000 & 78.000000 & 13.593056 & 720.000000 \\
    53 & 18a & -6.000000 & -57.000000 & 74.000000 & 7.156604 & NaN \\
    54 & 19 & 18.000000 & 18.000000 & 70.000000 & 13.349184 & 468.000000 \\
    55 & 20 & -27.000000 & -93.000000 & 30.000000 & 13.055884 & 540.000000 \\
    56 & 21 & 57.000000 & -51.000000 & -14.000000 & 12.623032 & 612.000000 \\
    57 & 22 & 27.000000 & -78.000000 & 50.000000 & 12.179269 & 612.000000 \\
    58 & 22a & 36.000000 & -75.000000 & 50.000000 & 6.740345 & NaN \\
    59 & 23 & -30.000000 & -102.000000 & 6.000000 & 11.286718 & 360.000000 \\
    60 & 24 & 63.000000 & -12.000000 & 10.000000 & 11.197878 & 648.000000 \\
    61 & 24a & 54.000000 & -15.000000 & 10.000000 & 5.924582 & NaN \\
    62 & 25 & -69.000000 & -30.000000 & -10.000000 & 11.079066 & 468.000000 \\
    63 & 25a & -69.000000 & -30.000000 & 2.000000 & 6.227700 & NaN \\
    64 & 26 & 63.000000 & 0.000000 & 26.000000 & 9.853608 & 576.000000 \\
    65 & 26a & 66.000000 & -3.000000 & 34.000000 & 7.613071 & NaN \\
    66 & 27 & -66.000000 & 9.000000 & 18.000000 & 9.502274 & 1296.000000 \\
    67 & 27a & -66.000000 & 3.000000 & 26.000000 & 7.890005 & NaN \\
    68 & 27b & -60.000000 & -3.000000 & 26.000000 & 7.157832 & NaN \\
    69 & 27c & -66.000000 & -6.000000 & 14.000000 & 5.843833 & NaN \\
    70 & 28 & 30.000000 & -99.000000 & 10.000000 & 8.887969 & 360.000000 \\
    71 & 28a & 33.000000 & -99.000000 & 2.000000 & 7.337295 & NaN \\
    72 & 29 & -66.000000 & -18.000000 & 42.000000 & 8.635179 & 468.000000 \\
    73 & 30 & -48.000000 & 21.000000 & 50.000000 & 8.539273 & 396.000000 \\
    74 & 31 & -36.000000 & -36.000000 & -22.000000 & 8.397082 & 396.000000 \\
    75 & 31a & -27.000000 & -27.000000 & -26.000000 & 4.379355 & NaN \\
    76 & 32 & -45.000000 & 6.000000 & 42.000000 & 7.232974 & 396.000000 \\
    77 & 33 & -15.000000 & -57.000000 & -10.000000 & 7.195093 & 396.000000 \\
    78 & 34 & -3.000000 & 42.000000 & 26.000000 & 6.714833 & 468.000000 \\
    79 & 34a & -6.000000 & 42.000000 & 18.000000 & 4.408391 & NaN \\
    80 & 35 & -3.000000 & 12.000000 & 54.000000 & 5.883419 & 396.000000 \\
    81 & 35a & 3.000000 & 18.000000 & 54.000000 & 5.321565 & NaN \\
    82 & 36 & -3.000000 & 9.000000 & 6.000000 & 5.772909 & 468.000000 \\
    \bottomrule
    \end{tabular}





.. GENERATED FROM PYTHON SOURCE LINES 314-325

You can also print the output to markdown,
if you have the `tabulate` dependencies installed.

.. code-block:: bash

   pip install tabulate

.. code-block:: python

   table.to_markdown()



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 39.984 seconds)

**Estimated memory usage:**  1005 MB


.. _sphx_glr_download_auto_examples_04_glm_first_level_plot_bids_features.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/04_glm_first_level/plot_bids_features.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bids_features.ipynb <plot_bids_features.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bids_features.py <plot_bids_features.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_bids_features.zip <plot_bids_features.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
