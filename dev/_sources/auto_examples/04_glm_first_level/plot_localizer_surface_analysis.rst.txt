
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/04_glm_first_level/plot_localizer_surface_analysis.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py:


Example of surface-based first-level analysis
=============================================

A full step-by-step example of fitting a :term:`GLM`
to experimental data sampled on the cortical surface
and visualizing the results.

More specifically:

1. A sequence of :term:`fMRI` volumes is loaded.
2. :term:`fMRI` data are projected onto a reference cortical surface
   (the FreeSurfer template, fsaverage).
3. A :term:`GLM` is applied to the dataset
   (effect/covariance, then contrast estimation).

The result of the analysis are statistical maps that are defined
on the brain mesh.
We display them using Nilearn capabilities.

The projection of :term:`fMRI` data onto a given brain :term:`mesh` requires
that both are initially defined in the same space.

* The functional data should be coregistered to the anatomy
  from which the mesh was obtained.

* Another possibility, used here, is to project
  the normalized :term:`fMRI` data to an :term:`MNI`-coregistered mesh,
  such as fsaverage.

The advantage of this second approach is that it makes it easy to run
second-level analyses on the surface.
On the other hand, it is obviously less accurate
than using a subject-tailored mesh.

.. GENERATED FROM PYTHON SOURCE LINES 38-42

Prepare data and analysis parameters
------------------------------------

Prepare the timing parameters.

.. GENERATED FROM PYTHON SOURCE LINES 42-45

.. code-block:: Python

    t_r = 2.4
    slice_time_ref = 0.5








.. GENERATED FROM PYTHON SOURCE LINES 46-47

Fetch the data.

.. GENERATED FROM PYTHON SOURCE LINES 47-51

.. code-block:: Python

    from nilearn.datasets import fetch_localizer_first_level

    data = fetch_localizer_first_level()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [get_dataset_dir] Dataset found in 
    /home/runner/nilearn_data/localizer_first_level




.. GENERATED FROM PYTHON SOURCE LINES 52-66

Project the :term:`fMRI` image to the surface
---------------------------------------------

For this we need to get a :term:`mesh`
representing the geometry of the surface.
We could use an individual :term:`mesh`,
but we first resort to a standard :term:`mesh`,
the so-called fsaverage5 template from the FreeSurfer software.

We use the :class:`~nilearn.surface.SurfaceImage`
to create an surface object instance
that contains both the mesh
(here we use the one from the fsaverage5 templates)
and the BOLD data that we project on the surface.

.. GENERATED FROM PYTHON SOURCE LINES 66-76

.. code-block:: Python


    from nilearn.datasets import load_fsaverage
    from nilearn.surface import SurfaceImage

    fsaverage5 = load_fsaverage()
    surface_image = SurfaceImage.from_volume(
        mesh=fsaverage5["pial"],
        volume_img=data.epi_img,
    )








.. GENERATED FROM PYTHON SOURCE LINES 77-87

Perform first level analysis
----------------------------

We can now simply run a GLM by directly passing
our :class:`~nilearn.surface.SurfaceImage` instance
as input to FirstLevelModel.fit

Here we use an :term:`HRF` model
containing the Glover model and its time derivative
The drift model is implicitly a cosine basis with a period cutoff at 128s.

.. GENERATED FROM PYTHON SOURCE LINES 87-95

.. code-block:: Python

    from nilearn.glm.first_level import FirstLevelModel

    glm = FirstLevelModel(
        t_r=t_r,
        slice_time_ref=slice_time_ref,
        hrf_model="glover + derivative",
    ).fit(run_imgs=surface_image, events=data.events)








.. GENERATED FROM PYTHON SOURCE LINES 96-103

Estimate contrasts
------------------

Specify the contrasts.

For practical purpose, we first generate an identity matrix whose size is
the number of columns of the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 103-108

.. code-block:: Python

    import numpy as np

    design_matrix = glm.design_matrices_[0]
    contrast_matrix = np.eye(design_matrix.shape[1])








.. GENERATED FROM PYTHON SOURCE LINES 109-110

At first, we create basic contrasts.

.. GENERATED FROM PYTHON SOURCE LINES 110-115

.. code-block:: Python

    basic_contrasts = {
        column: contrast_matrix[i]
        for i, column in enumerate(design_matrix.columns)
    }








.. GENERATED FROM PYTHON SOURCE LINES 116-118

Next, we add some intermediate contrasts and
one :term:`contrast` adding all conditions with some auditory parts.

.. GENERATED FROM PYTHON SOURCE LINES 118-144

.. code-block:: Python

    basic_contrasts["audio"] = (
        basic_contrasts["audio_left_hand_button_press"]
        + basic_contrasts["audio_right_hand_button_press"]
        + basic_contrasts["audio_computation"]
        + basic_contrasts["sentence_listening"]
    )

    # one contrast adding all conditions involving instructions reading
    basic_contrasts["visual"] = (
        basic_contrasts["visual_left_hand_button_press"]
        + basic_contrasts["visual_right_hand_button_press"]
        + basic_contrasts["visual_computation"]
        + basic_contrasts["sentence_reading"]
    )

    # one contrast adding all conditions involving computation
    basic_contrasts["computation"] = (
        basic_contrasts["visual_computation"]
        + basic_contrasts["audio_computation"]
    )

    # one contrast adding all conditions involving sentences
    basic_contrasts["sentences"] = (
        basic_contrasts["sentence_listening"] + basic_contrasts["sentence_reading"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 145-157

Finally, we create a dictionary of more relevant contrasts

* ``'left - right button press'``: probes motor activity
  in left versus right button presses.
* ``'audio - visual'``: probes the difference of activity between listening
  to some content or reading the same type of content
  (instructions, stories).
* ``'computation - sentences'``: looks at the activity
  when performing a mental computation task  versus simply reading sentences.

Of course, we could define other contrasts,
but we keep only 3 for simplicity.

.. GENERATED FROM PYTHON SOURCE LINES 157-171

.. code-block:: Python


    contrasts = {
        "(left - right) button press": (
            basic_contrasts["audio_left_hand_button_press"]
            - basic_contrasts["audio_right_hand_button_press"]
            + basic_contrasts["visual_left_hand_button_press"]
            - basic_contrasts["visual_right_hand_button_press"]
        ),
        "audio - visual": basic_contrasts["audio"] - basic_contrasts["visual"],
        "computation - sentences": (
            basic_contrasts["computation"] - basic_contrasts["sentences"]
        ),
    }








.. GENERATED FROM PYTHON SOURCE LINES 172-173

Let's estimate the contrasts by iterating over them.

.. GENERATED FROM PYTHON SOURCE LINES 173-197

.. code-block:: Python

    from nilearn.datasets import load_fsaverage_data
    from nilearn.plotting import plot_surf_stat_map, show

    fsaverage_data = load_fsaverage_data(data_type="sulcal")

    for contrast_id, contrast_val in contrasts.items():
        # compute contrast-related statistics
        z_score = glm.compute_contrast(contrast_val, stat_type="t")

        # we plot it on the surface, on the inflated fsaverage mesh,
        # together with a suitable background to give an impression
        # of the cortex folding.
        for hemi in ["left", "right"]:
            plot_surf_stat_map(
                surf_mesh=fsaverage5["inflated"],
                stat_map=z_score,
                hemi=hemi,
                title=contrast_id,
                colorbar=True,
                threshold=3.0,
                bg_map=fsaverage_data,
            )

    show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_001.png
         :alt: (left - right) button press
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_002.png
         :alt: (left - right) button press
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_003.png
         :alt: audio - visual
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_004.png
         :alt: audio - visual
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_005.png
         :alt: computation - sentences
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_006.png
         :alt: computation - sentences
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_006.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.590 seconds)

**Estimated memory usage:**  207 MB


.. _sphx_glr_download_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/04_glm_first_level/plot_localizer_surface_analysis.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_localizer_surface_analysis.ipynb <plot_localizer_surface_analysis.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_localizer_surface_analysis.py <plot_localizer_surface_analysis.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_localizer_surface_analysis.zip <plot_localizer_surface_analysis.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
