
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/03_connectivity/plot_data_driven_parcellations.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_03_connectivity_plot_data_driven_parcellations.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_03_connectivity_plot_data_driven_parcellations.py:


Clustering methods to learn a brain parcellation from fMRI
==========================================================

We use spatially-constrained Ward-clustering, KMeans, Hierarchical KMeans
and Recursive Neighbor Agglomeration (ReNA) to create a set of parcels.

In a high dimensional regime, these methods can be interesting
to create a 'compressed' representation of the data, replacing the data
in the :term:`fMRI` images by mean signals on the parcellation, which can
subsequently be used for statistical analysis or machine learning.

Also, these methods can be used to learn functional connectomes
and subsequently for classification tasks or to analyze data at a local
level.

.. seealso::

    Which clustering method to use, an empirical comparison can be found
    in :footcite:t:`Thirion2014`.

    This :term:`parcellation` may be useful in a supervised learning,
    see for instance :footcite:t:`Michel2011b`.

    The big picture discussion corresponding to this example can be found
    in the documentation section :ref:`parcellating_brain`.

.. GENERATED FROM PYTHON SOURCE LINES 28-33

.. code-block:: Python


    from nilearn._utils.helpers import check_matplotlib

    check_matplotlib()








.. GENERATED FROM PYTHON SOURCE LINES 34-38

Download a brain development fMRI dataset and turn it to a data matrix
----------------------------------------------------------------------

We download one subject of the movie watching dataset from Internet

.. GENERATED FROM PYTHON SOURCE LINES 38-54

.. code-block:: Python


    import time

    import numpy as np
    from matplotlib import patches, ticker

    from nilearn import datasets, plotting
    from nilearn.image import get_data, index_img, mean_img
    from nilearn.regions import Parcellations

    dataset = datasets.fetch_development_fmri(n_subjects=1)

    # print basic information on the dataset
    print(f"First subject functional nifti image (4D) is at: {dataset.func[0]}")






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_development_fmri] Dataset found in 
    /home/runner/nilearn_data/development_fmri
    [fetch_development_fmri] Dataset found in 
    /home/runner/nilearn_data/development_fmri/development_fmri
    [fetch_development_fmri] Dataset found in 
    /home/runner/nilearn_data/development_fmri/development_fmri
    First subject functional nifti image (4D) is at: /home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz




.. GENERATED FROM PYTHON SOURCE LINES 55-60

Brain parcellations with Ward Clustering
----------------------------------------

Transforming list of images to data matrix and build brain parcellations,
all can be done at once using `Parcellations` object.

.. GENERATED FROM PYTHON SOURCE LINES 60-101

.. code-block:: Python



    # Computing ward for the first time, will be long... This can be seen by
    # measuring using time
    start = time.time()

    # Agglomerative Clustering: ward

    # We build parameters of our own for this object. Parameters related to
    # masking, caching and defining number of clusters and specific parcellations
    # method.
    ward = Parcellations(
        method="ward",
        n_parcels=1000,
        standardize=False,
        smoothing_fwhm=2.0,
        memory="nilearn_cache",
        memory_level=1,
        verbose=1,
    )
    # Call fit on functional dataset: single subject (less samples).
    ward.fit(dataset.func)
    print(f"Ward agglomeration 1000 clusters: {time.time() - start:.2f}s")

    # We compute now ward clustering with 2000 clusters and compare
    # time with 1000 clusters. To see the benefits of caching for second time.

    # We initialize class again with n_parcels=2000 this time.
    start = time.time()
    ward = Parcellations(
        method="ward",
        n_parcels=2000,
        standardize=False,
        smoothing_fwhm=2.0,
        memory="nilearn_cache",
        memory_level=1,
        verbose=1,
    )
    ward.fit(dataset.func)
    print(f"Ward agglomeration 2000 clusters: {time.time() - start:.2f}s")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.fit] Loading data from 
    ['/home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-
    pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']
    [Parcellations.fit] Computing mask
    [Parcellations.fit] Resampling mask
    [Parcellations.fit] Finished fit
    [Parcellations.fit] Loading data
    [Parcellations.fit] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa969ebc820>
    [Parcellations.fit] Smoothing images
    [Parcellations.fit] Extracting region signals
    [Parcellations.fit] Cleaning extracted signals
    [Parcellations.fit] Computing image from signals
    [Parcellations.fit] computing ward
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.parcellations._estimator_fit...
    _estimator_fit(array([[-0.002154, ...,  0.002661],
           ...,
           [ 0.007053, ..., -0.007877]]), 
    AgglomerativeClustering(connectivity=<24256x24256 sparse matrix of type '<class 'numpy.int64'>'
            with 162682 stored elements in COOrdinate format>,
                            memory=Memory(location=nilearn_cache/joblib),
                            n_clusters=1000))
    ________________________________________________________________________________
    [Memory] Calling sklearn.cluster._agglomerative.ward_tree...
    ward_tree(array([[-0.002154, ...,  0.007053],
           ...,
           [ 0.002661, ..., -0.007877]]), connectivity=<24256x24256 sparse matrix of type '<class 'numpy.int64'>'
            with 162682 stored elements in COOrdinate format>, n_clusters=1000, return_distance=False)
    ________________________________________________________ward_tree - 1.8s, 0.0min
    ____________________________________________________estimator_fit - 2.0s, 0.0min
    [Parcellations.fit] Computing image from signals
    Ward agglomeration 1000 clusters: 6.39s
    [Parcellations.fit] Loading data from 
    ['/home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-
    pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']
    [Parcellations.fit] Computing mask
    [Parcellations.fit] Resampling mask
    [Parcellations.fit] Finished fit
    [Parcellations.fit] Loading data
    [Parcellations.fit] Computing image from signals
    [Parcellations.fit] computing ward
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.parcellations._estimator_fit...
    _estimator_fit(array([[-0.002154, ...,  0.002661],
           ...,
           [ 0.007053, ..., -0.007877]]), 
    AgglomerativeClustering(connectivity=<24256x24256 sparse matrix of type '<class 'numpy.int64'>'
            with 162682 stored elements in COOrdinate format>,
                            memory=Memory(location=nilearn_cache/joblib),
                            n_clusters=2000))
    ________________________________________________________________________________
    [Memory] Calling sklearn.cluster._agglomerative.ward_tree...
    ward_tree(array([[-0.002154, ...,  0.007053],
           ...,
           [ 0.002661, ..., -0.007877]]), connectivity=<24256x24256 sparse matrix of type '<class 'numpy.int64'>'
            with 162682 stored elements in COOrdinate format>, n_clusters=2000, return_distance=False)
    ________________________________________________________ward_tree - 1.7s, 0.0min
    ____________________________________________________estimator_fit - 1.9s, 0.0min
    [Parcellations.fit] Computing image from signals
    Ward agglomeration 2000 clusters: 4.45s




.. GENERATED FROM PYTHON SOURCE LINES 102-107

Visualize: Brain parcellations (Ward)
.....................................

First, we display the parcellations of the brain image stored in attribute
`labels_img_`

.. GENERATED FROM PYTHON SOURCE LINES 107-128

.. code-block:: Python

    ward_labels_img = ward.labels_img_

    # Now, ward_labels_img are Nifti1Image object, it can be saved to file
    # with the following code:
    from pathlib import Path

    output_dir = Path.cwd() / "results" / "plot_data_driven_parcellations"
    output_dir.mkdir(exist_ok=True, parents=True)
    print(f"Output will be saved to: {output_dir}")
    ward_labels_img.to_filename(output_dir / "ward_parcellation.nii.gz")


    first_plot = plotting.plot_roi(
        ward_labels_img, title="Ward parcellation", display_mode="xz"
    )

    plotting.show()

    # Grab cut coordinates from this plot to use as a common for all plots
    cut_coords = first_plot.cut_coords




.. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_001.png
   :alt: plot data driven parcellations
   :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Output will be saved to: /home/runner/work/nilearn/nilearn/examples/03_connectivity/results/plot_data_driven_parcellations




.. GENERATED FROM PYTHON SOURCE LINES 129-135

Compressed representation of Ward clustering
............................................

Second, we illustrate the effect that the clustering has on the signal.
We show the original data, and the approximation provided by the
clustering by averaging the signal on each parcel.

.. GENERATED FROM PYTHON SOURCE LINES 135-181

.. code-block:: Python


    # Grab number of voxels from attribute mask image (mask_img_).
    original_voxels = np.sum(get_data(ward.mask_img_))

    # Compute mean over time on the functional image to use the mean
    # image for compressed representation comparisons
    mean_func_img = mean_img(dataset.func[0])

    # Compute common vmin and vmax
    vmin = np.min(get_data(mean_func_img))
    vmax = np.max(get_data(mean_func_img))

    plotting.plot_epi(
        mean_func_img,
        cut_coords=cut_coords,
        title=f"Original ({int(original_voxels)} voxels)",
        vmax=vmax,
        vmin=vmin,
        display_mode="xz",
    )

    # A reduced dataset can be created by taking the parcel-level average:
    # Note that Parcellation objects with any method have the opportunity to
    # use a `transform` call that modifies input features. Here it reduces their
    # dimension. Note that we `fit` before calling a `transform` so that average
    # signals can be created on the brain parcellations with fit call.
    fmri_reduced = ward.transform(dataset.func)

    # Display the corresponding data compressed
    # using the parcellation using parcels=2000.
    fmri_compressed = ward.inverse_transform(fmri_reduced)

    plotting.plot_epi(
        index_img(fmri_compressed, 0),
        cut_coords=cut_coords,
        title="Ward compressed representation (2000 parcels)",
        vmin=vmin,
        vmax=vmax,
        display_mode="xz",
    )

    plotting.show()

    # As you can see below, this approximation is almost good, although there
    # are only 2000 parcels, instead of the original 60000 voxels




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_002.png
         :alt: plot data driven parcellations
         :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_003.png
         :alt: plot data driven parcellations
         :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_003.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.wrapped] Loading regions from <nibabel.nifti1.Nifti1Image object 
    at 0x7fa9ad27add0>
    [Parcellations.wrapped] Loading mask from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9ad27b640>
    [Parcellations.wrapped] Finished fit
    ________________________________________________________________________________
    [Memory] Calling nilearn.maskers.base_masker.filter_and_extract...
    filter_and_extract(<nibabel.nifti1.Nifti1Image object at 0x7fa9ad2796f0>, <nilearn.maskers.nifti_labels_masker._ExtractionFunctor object at 0x7fa978897340>, 
    { 'background_label': 0,
      'clean_args': None,
      'clean_kwargs': {},
      'cmap': 'CMRmap_r',
      'detrend': False,
      'dtype': None,
      'high_pass': None,
      'high_variance_confounds': False,
      'keep_masked_labels': False,
      'labels': None,
      'labels_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9ad27add0>,
      'low_pass': None,
      'lut': None,
      'mask_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9ad27b640>,
      'reports': True,
      'smoothing_fwhm': 2.0,
      'standardize': False,
      'standardize_confounds': True,
      'strategy': 'mean',
      't_r': None,
      'target_affine': None,
      'target_shape': None}, confounds=None, sample_mask=None, dtype=None, memory=Memory(location=nilearn_cache/joblib), memory_level=1, verbose=1, sklearn_output_config=None)
    [Parcellations.wrapped] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9ad2796f0>
    [Parcellations.wrapped] Smoothing images
    [Parcellations.wrapped] Extracting region signals
    [Parcellations.wrapped] Cleaning extracted signals
    _______________________________________________filter_and_extract - 1.3s, 0.0min




.. GENERATED FROM PYTHON SOURCE LINES 182-189

Brain parcellations with KMeans Clustering
------------------------------------------

We use the same approach as with building parcellations using Ward
clustering. But, in the range of a small number of clusters,
it is most likely that we want to use standardization. Indeed with
standardization and smoothing, the clusters will form as regions.

.. GENERATED FROM PYTHON SOURCE LINES 189-208

.. code-block:: Python


    # class/functions can be used here as they are already imported above.

    # This object uses method='kmeans' for KMeans clustering with 10mm smoothing
    # and standardization ON
    start = time.time()
    kmeans = Parcellations(
        method="kmeans",
        n_parcels=50,
        standardize="zscore_sample",
        smoothing_fwhm=10.0,
        memory="nilearn_cache",
        memory_level=1,
        verbose=1,
    )
    # Call fit on functional dataset: single subject (less samples)
    kmeans.fit(dataset.func)
    print(f"KMeans clusters: {time.time() - start:.2f}s")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.fit] Loading data from 
    ['/home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-
    pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']
    [Parcellations.fit] Computing mask
    [Parcellations.fit] Resampling mask
    [Parcellations.fit] Finished fit
    [Parcellations.fit] Loading data
    [Parcellations.fit] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9ad279900>
    [Parcellations.fit] Smoothing images
    [Parcellations.fit] Extracting region signals
    [Parcellations.fit] Cleaning extracted signals
    [Parcellations.fit] Computing image from signals
    [Parcellations.fit] computing kmeans
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.parcellations._estimator_fit...
    _estimator_fit(array([[ 0.004934, ..., -0.015396],
           ...,
           [-0.003247, ..., -0.001756]]), 
    MiniBatchKMeans(n_clusters=50, n_init=3, random_state=0))
    ____________________________________________________estimator_fit - 0.4s, 0.0min
    [Parcellations.fit] Computing image from signals
    KMeans clusters: 4.77s




.. GENERATED FROM PYTHON SOURCE LINES 209-213

Visualize: Brain parcellations (KMeans)
.......................................

Grab parcellations of brain image stored in attribute `labels_img_`

.. GENERATED FROM PYTHON SOURCE LINES 213-228

.. code-block:: Python

    kmeans_labels_img = kmeans.labels_img_

    display = plotting.plot_roi(
        kmeans_labels_img,
        mean_func_img,
        title="KMeans parcellation",
        display_mode="xz",
    )

    plotting.show()

    # kmeans_labels_img is a Nifti1Image object, it can be saved to file with
    # the following code:
    kmeans_labels_img.to_filename(output_dir / "kmeans_parcellation.nii.gz")




.. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_004.png
   :alt: plot data driven parcellations
   :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 229-245

Brain parcellations with Hierarchical KMeans Clustering
-------------------------------------------------------

As the number of images from which we try to cluster grows,
voxels display more and more specific activity patterns causing
KMeans clusters to be very unbalanced with a few big clusters and
many voxels left as singletons. Hierarchical Kmeans algorithm is
tailored to enforce more balanced clusterings. To do this,
Hierarchical Kmeans does a first Kmeans clustering in square root of
n_parcels. In a second step, it clusters voxels inside each
of these parcels in m pieces with m adapted to the size of
the cluster in order to have n balanced clusters in the end.

This object uses method='hierarchical_kmeans' for Hierarchical KMeans
clustering and 10mm smoothing and standardization to compare
with the previous method.

.. GENERATED FROM PYTHON SOURCE LINES 245-258

.. code-block:: Python

    start = time.time()
    hkmeans = Parcellations(
        method="hierarchical_kmeans",
        n_parcels=50,
        standardize="zscore_sample",
        smoothing_fwhm=10,
        memory="nilearn_cache",
        memory_level=1,
        verbose=1,
    )
    # Call fit on functional dataset: single subject (less samples)
    hkmeans.fit(dataset.func)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.fit] Loading data from 
    ['/home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-
    pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']
    [Parcellations.fit] Computing mask
    [Parcellations.fit] Resampling mask
    [Parcellations.fit] Finished fit
    [Parcellations.fit] Loading data
    [Parcellations.fit] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9acb670d0>
    [Parcellations.fit] Smoothing images
    [Parcellations.fit] Extracting region signals
    [Parcellations.fit] Cleaning extracted signals
    [Parcellations.fit] Computing image from signals
    [Parcellations.fit] computing hierarchical_kmeans
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.parcellations._estimator_fit...
    _estimator_fit(array([[ 0.004934, ..., -0.015396],
           ...,
           [-0.003247, ..., -0.001756]]), 
    HierarchicalKMeans(n_clusters=50), 'hierarchical_kmeans')
    ____________________________________________________estimator_fit - 1.1s, 0.0min
    [Parcellations.fit] Computing image from signals


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-4 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-4 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-4 pre {
      padding: 0;
    }

    #sk-container-id-4 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-4 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-4 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-4 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-4 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-4 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-4 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-4 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-4 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-4 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-4 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-4 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-4 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-4 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "▸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-4 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-4 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-4 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-4 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "▾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-4 div.sk-label label.sk-toggleable__label,
    #sk-container-id-4 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-4 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-4 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-4 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-4 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-4 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-4 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-4 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-4 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-4 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-4 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Parcellations(memory=&#x27;nilearn_cache&#x27;, memory_level=1,
                  method=&#x27;hierarchical_kmeans&#x27;, smoothing_fwhm=10,
                  standardize=&#x27;zscore_sample&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;Parcellations<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>Parcellations(memory=&#x27;nilearn_cache&#x27;, memory_level=1,
                  method=&#x27;hierarchical_kmeans&#x27;, smoothing_fwhm=10,
                  standardize=&#x27;zscore_sample&#x27;)</pre></div> </div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 259-263

Visualize: Brain parcellations (Hierarchical KMeans)
....................................................

Grab parcellations of brain image stored in attribute `labels_img_`

.. GENERATED FROM PYTHON SOURCE LINES 263-281

.. code-block:: Python

    hkmeans_labels_img = hkmeans.labels_img_

    plotting.plot_roi(
        hkmeans_labels_img,
        mean_func_img,
        title="Hierarchical KMeans parcellation",
        display_mode="xz",
        cut_coords=display.cut_coords,
    )

    plotting.show()

    # kmeans_labels_img is a :class:`nibabel.nifti1.Nifti1Image` object, it can be
    # saved to file with the following code:
    hkmeans_labels_img.to_filename(
        output_dir / "hierarchical_kmeans_parcellation.nii.gz"
    )




.. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_005.png
   :alt: plot data driven parcellations
   :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_005.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 282-291

Compare Hierarchical Kmeans clusters with those from Kmeans
...........................................................
To compare those, we'll first count how many voxels are contained in
each of the 50 clusters for both algorithms and compare those sizes
distribution. Hierarchical KMeans should give clusters closer to
average (600 here) than KMeans.

First count how many voxels have each label (except 0 which is the
background).

.. GENERATED FROM PYTHON SOURCE LINES 291-302

.. code-block:: Python


    _, kmeans_counts = np.unique(get_data(kmeans_labels_img), return_counts=True)

    _, hkmeans_counts = np.unique(get_data(hkmeans_labels_img), return_counts=True)

    voxel_ratio = np.round(np.sum(kmeans_counts[1:]) / 50)

    # If all voxels not in background were balanced between clusters ...

    print(f"... each cluster should contain {voxel_ratio} voxels")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ... each cluster should contain 485.0 voxels




.. GENERATED FROM PYTHON SOURCE LINES 303-306

Let's plot clusters sizes distributions for both algorithms

You can just skip the plotting code, the important part is the figure

.. GENERATED FROM PYTHON SOURCE LINES 306-336

.. code-block:: Python

    import matplotlib.pyplot as plt

    bins = np.concatenate(
        [
            np.linspace(0, 500, 11),
            np.linspace(600, 2000, 15),
            np.linspace(3000, 10000, 8),
        ]
    )
    fig, axes = plt.subplots(
        nrows=2, sharex=True, gridspec_kw={"height_ratios": [4, 1]}
    )
    plt.semilogx()
    axes[0].hist(kmeans_counts[1:], bins, color="blue")
    axes[1].hist(hkmeans_counts[1:], bins, color="green")
    axes[0].set_ylim(0, 16)
    axes[1].set_ylim(4, 0)
    axes[1].xaxis.set_major_formatter(ticker.ScalarFormatter())
    axes[1].yaxis.set_label_coords(-0.08, 2)
    fig.subplots_adjust(hspace=0)
    plt.xlabel("Number of voxels (log)", fontsize=12)
    plt.ylabel("Number of clusters", fontsize=12)
    handles = [
        patches.Rectangle((0, 0), 1, 1, color=c, ec="k") for c in ["blue", "green"]
    ]
    labels = ["Kmeans", "Hierarchical Kmeans"]
    fig.legend(handles, labels, loc=(0.5, 0.8))

    plotting.show()




.. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_006.png
   :alt: plot data driven parcellations
   :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_006.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 337-342

As we can see, half of the 50 KMeans clusters contain less than
100 voxels whereas three contain several thousands voxels
Hierarchical KMeans yield better balanced clusters, with a significant
proportion of them containing hundreds to thousands of voxels.


.. GENERATED FROM PYTHON SOURCE LINES 344-356

Brain parcellations with :term:`ReNA` Clustering
------------------------------------------------

One interesting algorithmic property of :term:`ReNA` (see References)
is that it is very fast
for a large number of parcels (notably faster than Ward).
As before, the :term:`parcellation` is done with a ``Parcellations`` object.
The spatial constraints are implemented inside the ``Parcellations`` object.

More about :term:`ReNA` clustering algorithm
in the original paper (:footcite:t:`Hoyos2019`).


.. GENERATED FROM PYTHON SOURCE LINES 356-371

.. code-block:: Python

    start = time.time()
    rena = Parcellations(
        method="rena",
        n_parcels=5000,
        standardize=False,
        smoothing_fwhm=2.0,
        scaling=True,
        memory="nilearn_cache",
        memory_level=1,
        verbose=1,
    )

    rena.fit_transform(dataset.func)
    print(f"ReNA 5000 clusters: {time.time() - start:.2f}s")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.wrapped] Loading data from 
    ['/home/runner/nilearn_data/development_fmri/development_fmri/sub-pixar123_task-
    pixar_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz']
    [Parcellations.wrapped] Computing mask
    [Parcellations.wrapped] Resampling mask
    [Parcellations.wrapped] Finished fit
    [Parcellations.wrapped] Loading data
    [Parcellations.wrapped] Computing image from signals
    [Parcellations.wrapped] computing rena
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.parcellations._estimator_fit...
    _estimator_fit(array([[-0.002154, ...,  0.002661],
           ...,
           [ 0.007053, ..., -0.007877]]), 
    ReNA(mask_img=<nibabel.nifti1.Nifti1Image object at 0x7fa9add0cca0>,
         memory=Memory(location=nilearn_cache/joblib), n_clusters=5000,
         scaling=True), 
    'rena')
    ________________________________________________________________________________
    [Memory] Calling nilearn.regions.rena_clustering.recursive_neighbor_agglomeration...
    recursive_neighbor_agglomeration(array([[-0.002154, ...,  0.002661],
           ...,
           [ 0.007053, ..., -0.007877]]), 
    <nibabel.nifti1.Nifti1Image object at 0x7fa9935cac20>, 5000, n_iter=10, threshold=1e-07, verbose=0)
    _________________________________recursive_neighbor_agglomeration - 0.4s, 0.0min
    ____________________________________________________estimator_fit - 0.6s, 0.0min
    [Parcellations.wrapped] Computing image from signals
    [Parcellations.wrapped] Loading regions from <nibabel.nifti1.Nifti1Image object 
    at 0x7fa9baeea140>
    [Parcellations.wrapped] Loading mask from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9baee8a30>
    [Parcellations.wrapped] Finished fit
    ________________________________________________________________________________
    [Memory] Calling nilearn.maskers.base_masker.filter_and_extract...
    filter_and_extract(<nibabel.nifti1.Nifti1Image object at 0x7fa9baeeb7c0>, <nilearn.maskers.nifti_labels_masker._ExtractionFunctor object at 0x7fa9baee8b50>, 
    { 'background_label': 0,
      'clean_args': None,
      'clean_kwargs': {},
      'cmap': 'CMRmap_r',
      'detrend': False,
      'dtype': None,
      'high_pass': None,
      'high_variance_confounds': False,
      'keep_masked_labels': False,
      'labels': None,
      'labels_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9baeea140>,
      'low_pass': None,
      'lut': None,
      'mask_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9baee8a30>,
      'reports': True,
      'smoothing_fwhm': 2.0,
      'standardize': False,
      'standardize_confounds': True,
      'strategy': 'mean',
      't_r': None,
      'target_affine': None,
      'target_shape': None}, confounds=None, sample_mask=None, dtype=None, memory=Memory(location=nilearn_cache/joblib), memory_level=1, verbose=1, sklearn_output_config=None)
    [Parcellations.wrapped] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9baeeb7c0>
    [Parcellations.wrapped] Smoothing images
    [Parcellations.wrapped] Extracting region signals
    [Parcellations.wrapped] Cleaning extracted signals
    _______________________________________________filter_and_extract - 1.4s, 0.0min
    ReNA 5000 clusters: 6.45s




.. GENERATED FROM PYTHON SOURCE LINES 372-377

Visualize: Brain parcellations (ReNA)
.....................................

First, we display the parcellations of the brain image stored in attribute
`labels_img_`

.. GENERATED FROM PYTHON SOURCE LINES 377-392

.. code-block:: Python

    rena_labels_img = rena.labels_img_

    # Now, rena_labels_img are Nifti1Image object, it can be saved to file
    # with the following code:
    rena_labels_img.to_filename(output_dir / "rena_parcellation.nii.gz")

    plotting.plot_roi(
        ward_labels_img,
        title="ReNA parcellation",
        display_mode="xz",
        cut_coords=cut_coords,
    )

    plotting.show()




.. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_007.png
   :alt: plot data driven parcellations
   :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_007.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 393-402

Compressed representation of :term:`ReNA` clustering
....................................................

We illustrate the effect that the clustering has on the signal.
We show the original data, and the approximation provided by
the clustering by averaging the signal on each parcel.

We can then compare the results with the compressed representation
obtained with Ward.

.. GENERATED FROM PYTHON SOURCE LINES 402-437

.. code-block:: Python


    # Display the original data
    plotting.plot_epi(
        mean_func_img,
        cut_coords=cut_coords,
        title=f"Original ({int(original_voxels)} voxels)",
        vmax=vmax,
        vmin=vmin,
        display_mode="xz",
    )

    plotting.show()

    # A reduced data can be created by taking the parcel-level average:
    # Note that, as many scikit-learn objects, the ``rena`` object exposes
    # a transform method that modifies input features. Here it reduces their
    # dimension.
    # However, the data are in one single large 4D image, we need to use
    # index_img to do the split easily:
    fmri_reduced_rena = rena.transform(dataset.func)

    # Display the corresponding data compression using the parcellation
    compressed_img_rena = rena.inverse_transform(fmri_reduced_rena)

    plotting.plot_epi(
        index_img(compressed_img_rena, 0),
        cut_coords=cut_coords,
        title="ReNA compressed representation (5000 parcels)",
        vmin=vmin,
        vmax=vmax,
        display_mode="xz",
    )

    plotting.show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_008.png
         :alt: plot data driven parcellations
         :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_008.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_009.png
         :alt: plot data driven parcellations
         :srcset: /auto_examples/03_connectivity/images/sphx_glr_plot_data_driven_parcellations_009.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [Parcellations.wrapped] Loading regions from <nibabel.nifti1.Nifti1Image object 
    at 0x7fa9ad92b850>
    [Parcellations.wrapped] Loading mask from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9ad929390>
    [Parcellations.wrapped] Finished fit
    ________________________________________________________________________________
    [Memory] Calling nilearn.maskers.base_masker.filter_and_extract...
    filter_and_extract(<nibabel.nifti1.Nifti1Image object at 0x7fa9ad92b130>, <nilearn.maskers.nifti_labels_masker._ExtractionFunctor object at 0x7fa99334b340>, 
    { 'background_label': 0,
      'clean_args': None,
      'clean_kwargs': {},
      'cmap': 'CMRmap_r',
      'detrend': False,
      'dtype': None,
      'high_pass': None,
      'high_variance_confounds': False,
      'keep_masked_labels': False,
      'labels': None,
      'labels_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9ad92b850>,
      'low_pass': None,
      'lut': None,
      'mask_img': <nibabel.nifti1.Nifti1Image object at 0x7fa9ad929390>,
      'reports': True,
      'smoothing_fwhm': 2.0,
      'standardize': False,
      'standardize_confounds': True,
      'strategy': 'mean',
      't_r': None,
      'target_affine': None,
      'target_shape': None}, confounds=None, sample_mask=None, dtype=None, memory=Memory(location=nilearn_cache/joblib), memory_level=1, verbose=1, sklearn_output_config=None)
    [Parcellations.wrapped] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fa9ad92b130>
    [Parcellations.wrapped] Smoothing images
    [Parcellations.wrapped] Extracting region signals
    [Parcellations.wrapped] Cleaning extracted signals
    _______________________________________________filter_and_extract - 1.4s, 0.0min




.. GENERATED FROM PYTHON SOURCE LINES 438-444

Even if the compressed signal is relatively close
to the original signal, we can notice that Ward Clustering
gives a slightly more accurate compressed representation.
However, as said in the previous section, the computation time is
reduced which could still make :term:`ReNA` more relevant than Ward in
some cases.

.. GENERATED FROM PYTHON SOURCE LINES 446-450

References
----------

.. footbibliography::


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 44.484 seconds)

**Estimated memory usage:**  3000 MB


.. _sphx_glr_download_auto_examples_03_connectivity_plot_data_driven_parcellations.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/03_connectivity/plot_data_driven_parcellations.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_data_driven_parcellations.ipynb <plot_data_driven_parcellations.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_data_driven_parcellations.py <plot_data_driven_parcellations.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_data_driven_parcellations.zip <plot_data_driven_parcellations.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
