
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_decoding/plot_miyawaki_encoding.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_02_decoding_plot_miyawaki_encoding.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_decoding_plot_miyawaki_encoding.py:


Encoding models for visual stimuli from Miyawaki et al. 2008
============================================================

This example partly reproduces the encoding model presented
in :footcite:t:`Miyawaki2008`.

Encoding models try to predict neuronal activity using information from
presented stimuli, like an image or sound. Where decoding goes from
brain data to real-world stimulus, encoding goes the other direction.

We demonstrate how to build such an **encoding model** in nilearn,
predicting **fMRI data** from **visual stimuli**,
using the dataset from :footcite:t:`Miyawaki2008`.

Participants were shown images, which consisted of random 10x10 binary
(either black or white) pixels, and the corresponding :term:`fMRI` activity
was recorded. We will try to predict the activity in each :term:`voxel`
from the binary pixel-values of the presented images. Then we extract the
receptive fields for a set of voxels to see which pixel location a
:term:`voxel` is most sensitive to.

.. seealso::

    :doc:`plot_miyawaki_reconstruction` for a decoding approach
    for the same dataset.

.. include:: ../../../examples/masker_note.rst

.. GENERATED FROM PYTHON SOURCE LINES 31-38

.. code-block:: Python


    from nilearn._utils.helpers import check_matplotlib

    check_matplotlib()

    import matplotlib.pyplot as plt








.. GENERATED FROM PYTHON SOURCE LINES 39-42

Loading the data
----------------
Now we can load the data set:

.. GENERATED FROM PYTHON SOURCE LINES 42-46

.. code-block:: Python

    from nilearn.datasets import fetch_miyawaki2008

    dataset = fetch_miyawaki2008()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_miyawaki2008] Dataset found in /home/runner/nilearn_data/miyawaki2008




.. GENERATED FROM PYTHON SOURCE LINES 47-49

We only use the training data of this study,
where random binary images were shown.

.. GENERATED FROM PYTHON SOURCE LINES 49-54

.. code-block:: Python


    # training data starts after the first 12 files
    fmri_random_runs_filenames = dataset.func[12:]
    stimuli_random_runs_filenames = dataset.label[12:]








.. GENERATED FROM PYTHON SOURCE LINES 55-57

We can use :func:`~nilearn.maskers.MultiNiftiMasker` to load the fMRI
data, clean and mask it.

.. GENERATED FROM PYTHON SOURCE LINES 57-85

.. code-block:: Python


    import numpy as np

    from nilearn.maskers import MultiNiftiMasker

    masker = MultiNiftiMasker(
        mask_img=dataset.mask,
        detrend=True,
        standardize="zscore_sample",
        n_jobs=2,
    )
    masker.fit()
    fmri_data = masker.transform(fmri_random_runs_filenames)

    # shape of the binary (i.e. black and white values) image in pixels
    stimulus_shape = (10, 10)

    # We load the visual stimuli from csv files
    stimuli = [
        np.reshape(
            np.loadtxt(stimulus_run, dtype=int, delimiter=","),
            (-1, *stimulus_shape),
            order="F",
        )
        for stimulus_run in stimuli_random_runs_filenames
    ]






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/02_decoding/plot_miyawaki_encoding.py:68: UserWarning: Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.
      masker.fit()




.. GENERATED FROM PYTHON SOURCE LINES 86-87

Let's take a look at some of these binary images:

.. GENERATED FROM PYTHON SOURCE LINES 87-98

.. code-block:: Python

    plt.figure(figsize=(8, 4))
    plt.subplot(1, 2, 1)
    plt.imshow(stimuli[0][124], interpolation="nearest", cmap="gray")
    plt.axis("off")
    plt.title("Run 1, Stimulus 125")
    plt.subplot(1, 2, 2)
    plt.imshow(stimuli[2][101], interpolation="nearest", cmap="gray")
    plt.axis("off")
    plt.title("Run 3, Stimulus 102")
    plt.subplots_adjust(wspace=0.5)




.. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_001.png
   :alt: Run 1, Stimulus 125, Run 3, Stimulus 102
   :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 99-101

We now stack the :term:`fMRI` and stimulus data and remove an offset in the
beginning/end.

.. GENERATED FROM PYTHON SOURCE LINES 101-107

.. code-block:: Python


    fmri_data = np.vstack([fmri_run[2:] for fmri_run in fmri_data])
    stimuli = np.vstack([stimuli_run[:-2] for stimuli_run in stimuli]).astype(
        float
    )








.. GENERATED FROM PYTHON SOURCE LINES 108-109

fmri_data is a matrix of *samples* x *voxels*

.. GENERATED FROM PYTHON SOURCE LINES 109-112

.. code-block:: Python


    print(f"{fmri_data.shape=}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    fmri_data.shape=(2860, 5438)




.. GENERATED FROM PYTHON SOURCE LINES 113-115

We flatten the last two dimensions of stimuli
so it is a matrix of *samples* x *pixels*.

.. GENERATED FROM PYTHON SOURCE LINES 115-121

.. code-block:: Python


    # Flatten the stimuli
    stimuli = np.reshape(stimuli, (-1, stimulus_shape[0] * stimulus_shape[1]))

    print(f"{stimuli.shape=}")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    stimuli.shape=(2860, 100)




.. GENERATED FROM PYTHON SOURCE LINES 122-129

Building the encoding models
----------------------------
We can now proceed to build a simple **voxel-wise encoding model** using
`Ridge regression <https://en.wikipedia.org/wiki/Tikhonov_regularization>`_.
For each voxel we fit an independent regression model,
using the pixel-values of the visual stimuli to predict the neuronal
activity in this voxel.

.. GENERATED FROM PYTHON SOURCE LINES 131-135

Using 10-fold cross-validation, we partition the data into 10 'folds'.
We hold out each fold of the data for testing, then fit a ridge regression
to the remaining 9/10 of the data, using stimuli as predictors
and fmri_data as targets, and create predictions for the held-out 10th.

.. GENERATED FROM PYTHON SOURCE LINES 135-154

.. code-block:: Python

    from sklearn.linear_model import Ridge
    from sklearn.metrics import r2_score
    from sklearn.model_selection import KFold

    estimator = Ridge(alpha=100.0)
    cv = KFold(n_splits=10)

    scores = []
    for train, test in cv.split(X=stimuli):
        # we train the Ridge estimator on the training set
        # and predict the fMRI activity for the test set
        predictions = estimator.fit(
            stimuli.reshape(-1, 100)[train], fmri_data[train]
        ).predict(stimuli.reshape(-1, 100)[test])
        # we compute how much variance our encoding model explains in each voxel
        scores.append(
            r2_score(fmri_data[test], predictions, multioutput="raw_values")
        )








.. GENERATED FROM PYTHON SOURCE LINES 155-159

Mapping the encoding scores on the brain
----------------------------------------
To plot the scores onto the brain, we create a Nifti1Image containing
the scores and then threshold it:

.. GENERATED FROM PYTHON SOURCE LINES 159-173

.. code-block:: Python


    from nilearn.image import threshold_img

    cut_score = np.mean(scores, axis=0)
    cut_score[cut_score < 0] = 0

    # bring the scores into the shape of the background brain
    score_map_img = masker.inverse_transform(cut_score)

    thresholded_score_map_img = threshold_img(
        score_map_img, threshold=1e-6, copy=False
    )









.. GENERATED FROM PYTHON SOURCE LINES 174-176

Plotting the statistical map on a background brain, we mark four voxels
which we will inspect more closely later on.

.. GENERATED FROM PYTHON SOURCE LINES 176-218

.. code-block:: Python

    from nilearn.image import coord_transform
    from nilearn.plotting import plot_stat_map, show


    def index_to_xy_coord(x, y, z=10):
        """Transform data index to coordinates of the background + offset."""
        coords = coord_transform(x, y, z, affine=thresholded_score_map_img.affine)
        return np.array(coords)[np.newaxis, :] + np.array([0, 1, 0])


    xy_indices_of_special_voxels = [(30, 10), (32, 10), (31, 9), (31, 10)]

    display = plot_stat_map(
        thresholded_score_map_img,
        bg_img=dataset.background,
        cut_coords=[-8],
        display_mode="z",
        aspect=1.25,
        title="Explained variance per voxel",
        cmap="inferno",
    )

    # creating a marker for each voxel and adding it to the statistical map

    for i, (x, y) in enumerate(xy_indices_of_special_voxels):
        display.add_markers(
            index_to_xy_coord(x, y),
            marker_color="none",
            edgecolor=["b", "r", "magenta", "g"][i],
            marker_size=140,
            marker="s",
            facecolor="none",
            lw=4.5,
        )


    # re-set figure size after construction so colorbar gets rescaled too
    fig = plt.gcf()
    fig.set_size_inches(12, 12)

    show()




.. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_002.png
   :alt: plot miyawaki encoding
   :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/.tox/doc/lib/python3.10/site-packages/nilearn/plotting/displays/_slicers.py:862: UserWarning: You passed both c and facecolor/facecolors for the markers. c has precedence over facecolor/facecolors. This behavior may change in the future.
      display_ax.ax.scatter(




.. GENERATED FROM PYTHON SOURCE LINES 219-231

Estimating receptive fields
---------------------------
Now we take a closer look at the receptive fields of the four marked voxels.
A voxel's `receptive field <https://en.wikipedia.org/wiki/Receptive_field>`_
is the region of a stimulus (like an image) where the presence of an object,
like a white instead of a black pixel, results in a change in activity
in the voxel. In our case the receptive field is just the vector of 100
regression  coefficients (one for each pixel) reshaped into the 10x10
form of the original images. Some voxels are receptive to only very few
pixels, so we use `Lasso regression
<https://en.wikipedia.org/wiki/Lasso_(statistics)>`_ to estimate a sparse
set of regression coefficients.

.. GENERATED FROM PYTHON SOURCE LINES 231-311

.. code-block:: Python


    from sklearn.linear_model import LassoLarsCV
    from sklearn.pipeline import make_pipeline
    from sklearn.preprocessing import StandardScaler

    # automatically estimate the sparsity by cross-validation

    lasso = make_pipeline(StandardScaler(), LassoLarsCV(max_iter=10))

    # Mark the same pixel in each receptive field
    marked_pixel = (4, 2)

    from matplotlib import gridspec
    from matplotlib.patches import Rectangle

    fig = plt.figure(figsize=(12, 8))
    fig.suptitle("Receptive fields of the marked voxels", fontsize=25)

    # GridSpec allows us to do subplots with more control of the spacing
    gs1 = gridspec.GridSpec(2, 3)

    # we fit the Lasso for each of the three voxels of the upper row
    for i, index in enumerate([1780, 1951, 2131]):
        ax = plt.subplot(gs1[0, i])
        lasso.fit(stimuli, fmri_data[:, index])
        # we reshape the coefficients into the form of the original images
        rf = lasso.named_steps["lassolarscv"].coef_.reshape((10, 10))
        # add a black background
        ax.imshow(np.zeros_like(rf), vmin=0.0, vmax=1.0, cmap="gray")
        ax_im = ax.imshow(
            np.ma.masked_less(rf, 0.1),
            interpolation="nearest",
            cmap=["Blues", "Greens", "Reds"][i],
            vmin=0.0,
            vmax=0.75,
        )
        # add the marked pixel
        ax.add_patch(
            Rectangle(
                (marked_pixel[1] - 0.5, marked_pixel[0] - 0.5),
                1,
                1,
                facecolor="none",
                edgecolor="r",
                lw=4,
            )
        )
        plt.axis("off")
        plt.colorbar(ax_im, ax=ax)

    # and then for the voxel at the bottom

    gs1.update(left=0.0, right=1.0, wspace=0.1)
    ax = plt.subplot(gs1[1, 1])
    lasso.fit(stimuli, fmri_data[:, 1935])
    # we reshape the coefficients into the form of the original images
    rf = lasso.named_steps["lassolarscv"].coef_.reshape((10, 10))
    ax.imshow(np.zeros_like(rf), vmin=0.0, vmax=1.0, cmap="gray")
    ax_im = ax.imshow(
        np.ma.masked_less(rf, 0.1),
        interpolation="nearest",
        cmap="RdPu",
        vmin=0.0,
        vmax=0.75,
    )

    # add the marked pixel
    ax.add_patch(
        Rectangle(
            (marked_pixel[1] - 0.5, marked_pixel[0] - 0.5),
            1,
            1,
            facecolor="none",
            edgecolor="r",
            lw=4,
        )
    )
    plt.axis("off")
    plt.colorbar(ax_im, ax=ax)




.. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_003.png
   :alt: Receptive fields of the marked voxels
   :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_miyawaki_encoding_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7f08ed1e53f0>



.. GENERATED FROM PYTHON SOURCE LINES 312-317

The receptive fields of the four voxels are not only close to each other,
the relative location of the pixel each voxel is most sensitive to
roughly maps to the relative location of the voxels to each other.
We can see a relationship between some voxel's receptive field and
its location in the brain.

.. GENERATED FROM PYTHON SOURCE LINES 319-323

References
----------

.. footbibliography::


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 15.461 seconds)

**Estimated memory usage:**  1135 MB


.. _sphx_glr_download_auto_examples_02_decoding_plot_miyawaki_encoding.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/02_decoding/plot_miyawaki_encoding.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_miyawaki_encoding.ipynb <plot_miyawaki_encoding.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_miyawaki_encoding.py <plot_miyawaki_encoding.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_miyawaki_encoding.zip <plot_miyawaki_encoding.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
