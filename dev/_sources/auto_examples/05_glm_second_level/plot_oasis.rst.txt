
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/05_glm_second_level/plot_oasis.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_05_glm_second_level_plot_oasis.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_05_glm_second_level_plot_oasis.py:


Voxel-Based Morphometry on OASIS dataset
========================================

This example uses voxel-based morphometry (:term:`VBM`) to study the
relationship between aging, sex, and gray matter density.

The data come from the `OASIS <https://sites.wustl.edu/oasisbrains/>`_ project.
If you use it, you need to agree with the data usage agreement available
on the website.

It has been run through a standard :term:`VBM` pipeline
(using SPM8 and NewSegment)
to create :term:`VBM` maps, which we study here.

VBM analysis of aging
---------------------

We run a standard :term:`GLM` analysis
to study the association between age and gray matter density
from the :term:`VBM` data.
We use only 100 subjects from the OASIS dataset to limit the memory usage.

Note that more power would be obtained from using a larger sample of subjects.

.. seealso::

    For more information
    see the :ref:`dataset description <oasis_maps>`.

.. GENERATED FROM PYTHON SOURCE LINES 33-35

Load Oasis dataset
------------------

.. GENERATED FROM PYTHON SOURCE LINES 35-50

.. code-block:: Python

    from nilearn.datasets import (
        fetch_icbm152_2009,
        fetch_icbm152_brain_gm_mask,
        fetch_oasis_vbm,
    )
    from nilearn.plotting import plot_design_matrix, plot_stat_map

    n_subjects = 100  # more subjects requires more memory

    oasis_dataset = fetch_oasis_vbm(
        n_subjects=n_subjects,
    )
    gray_matter_map_filenames = oasis_dataset.gray_matter_maps
    age = oasis_dataset.ext_vars["age"].astype(float)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_oasis_vbm] Dataset found in /home/runner/nilearn_data/oasis1




.. GENERATED FROM PYTHON SOURCE LINES 51-52

Sex is encoded as 'M' or 'F'. Hence, we make it a binary variable.

.. GENERATED FROM PYTHON SOURCE LINES 52-54

.. code-block:: Python

    sex = oasis_dataset.ext_vars["mf"] == "F"








.. GENERATED FROM PYTHON SOURCE LINES 55-56

Print basic information on the dataset.

.. GENERATED FROM PYTHON SOURCE LINES 56-65

.. code-block:: Python

    print(
        "First gray-matter anatomy image (3D) is located at: "
        f"{oasis_dataset.gray_matter_maps[0]}"
    )
    print(
        "First white-matter anatomy image (3D) is located at: "
        f"{oasis_dataset.white_matter_maps[0]}"
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    First gray-matter anatomy image (3D) is located at: /home/runner/nilearn_data/oasis1/OAS1_0001_MR1/mwrc1OAS1_0001_MR1_mpr_anon_fslswapdim_bet.nii.gz
    First white-matter anatomy image (3D) is located at: /home/runner/nilearn_data/oasis1/OAS1_0001_MR1/mwrc2OAS1_0001_MR1_mpr_anon_fslswapdim_bet.nii.gz




.. GENERATED FROM PYTHON SOURCE LINES 66-67

Get a mask image: A mask of the cortex of the ICBM template.

.. GENERATED FROM PYTHON SOURCE LINES 67-69

.. code-block:: Python

    gm_mask = fetch_icbm152_brain_gm_mask()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_icbm152_brain_gm_mask] Dataset found in 
    /home/runner/nilearn_data/icbm152_2009




.. GENERATED FROM PYTHON SOURCE LINES 70-71

Resample the mask, since this mask has a different resolution.

.. GENERATED FROM PYTHON SOURCE LINES 71-80

.. code-block:: Python

    from nilearn.image import resample_to_img

    mask_img = resample_to_img(
        gm_mask,
        gray_matter_map_filenames[0],
        interpolation="nearest",
        copy_header=True,
    )








.. GENERATED FROM PYTHON SOURCE LINES 81-85

Analyze data
------------
First, we create an adequate design matrix with three columns: 'age', 'sex',
and 'intercept'.

.. GENERATED FROM PYTHON SOURCE LINES 85-96

.. code-block:: Python

    import numpy as np
    import pandas as pd

    intercept = np.ones(n_subjects)
    design_matrix = pd.DataFrame(
        np.vstack((age, sex, intercept)).T,
        columns=["age", "sex", "intercept"],
    )

    from matplotlib import pyplot as plt








.. GENERATED FROM PYTHON SOURCE LINES 97-98

Let's plot the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 98-103

.. code-block:: Python

    fig, ax1 = plt.subplots(1, 1, figsize=(4, 8))
    ax = plot_design_matrix(design_matrix, axes=ax1)
    ax.set_ylabel("maps")
    fig.suptitle("Second level design matrix")




.. image-sg:: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_001.png
   :alt: Second level design matrix
   :srcset: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Text(0.5, 0.98, 'Second level design matrix')



.. GENERATED FROM PYTHON SOURCE LINES 104-106

Next, we specify and fit the second-level model when loading the data and
also smooth a little bit to improve statistical behavior.

.. GENERATED FROM PYTHON SOURCE LINES 106-120

.. code-block:: Python

    from nilearn.glm.second_level import SecondLevelModel

    second_level_model = SecondLevelModel(
        smoothing_fwhm=2.0,
        mask_img=mask_img,
        n_jobs=2,
        minimize_memory=False,
        verbose=1,
    )
    second_level_model.fit(
        gray_matter_map_filenames,
        design_matrix=design_matrix,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [SecondLevelModel.fit] Fitting second level model. Take a deep breath.
    [SecondLevelModel.fit] Loading data from <nibabel.nifti1.Nifti1Image object at 
    0x7fd520efca90>
    [SecondLevelModel.fit] Loading mask from <nibabel.nifti1.Nifti1Image object at 
    0x7fd548e9d270>
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:115: UserWarning: [NiftiMasker.fit] Generation of a mask has been requested (imgs != None) while a mask was given at masker creation. Given mask will be used.
      second_level_model.fit(
    [SecondLevelModel.fit] Resampling mask
    [SecondLevelModel.fit] Finished fit
    [SecondLevelModel.fit] 
    Computation of second level model done in 0.38 seconds.



.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-11 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-11 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-11 pre {
      padding: 0;
    }

    #sk-container-id-11 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-11 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-11 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-11 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-11 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-11 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-11 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-11 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-11 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-11 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-11 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-11 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-11 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-11 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "▸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-11 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-11 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-11 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-11 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "▾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-11 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-11 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-11 div.sk-label label.sk-toggleable__label,
    #sk-container-id-11 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-11 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-11 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-11 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-11 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-11 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-11 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-11 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-11 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-11 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-11 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-11 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-11" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Second Level Model</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" checked><label for="sk-estimator-id-11" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;SecondLevelModel<span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>Second Level Model</pre></div> </div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 121-123

Estimating the :term:`contrast` is very simple.
We can just provide the column name of the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 123-128

.. code-block:: Python

    z_map = second_level_model.compute_contrast(
        second_level_contrast=[1, 0, 0],
        output_type="z_score",
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [SecondLevelModel.compute_contrast] Loading data from 
    <nibabel.nifti1.Nifti1Image object at 0x7fd520efffa0>
    [SecondLevelModel.compute_contrast] Smoothing images
    [SecondLevelModel.compute_contrast] Extracting region signals
    [SecondLevelModel.compute_contrast] Cleaning extracted signals
    [SecondLevelModel.compute_contrast] Computing image from signals




.. GENERATED FROM PYTHON SOURCE LINES 129-133

View results
------------
We threshold the second level :term:`contrast`
at FDR-corrected p < 0.05 and plot it.

.. GENERATED FROM PYTHON SOURCE LINES 133-150

.. code-block:: Python

    from nilearn.glm import threshold_stats_img
    from nilearn.plotting import show

    _, threshold = threshold_stats_img(z_map, alpha=0.05, height_control="fdr")
    print(f"The FDR=.05-corrected threshold is: {threshold:03g}")

    fig = plt.figure(figsize=(5, 3))
    display = plot_stat_map(
        z_map,
        threshold=threshold,
        display_mode="z",
        cut_coords=[-4, 26],
        figure=fig,
    )
    fig.suptitle("age effect on gray matter density (FDR = .05)")
    show()




.. image-sg:: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_002.png
   :alt: age effect on gray matter density (FDR = .05)
   :srcset: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    The FDR=.05-corrected threshold is: 2.40175




.. GENERATED FROM PYTHON SOURCE LINES 151-153

We can also study the effect of sex by computing the contrast, thresholding
it and plot the resulting map.

.. GENERATED FROM PYTHON SOURCE LINES 153-165

.. code-block:: Python

    z_map = second_level_model.compute_contrast(
        second_level_contrast="sex",
        output_type="z_score",
    )
    _, threshold = threshold_stats_img(z_map, alpha=0.05, height_control="fdr")
    plot_stat_map(
        z_map,
        threshold=threshold,
        title="sex effect on gray matter density (FDR = .05)",
    )
    show()




.. image-sg:: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_003.png
   :alt: plot oasis
   :srcset: /auto_examples/05_glm_second_level/images/sphx_glr_plot_oasis_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [SecondLevelModel.compute_contrast] Loading data from 
    <nibabel.nifti1.Nifti1Image object at 0x7fd520efc5e0>
    [SecondLevelModel.compute_contrast] Smoothing images
    [SecondLevelModel.compute_contrast] Extracting region signals
    [SecondLevelModel.compute_contrast] Cleaning extracted signals
    [SecondLevelModel.compute_contrast] Computing image from signals




.. GENERATED FROM PYTHON SOURCE LINES 166-168

Note that there does not seem to be any significant effect of sex on
gray matter density on that dataset.

.. GENERATED FROM PYTHON SOURCE LINES 170-178

Saving model outputs to disk
----------------------------
It can be useful to quickly generate a portable, ready-to-view report with
most of the pertinent information.
We can do this by saving the output of the GLM to disk
including an HTML report.
This is easy to do if you have a fitted model and the list of contrasts,
which we do here.

.. GENERATED FROM PYTHON SOURCE LINES 178-197

.. code-block:: Python

    from pathlib import Path

    from nilearn.interfaces.bids import save_glm_to_bids

    output_dir = Path.cwd() / "results" / "plot_oasis"
    output_dir.mkdir(exist_ok=True, parents=True)

    icbm152_2009 = fetch_icbm152_2009()

    second_level_model = save_glm_to_bids(
        second_level_model,
        contrasts=["age", "sex"],
        out_dir=output_dir / "derivatives" / "nilearn_glm",
        prefix="ageEffectOnGM",
        bg_img=icbm152_2009["t1"],
        alpha=0.05,
        height_control="fdr",
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [fetch_icbm152_2009] Dataset found in /home/runner/nilearn_data/icbm152_2009
    [save_glm_to_bids] Saving mask...
    [save_glm_to_bids] Generating design matrices figures...
    [save_glm_to_bids] Generating contrast matrices figures...
    [save_glm_to_bids] Saving contrast-level statistical maps...
    [SecondLevelModel._make_stat_maps] Loading data from <nibabel.nifti1.Nifti1Image
    object at 0x7fd53a86c4f0>
    [SecondLevelModel._make_stat_maps] Smoothing images
    [SecondLevelModel._make_stat_maps] Extracting region signals
    [SecondLevelModel._make_stat_maps] Cleaning extracted signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Loading data from <nibabel.nifti1.Nifti1Image
    object at 0x7fd51b1e5660>
    [SecondLevelModel._make_stat_maps] Smoothing images
    [SecondLevelModel._make_stat_maps] Extracting region signals
    [SecondLevelModel._make_stat_maps] Cleaning extracted signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    [SecondLevelModel._make_stat_maps] Computing image from signals
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: 'threshold=3.09' will not be used with 'height_control='fdr''. 'threshold' is only used when 'height_control=None'. Set 'threshold' to '3.0' to avoid this warning.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 2.75855356998246e-06. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 0.0. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: Attention: No clusters with stat higher than inf
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: 'threshold=3.09' will not be used with 'height_control='fdr''. 'threshold' is only used when 'height_control=None'. Set 'threshold' to '3.0' to avoid this warning.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 0.00743128949229107. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 0.0. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: Attention: No clusters with stat higher than inf
      second_level_model = save_glm_to_bids(
    [save_glm_to_bids] Saving model level statistical maps...
    [SecondLevelModel.residuals] Computing image from signals
    [SecondLevelModel.r_square] Computing image from signals
    [save_glm_to_bids] Generating HTML...
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: 'threshold=3.09' will not be used with 'height_control='fdr''. 'threshold' is only used when 'height_control=None'. Set 'threshold' to '3.0' to avoid this warning.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: 'threshold=3.09' will not be used with 'height_control='fdr''. 'threshold' is only used when 'height_control=None'. Set 'threshold' to '3.0' to avoid this warning.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 3.800631000800836. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: The given float value must not exceed 0.0. But, you have given threshold=inf.
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: Attention: No clusters with stat higher than inf
      second_level_model = save_glm_to_bids(
    /home/runner/work/nilearn/nilearn/examples/05_glm_second_level/plot_oasis.py:187: UserWarning: empty mask
      second_level_model = save_glm_to_bids(




.. GENERATED FROM PYTHON SOURCE LINES 198-199

View the generated files

.. GENERATED FROM PYTHON SOURCE LINES 199-214

.. code-block:: Python

    files = sorted((output_dir / "derivatives" / "nilearn_glm").glob("**/*"))
    print("\n".join([str(x.relative_to(output_dir)) for x in files]))

    #  %%
    # Generate a report and view it.
    # If no new contrast is passed to ``generate_report``,
    # the results saved to disk will be reused to generate the report.

    report = second_level_model.generate_report(
        bg_img=icbm152_2009["t1"],
        plot_type="glass",
        alpha=0.05,
        height_control=None,
    )
    report.save_as_html(output_dir / "report.html")




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    derivatives/nilearn_glm/dataset_description.json
    derivatives/nilearn_glm/group
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_clusters.json
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_clusters.tsv
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_design.png
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_stat-effect_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_stat-p_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_stat-t_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_stat-variance_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-age_stat-z_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_clusters.json
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_clusters.tsv
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_design.png
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_stat-effect_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_stat-p_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_stat-t_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_stat-variance_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_contrast-sex_stat-z_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_corrdesign.png
    derivatives/nilearn_glm/group/ageEffectOnGM_design.png
    derivatives/nilearn_glm/group/ageEffectOnGM_design.tsv
    derivatives/nilearn_glm/group/ageEffectOnGM_mask.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_report.html
    derivatives/nilearn_glm/group/ageEffectOnGM_stat-errorts_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_stat-rsquared_statmap.nii.gz
    derivatives/nilearn_glm/group/ageEffectOnGM_statmap.json
    threshold=3.09 current_default=3.09 old_default=3.09
    [SecondLevelModel.generate_report] Generating contrast-level figures...
    [SecondLevelModel.generate_report] Generating design matrices figures...
    [SecondLevelModel.generate_report] Generating contrast matrices figures...





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 21.587 seconds)

**Estimated memory usage:**  1755 MB


.. _sphx_glr_download_auto_examples_05_glm_second_level_plot_oasis.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/05_glm_second_level/plot_oasis.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_oasis.ipynb <plot_oasis.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_oasis.py <plot_oasis.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_oasis.zip <plot_oasis.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
