{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Example of surface-based first-level analysis\n\nA full step-by-step example of fitting a :term:`GLM`\nto experimental data sampled on the cortical surface\nand visualizing the results.\n\nMore specifically:\n\n1. A sequence of :term:`fMRI` volumes is loaded.\n2. :term:`fMRI` data are projected onto a reference cortical surface\n   (the FreeSurfer template, fsaverage).\n3. A :term:`GLM` is applied to the dataset\n   (effect/covariance, then contrast estimation).\n4. Inspect GLM reports and save the results to disk.\n\nThe result of the analysis are statistical maps that are defined\non the brain mesh.\nWe display them using Nilearn capabilities.\n\nThe projection of :term:`fMRI` data onto a given brain :term:`mesh` requires\nthat both are initially defined in the same space.\n\n* The functional data should be coregistered to the anatomy\n  from which the mesh was obtained.\n\n* Another possibility, used here, is to project\n  the normalized :term:`fMRI` data to an :term:`MNI`-coregistered mesh,\n  such as fsaverage.\n\nThe advantage of this second approach is that it makes it easy to run\nsecond-level analyses on the surface.\nOn the other hand, it is obviously less accurate\nthan using a subject-tailored mesh.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data and analysis parameters\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fetch the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_localizer_first_level\n\ndata = fetch_localizer_first_level()\nt_r = data.t_r\nslice_time_ref = data.slice_time_ref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project the :term:`fMRI` image to the surface\n\nFor this we need to get a :term:`mesh`\nrepresenting the geometry of the surface.\nWe could use an individual :term:`mesh`,\nbut we first resort to a standard :term:`mesh`,\nthe so-called fsaverage5 template from the FreeSurfer software.\n\nWe use the :class:`~nilearn.surface.SurfaceImage`\nto create an surface object instance\nthat contains both the mesh\n(here we use the one from the fsaverage5 templates)\nand the BOLD data that we project on the surface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import load_fsaverage\nfrom nilearn.surface import SurfaceImage\n\nfsaverage5 = load_fsaverage()\nsurface_image = SurfaceImage.from_volume(\n    mesh=fsaverage5[\"pial\"],\n    volume_img=data.epi_img,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform first level analysis\n\nWe can now simply run a GLM by directly passing\nour :class:`~nilearn.surface.SurfaceImage` instance\nas input to FirstLevelModel.fit\n\nHere we use an :term:`HRF` model\ncontaining the Glover model and its time derivative\nThe drift model is implicitly a cosine basis with a period cutoff at 128s.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import FirstLevelModel\n\nglm = FirstLevelModel(\n    t_r=t_r,\n    slice_time_ref=slice_time_ref,\n    hrf_model=\"glover + derivative\",\n    minimize_memory=False,\n    verbose=1,\n).fit(run_imgs=surface_image, events=data.events)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate contrasts\n\nSpecify the contrasts.\n\nFor practical purpose, we first generate an identity matrix whose size is\nthe number of columns of the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndesign_matrix = glm.design_matrices_[0]\ncontrast_matrix = np.eye(design_matrix.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At first, we create basic contrasts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts = {\n    column: contrast_matrix[i]\n    for i, column in enumerate(design_matrix.columns)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add some intermediate contrasts and\none :term:`contrast` adding all conditions with some auditory parts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts[\"audio\"] = (\n    basic_contrasts[\"audio_left_hand_button_press\"]\n    + basic_contrasts[\"audio_right_hand_button_press\"]\n    + basic_contrasts[\"audio_computation\"]\n    + basic_contrasts[\"sentence_listening\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "one contrast adding all conditions involving instructions reading\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts[\"visual\"] = (\n    basic_contrasts[\"visual_left_hand_button_press\"]\n    + basic_contrasts[\"visual_right_hand_button_press\"]\n    + basic_contrasts[\"visual_computation\"]\n    + basic_contrasts[\"sentence_reading\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "one contrast adding all conditions involving computation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts[\"computation\"] = (\n    basic_contrasts[\"visual_computation\"]\n    + basic_contrasts[\"audio_computation\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "one contrast adding all conditions involving sentences\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts[\"sentences\"] = (\n    basic_contrasts[\"sentence_listening\"] + basic_contrasts[\"sentence_reading\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create a dictionary of more relevant contrasts\n\n* ``'left - right button press'``: probes motor activity\n  in left versus right button presses.\n* ``'audio - visual'``: probes the difference of activity between listening\n  to some content or reading the same type of content\n  (instructions, stories).\n* ``'computation - sentences'``: looks at the activity\n  when performing a mental computation task  versus simply reading sentences.\n\nOf course, we could define other contrasts,\nbut we keep only 3 for simplicity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrasts = {\n    \"(left - right) button press\": (\n        basic_contrasts[\"audio_left_hand_button_press\"]\n        - basic_contrasts[\"audio_right_hand_button_press\"]\n        + basic_contrasts[\"visual_left_hand_button_press\"]\n        - basic_contrasts[\"visual_right_hand_button_press\"]\n    ),\n    \"audio - visual\": basic_contrasts[\"audio\"] - basic_contrasts[\"visual\"],\n    \"computation - sentences\": (\n        basic_contrasts[\"computation\"] - basic_contrasts[\"sentences\"]\n    ),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's estimate the t-contrasts,\nand extract a table of clusters\nthat survive our thresholds.\n\nWe use the same threshold (uncorrected p < 0.001)\nfor all contrasts.\n\nWe plot each contrast map on the inflated fsaverage mesh,\ntogether with a suitable background to give an impression\nof the cortex folding.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n\nfrom nilearn.datasets import load_fsaverage_data\nfrom nilearn.plotting import plot_surf_stat_map, show\nfrom nilearn.reporting import get_clusters_table\n\np_val = 0.001\nthreshold = norm.isf(p_val)\ncluster_threshold = 20\ntwo_sided = True\n\nfsaverage_data = load_fsaverage_data(data_type=\"sulcal\")\n\nresults = {}\nfor contrast_id, contrast_val in contrasts.items():\n    results[contrast_id] = glm.compute_contrast(contrast_val, stat_type=\"t\")\n\n    table = get_clusters_table(\n        results[contrast_id],\n        stat_threshold=threshold,\n        cluster_threshold=cluster_threshold,\n        two_sided=two_sided,\n    )\n    print(f\"\\n{contrast_id=}\")\n    print(table)\n\nfor contrast_id, z_score in results.items():\n    hemi = \"left\"\n    if contrast_id == \"(left - right) button press\":\n        hemi = \"both\"\n\n    plot_surf_stat_map(\n        surf_mesh=fsaverage5[\"inflated\"],\n        stat_map=z_score,\n        hemi=hemi,\n        title=contrast_id,\n        threshold=threshold,\n        bg_map=fsaverage_data,\n    )\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cluster-level inference\n\nWe can also perform cluster-level inference\n(aka \"All resolution Inference\") for a given contrast.\nThis gives a high-probability lower bound\non the proportion of true discoveries in each cluster\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm import cluster_level_inference\nfrom nilearn.surface.surface import get_data as get_surf_data\n\nproportion_true_discoveries_img = cluster_level_inference(\n    results[\"audio - visual\"], threshold=[2.5, 3.5, 4.5], alpha=0.05\n)\n\ndata = get_surf_data(proportion_true_discoveries_img)\nunique_vals = np.unique(data.ravel())\nprint(unique_vals)\n\nplot_surf_stat_map(\n    surf_mesh=fsaverage5[\"inflated\"],\n    stat_map=proportion_true_discoveries_img,\n    hemi=\"left\",\n    cmap=\"inferno\",\n    title=\"audio - visual, proportion true positives\",\n    bg_map=fsaverage_data,\n    avg_method=\"max\",\n)\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a report for the GLM\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report = glm.generate_report(\n    contrasts,\n    threshold=threshold,\n    bg_img=load_fsaverage_data(data_type=\"sulcal\", mesh_type=\"inflated\"),\n    height_control=None,\n    cluster_threshold=cluster_threshold,\n    two_sided=two_sided,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. include:: ../../../examples/report_note.rst\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}