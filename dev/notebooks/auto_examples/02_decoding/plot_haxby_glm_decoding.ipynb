{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decoding of a dataset after GLM fit for signal extraction\n\nFull step-by-step example of fitting a :term:`GLM`\nto perform a decoding experiment. In this decoding analysis,\nwe will be doing a one-vs-all classification.\nWe use the data from one subject of the Haxby dataset.\n\nMore specifically:\n\n1. Download the Haxby dataset.\n2. Extract the information to generate a glm\n   representing the blocks of stimuli.\n3. Analyze the decoding performance using a classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch example Haxby dataset\nWe download the Haxby dataset\nThis is a study of visual object category representation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# By default 2nd subject will be fetched\nimport numpy as np\nimport pandas as pd\n\nfrom nilearn.datasets import fetch_haxby\n\nhaxby_dataset = fetch_haxby()\n\n# repetition has to be known\nt_r = 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the behavioral data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load target information as string and give a numerical identifier to each\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\nconditions = behavioral[\"labels\"].to_numpy()\n\n# Record these as an array of runs\nruns = behavioral[\"chunks\"].to_numpy()\nunique_runs = behavioral[\"chunks\"].unique()\n\n# fMRI data: a unique file for each run\nfunc_filename = haxby_dataset.func[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a proper event structure for each run\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = {}\n# events will take the form of a dictionary of Dataframes, one per run\nfor run in unique_runs:\n    # get the condition label per run\n    conditions_run = conditions[runs == run]\n    # get the number of scans per run, then the corresponding\n    # vector of frame times\n    n_scans = len(conditions_run)\n    frame_times = t_r * np.arange(n_scans)\n    # each event last the full TR\n    duration = t_r * np.ones(n_scans)\n    # Define the events object\n    events_ = pd.DataFrame(\n        {\n            \"onset\": frame_times,\n            \"trial_type\": conditions_run,\n            \"duration\": duration,\n        }\n    )\n    # remove the rest condition and insert into the dictionary\n    events[run] = events_[events_.trial_type != \"rest\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate and run FirstLevelModel\n\nWe generate a list of z-maps together with their run and condition index\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_maps = []\nconditions_label = []\nrun_label = []\n\n# Instantiate the glm\nfrom nilearn.glm.first_level import FirstLevelModel\n\nglm = FirstLevelModel(\n    t_r=t_r,\n    mask_img=haxby_dataset.mask,\n    high_pass=0.008,\n    smoothing_fwhm=4,\n    memory=\"nilearn_cache\",\n    memory_level=2,\n    verbose=1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the :term:`GLM` on data from each run\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events[run].trial_type.unique()\nfrom nilearn.image import index_img\n\nfor run in unique_runs:\n    # grab the fmri data for that particular run\n    fmri_run = index_img(func_filename, runs == run)\n\n    # fit the GLM\n    glm.fit(fmri_run, events=events[run])\n\n    # set up contrasts: one per condition\n    conditions = events[run].trial_type.unique()\n    for condition_ in conditions:\n        z_maps.append(glm.compute_contrast(condition_))\n        conditions_label.append(condition_)\n        run_label.append(run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating a report\nSince we have already computed the FirstLevelModel\nand have the :term:`contrast`, we can quickly create a summary report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import mean_img\n\nmean_img_ = mean_img(func_filename, copy_header=True)\nreport = glm.generate_report(\n    contrasts=conditions,\n    bg_img=mean_img_,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This report can be viewed in a notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a jupyter notebook, the report will be automatically inserted, as above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can access the report via a browser:\n# report.open_in_browser()\n\n# Or we can save as an html file.\nfrom pathlib import Path\n\noutput_dir = Path.cwd() / \"results\" / \"plot_haxby_glm_decoding\"\noutput_dir.mkdir(exist_ok=True, parents=True)\nreport.save_as_html(output_dir / \"report.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the decoding pipeline\nTo define the decoding pipeline we use Decoder object, we choose :\n\n* a prediction model, here a Support Vector Classifier,\n  with a linear kernel\n\n* the mask to use, here a ventral temporal ROI in the visual cortex\n\n* although it usually helps to decode better, z-maps time series don't\n  need to be rescaled to a 0 mean, variance of 1 so we use\n  standardize=False.\n\n* we use univariate feature selection to reduce the dimension of the\n  problem keeping only 5% of voxels which are most informative.\n\n* a cross-validation scheme, here we use LeaveOneGroupOut\n  cross-validation on the runs which corresponds\n  to a leave-one-run-out\n\nWe fit directly this pipeline on the Niimgs outputs of the GLM, with\ncorresponding conditions labels and run labels\n(for the cross validation).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\n\nfrom nilearn.decoding import Decoder\n\ndecoder = Decoder(\n    estimator=\"svc\",\n    mask=haxby_dataset.mask,\n    standardize=False,\n    screening_percentile=5,\n    cv=LeaveOneGroupOut(),\n    verbose=1,\n)\ndecoder.fit(z_maps, conditions_label, groups=run_label)\n\n# Return the corresponding mean prediction accuracy compared to chance\n# for classifying one-vs-all items.\n\nclassification_accuracy = np.mean(list(decoder.cv_scores_.values()))\nchance_level = 1.0 / len(np.unique(conditions))\nprint(\n    f\"Classification accuracy: {classification_accuracy:.4f} / \"\n    f\"Chance level: {chance_level}\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}