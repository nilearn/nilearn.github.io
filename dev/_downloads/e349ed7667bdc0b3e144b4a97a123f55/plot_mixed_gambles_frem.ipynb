{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# FREM on Jimura et al \"mixed gambles\" dataset\n\nIn this example, we use fast ensembling of regularized models (FREM) to\nsolve a regression problem, predicting the gain level corresponding to each\n:term:`beta<Beta>` maps regressed from mixed gambles experiment.\n:term:`FREM` uses an implicit spatial regularization through fast clustering\nand aggregates a high number\nof  estimators trained on various splits of the training set, thus returning\na very robust decoder at a lower computational cost than other spatially\nregularized methods.\n\nTo have more details, see: `frem`.\n\nSee the `dataset description <mixed_gamble_maps>`\nfor more information on the data used in this example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the data from the Jimura mixed-gamble experiment\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_mixed_gambles\n\ndata = fetch_mixed_gambles(n_subjects=16)\n\nzmap_filenames = data.zmaps\nbehavioral_target = data.gain.to_numpy().ravel()\nmask_filename = data.mask_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit FREM\nWe compare both of these models to a pipeline ensembling many models\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\n\nfrom nilearn.decoding import FREMRegressor\n\nfrem = FREMRegressor(\"svr\", cv=10, standardize=\"zscore_sample\", verbose=1)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(action=\"ignore\", category=ConvergenceWarning)\n    frem.fit(zmap_filenames, behavioral_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize FREM weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_stat_map, show\n\nplot_stat_map(\n    frem.coef_img_[\"beta\"],\n    title=\"FREM\",\n    display_mode=\"yz\",\n    cut_coords=[20, -2],\n    threshold=0.2,\n)\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that the coefficients map learnt\nby :term:`FREM` is structured,\ndue to the spatial regularity imposed by working on clusters and model\nensembling. Although these maps have been thresholded for display, they are\nnot sparse (i.e. almost all voxels have non-zero coefficients).\n\n.. seealso::\n\n  `other example\n  <sphx_glr_auto_examples_02_decoding_plot_haxby_frem.py>`\n  using FREM, and related `section of user guide <frem>`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example use of TV-L1 SpaceNet\n`SpaceNet<space_net>` is another method available in Nilearn to decode\nwith spatially sparse models. Depending on the penalty that is used,\nit yields either very structured maps (TV-L1) or unstructured maps\n(graph_net). Because of their heavy computational costs, these methods are\nnot demonstrated on this example but you can try them easily if you have a\nfew minutes. Example code is included below.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decoding import SpaceNetRegressor\n\n# We use the regressor object since the task is to predict a continuous\n# variable (gain of the gamble).\n\ntv_l1 = SpaceNetRegressor(\n    mask=mask_filename,\n    penalty=\"tv-l1\",\n    eps=1e-1,  # prefer large alphas\n    memory=\"nilearn_cache\",\n    n_jobs=2,\n)\n# tv_l1.fit(zmap_filenames, behavioral_target)\n# plot_stat_map(tv_l1.coef_img_, title=\"TV-L1\", display_mode=\"yz\",\n#               cut_coords=[20, -2])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}