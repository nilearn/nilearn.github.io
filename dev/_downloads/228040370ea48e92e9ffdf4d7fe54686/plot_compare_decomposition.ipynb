{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deriving spatial maps from group fMRI data using ICA and Dictionary Learning\n\nVarious approaches exist to derive spatial maps or networks from\ngroup fmr data. The methods extract distributed brain regions that\nexhibit similar :term:`BOLD` fluctuations over time. Decomposition\nmethods allow for generation of many independent maps simultaneously\nwithout the need to provide a priori information (e.g. seeds or priors.)\n\nThis example will apply two popular decomposition methods, :term:`ICA` and\n:term:`Dictionary learning`, to :term:`fMRI` data measured while children\nand young adults watch movies. The resulting maps will be visualized using\natlas plotting tools.\n\n:term:`CanICA` is an :term:`ICA` method\nfor group-level analysis of :term:`fMRI` data.\nCompared to other strategies, it brings a well-controlled group model,\nas well as a\nthresholding algorithm controlling for specificity and sensitivity with\nan explicit model of the signal.\n\nThe reference paper is :footcite:t:`Varoquaux2010c`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load brain development :term:`fMRI` dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_development_fmri\n\nrest_dataset = fetch_development_fmri(n_subjects=30)\nfunc_filenames = rest_dataset.func  # list of 4D nifti files for each subject\n\n# print basic information on the dataset\nprint(f\"First functional nifti image (4D) is at: {rest_dataset.func[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply :term:`CanICA` on the data\nWe use \"whole-brain-template\" as a strategy to compute the mask,\nas this leads to slightly faster and more reproducible results.\nHowever, the images need to be in :term:`MNI` template space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decomposition import CanICA\n\ncanica = CanICA(\n    n_components=20,\n    memory=\"nilearn_cache\",\n    memory_level=2,\n    verbose=10,\n    mask_strategy=\"whole-brain-template\",\n    random_state=0,\n    standardize=\"zscore_sample\",\n    n_jobs=2,\n)\ncanica.fit(func_filenames)\n\n# Retrieve the independent components in brain space. Directly\n# accessible through attribute `components_img_`.\ncanica_components_img = canica.components_img_\n# components_img is a Nifti Image object, and can be saved to a file with\n# the following lines:\nfrom pathlib import Path\n\noutput_dir = Path.cwd() / \"results\" / \"plot_compare_decomposition\"\noutput_dir.mkdir(exist_ok=True, parents=True)\nprint(f\"Output will be saved to: {output_dir}\")\ncanica_components_img.to_filename(output_dir / \"canica_resting_state.nii.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize we plot the outline of all components on one figure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_prob_atlas\n\n# Plot all ICA components together\nplot_prob_atlas(canica_components_img, title=\"All ICA components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the map for each :term:`ICA` component separately\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import iter_img\nfrom nilearn.plotting import plot_stat_map, show\n\nfor i, cur_img in enumerate(iter_img(canica_components_img)):\n    plot_stat_map(\n        cur_img,\n        display_mode=\"z\",\n        title=f\"IC {int(i)}\",\n        cut_coords=1,\n        vmax=0.05,\n        vmin=-0.05,\n        colorbar=False,\n    )\n\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare :term:`CanICA` to dictionary learning\n:term:`Dictionary learning` is a sparsity based decomposition method\nfor extracting spatial maps. It extracts maps that are naturally sparse\nand usually cleaner than :term:`ICA`. Here, we will compare networks built\nwith :term:`CanICA` to networks built with :term:`Dictionary learning`.\n\nFor more detailse see :footcite:t:`Mensch2016`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary learning estimator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decomposition import DictLearning\n\ndict_learning = DictLearning(\n    n_components=20,\n    memory=\"nilearn_cache\",\n    memory_level=2,\n    verbose=1,\n    random_state=0,\n    n_epochs=1,\n    mask_strategy=\"whole-brain-template\",\n    standardize=\"zscore_sample\",\n    n_jobs=2,\n)\n\nprint(\"[Example] Fitting dictionary learning model\")\ndict_learning.fit(func_filenames)\nprint(\"[Example] Saving results\")\n# Grab extracted components umasked back to Nifti image.\n# Note: For older versions, less than 0.4.1. components_img_\n# is not implemented. See Note section above for details.\ndictlearning_components_img = dict_learning.components_img_\ndictlearning_components_img.to_filename(\n    output_dir / \"dictionary_learning_resting_state.nii.gz\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the results\n\nFirst plot all DictLearning components together\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_prob_atlas(\n    dictlearning_components_img, title=\"All DictLearning components\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One plot of each component\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, cur_img in enumerate(iter_img(dictlearning_components_img)):\n    plot_stat_map(\n        cur_img,\n        display_mode=\"z\",\n        title=f\"Comp {int(i)}\",\n        cut_coords=1,\n        vmax=0.1,\n        vmin=-0.1,\n        colorbar=False,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimate explained variance per component and plot using matplotlib\n\nThe fitted object `dict_learning` can be used\nto calculate the score per component\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = dict_learning.score(func_filenames, per_component=True)\n\n# Plot the scores\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\nplt.figure(figsize=(4, 4), constrained_layout=True)\n\npositions = np.arange(len(scores))\nplt.barh(positions, scores)\nplt.ylabel(\"Component #\", size=12)\nplt.xlabel(\"Explained variance\", size=12)\nplt.yticks(np.arange(20))\nplt.gca().xaxis.set_major_formatter(FormatStrFormatter(\"%.3f\"))\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To see how to extract subject-level timeseries' from regions\n    created using :term:`Dictionary learning`, see `example Regions\n    extraction using dictionary learning and functional connectomes\n    <sphx_glr_auto_examples_03_connectivity\\\n    _plot_extract_regions_dictlearning_maps.py>`.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}