
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta property="og:title" content="2.1. An introduction to decoding" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://nilearn.github.io/decoding/decoding_intro.html" />
  <meta property="og:site_name" content="Nilearn" />
  <meta property="og:description" content="This section gives an introduction to the main concept of decoding: predicting from brain images. The discussion and examples are articulated on the analysis of the Haxby 2001 dataset, showing how ..." />
  <meta property="og:image" content="https://nilearn.github.io/_images/sphx_glr_plot_haxby_stimuli_004.png" />
  <meta property="og:image:alt" content="Nilearn" />
  
    <title>Nilearn: Statistical Analysis for NeuroImaging in Python &#8212; Machine learning for NeuroImaging</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/nature.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.2. Choosing the right predictive model for neuroimaging" href="estimator_choice.html" />
    <link rel="prev" title="2. Decoding and MVPA: predicting from brain images" href="index.html" />
<meta content="True" name="HandheldFriendly">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
<meta name="keywords" content="nilearn, neuroimaging, python, neuroscience, machinelearning">


<script type="text/javascript">
function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});
</script>
<script type="text/javascript">
function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});
</script>


<script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>

  </head><body>
<div id="logo-banner">
  <div class="logo">
    <a href="../index.html">
      <img src="../_static/nilearn-logo.png" alt="Nilearn logo"  border="0" />
    </a>
  </div>
  <!-- A tag cloud to make it easy for people to find what they are
                         looking for -->
 <div class="tags">
  <ul>
    <li>
      <big><a href="../auto_examples/decoding/plot_haxby_anova_svm.html">SVM</a></big>
    </li>
    <li>
      <small><a href="../connectivity/parcellating.html">Ward
          clustering</a></small>
    </li>
    <li>
      <a href="searchlight.html">Searchlight</a>
    </li>
    <li>
      <big><a href="../connectivity/resting_state_networks.html">ICA</a></big>
    </li>
    <li>
      <a href="../manipulating_images/data_preparation.html">Nifti IO</a>
    </li>
    <li>
      <a href="../modules/reference.html#module-nilearn.datasets">Datasets</a>
    </li>
  </ul>
 </div>

  <div class="banner">
    <h1>Nilearn:</h1>
    <h2>Statistics for NeuroImaging in Python</h2>
  </div>
  <div class="search_form">
    <div class="gcse-search" id="cse" style="width: 100%;"></div>
    <script>
      (function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </div>
</div>



<div class=related-wrapper>
    
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a></li>
        <li class="right" >
          <a href="estimator_choice.html" title="2.2. Choosing the right predictive model for neuroimaging"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="2. Decoding and MVPA: predicting from brain images"
             accesskey="P">previous</a> |</li>
<li><a href="../index.html">Nilearn Home</a> |&nbsp;</li>
<li><a href="../user_guide.html">User Guide</a> |&nbsp;</li>
<li><a href="../auto_examples/index.html">Examples</a> |&nbsp;</li>
<li><a href="../modules/reference.html">Reference</a> |&nbsp;</li>
<li id="navbar-about"><a href="../authors.html">About</a>|&nbsp;</li>
<li><a href="../glossary.html">Glossary</a>|&nbsp;</li>
<li><a href="../bibliography.html">Bibliography</a>|&nbsp;</li>
<li id="navbar-ecosystem"><a href="http://www.nipy.org/">Nipy ecosystem</a></li>

          <li class="nav-item nav-item-1"><a href="../user_guide.html" >User guide: table of contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="index.html" accesskey="U"><span class="section-number">2. </span>Decoding and MVPA: predicting from brain images</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Nilearn: Statistical Analysis for NeuroImaging in Python</a></li> 
      </ul>
    </div>
</div>

<div class="devel-alert-banner">
This is documentation for the <em>unstable development version</em> of Nilearn,
the current stable version is available <a href="https://nilearn.github.io/stable/index.html">here</a>.
</div>

  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="an-introduction-to-decoding">
<span id="decoding-intro"></span><h1><span class="section-number">2.1. </span>An introduction to decoding<a class="headerlink" href="#an-introduction-to-decoding" title="Permalink to this headline">¶</a></h1>
<p>This section gives an introduction to the main concept of decoding:
predicting from brain images.</p>
<p>The discussion and examples are articulated on the analysis of the Haxby
2001 dataset, showing how to predict from <a class="reference internal" href="../glossary.html#term-fMRI"><span class="xref std std-term">fMRI</span></a> images the stimuli that
the subject is viewing. However the process is the same in other settings
predicting from other brain imaging modalities, for instance predicting
phenotype or diagnostic status from <a class="reference internal" href="../glossary.html#term-VBM"><span class="xref std std-term">VBM</span></a> (Voxel Based Morphometry) maps
(as illustrated in <a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm.html#sphx-glr-auto-examples-02-decoding-plot-oasis-vbm-py"><span class="std std-ref">a more complex example</span></a>), or from FA maps
to capture diffusion mapping.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This documentation only aims at explaining the necessary concepts and common
pitfalls of decoding analysis. For an introduction on the code to use please
refer to : <a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<div class="contents local topic" id="contents">
<p class="topic-title"><strong>Contents</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#loading-and-preparing-the-data" id="id6">Loading and preparing the data</a></p></li>
<li><p><a class="reference internal" href="#performing-a-simple-decoding-analysis" id="id7">Performing a simple decoding analysis</a></p></li>
<li><p><a class="reference internal" href="#decoding-without-a-mask-anova-svm" id="id8">Decoding without a mask: Anova-SVM</a></p></li>
</ul>
</div>
<section id="loading-and-preparing-the-data">
<h2><a class="toc-backref" href="#id6"><span class="section-number">2.1.1. </span>Loading and preparing the data</a><a class="headerlink" href="#loading-and-preparing-the-data" title="Permalink to this headline">¶</a></h2>
<section id="the-haxby-2001-experiment">
<h3><span class="section-number">2.1.1.1. </span>The Haxby 2001 experiment<a class="headerlink" href="#the-haxby-2001-experiment" title="Permalink to this headline">¶</a></h3>
<p>In the Haxby experiment, subjects were presented visual stimuli from
different categories. We are going to predict which category the subject is
seeing from the <a class="reference internal" href="../glossary.html#term-fMRI"><span class="xref std std-term">fMRI</span></a> activity recorded in regions of the ventral visual system.
Significant prediction shows that the signal in the region contains
information on the corresponding category.</p>
<figure class="align-left" id="id1">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_stimuli.html"><img alt="../_images/sphx_glr_plot_haxby_stimuli_004.png" src="../_images/sphx_glr_plot_haxby_stimuli_004.png" style="width: 192.0px; height: 144.0px;" /></a>
<figcaption>
<p><span class="caption-text">Face stimuli</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-left" id="id2">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_stimuli.html"><img alt="../_images/sphx_glr_plot_haxby_stimuli_002.png" src="../_images/sphx_glr_plot_haxby_stimuli_002.png" style="width: 192.0px; height: 144.0px;" /></a>
<figcaption>
<p><span class="caption-text">Cat stimuli</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-left" id="id3">
<a class="reference external image-reference" href="../auto_examples/01_plotting/plot_haxby_masks.html"><img alt="../_images/sphx_glr_plot_haxby_masks_001.png" src="../_images/sphx_glr_plot_haxby_masks_001.png" style="width: 120.0px; height: 162.0px;" /></a>
<figcaption>
<p><span class="caption-text">Masks</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-left" id="id4">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html"><img alt="../_images/sphx_glr_plot_haxby_full_analysis_001.png" src="../_images/sphx_glr_plot_haxby_full_analysis_001.png" style="width: 224.0px; height: 168.0px;" /></a>
<figcaption>
<p><span class="caption-text">Decoding scores per mask</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<div class="topic">
<p class="topic-title"><strong>fMRI: using beta maps of a first-level analysis</strong></p>
<p>The Haxby experiment is unusual because the experimental paradigm is
made of many blocks of continuous stimulation. Most cognitive
experiments have a more complex temporal structure with rich sequences
of events
(<a class="reference internal" href="../manipulating_images/input_output.html#loading-data"><span class="std std-ref">more on data input</span></a>).</p>
<p>The standard approach to decoding consists in fitting a first-level
<a class="reference internal" href="../glm/glm_intro.html#glm-intro"><span class="std std-ref">general linear model (or GLM)</span></a> to retrieve one response
map (a beta map) per trial as shown in
<a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_glm_decoding.html#sphx-glr-auto-examples-02-decoding-plot-haxby-glm-decoding-py"><span class="std std-ref">Decoding of a dataset after GLM fit for signal extraction</span></a>.
This is sometimes known as “beta-series regressions” (see Mumford et al,
<em>Deconvolving bold activation in event-related designs for multivoxel
pattern classification analyses</em>, NeuroImage 2012). These maps can
then be input to the decoder as below, predicting the conditions
associated to trial.</p>
<p>For simplicity, we will work on the raw time-series of the data.
However, <strong>it is strongly recommended that you fit a first-level model to
include an hemodynamic response function (HRF) model and isolate the
responses from various confounds</strong> as demonstrated in <a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_glm_decoding.html#sphx-glr-auto-examples-02-decoding-plot-haxby-glm-decoding-py"><span class="std std-ref">a more advanced example</span></a>.</p>
</div>
</section>
<section id="loading-the-data-into-nilearn">
<h3><span class="section-number">2.1.1.2. </span>Loading the data into nilearn<a class="headerlink" href="#loading-the-data-into-nilearn" title="Permalink to this headline">¶</a></h3>
<div class="topic">
<p class="topic-title"><strong>Full code example</strong></p>
<p>The documentation here just gives the big idea. A full code example,
with explanation, can be found on
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<ul class="simple">
<li><p><strong>Starting an environment</strong>: Launch IPython via “ipython –matplotlib”
in a terminal, or use the Jupyter notebook.</p></li>
<li><p><strong>Retrieving the data</strong>: In the tutorial, we load the data using nilearn
data downloading function, <a class="reference internal" href="../modules/generated/nilearn.datasets.fetch_haxby.html#nilearn.datasets.fetch_haxby" title="nilearn.datasets.fetch_haxby"><code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.datasets.fetch_haxby</span></code></a>.
However, all this function does is to download the data and return
paths to the files downloaded on the disk. To input your own data to
nilearn, you can pass in the path to your own files
(<a class="reference internal" href="../manipulating_images/input_output.html#loading-data"><span class="std std-ref">more on data input</span></a>).</p></li>
<li><p><strong>Masking fMRI data</strong>: To perform the analysis on some <a class="reference internal" href="../glossary.html#term-voxel"><span class="xref std std-term">voxels</span></a> only, we will
provide a spatial mask of <a class="reference internal" href="../glossary.html#term-voxel"><span class="xref std std-term">voxels</span></a> to keep, which is provided with the dataset
(here <cite>mask_vt</cite> a mask of the ventral temporal cortex that comes with data).</p></li>
<li><p><strong>Loading the behavioral labels</strong>: Behavioral information is often stored
in a text file such as a CSV, and must be load with
<strong>numpy.recfromcsv</strong> or <a class="reference external" href="http://pandas.pydata.org/">pandas</a></p></li>
<li><p><strong>Sample mask</strong>: Masking some of the time points
may be useful to
restrict to a specific pair of conditions (<em>eg</em> cats versus faces).</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../building_blocks/manual_pipeline.html#masking"><span class="std std-ref">Masking the data: from 4D image to 2D array</span></a>
To better control this process of spatial masking and add additional signal
processing steps (smoothing, filtering, standardizing…), we could
explicitly define a masker :  <a class="reference internal" href="../modules/generated/nilearn.maskers.NiftiMasker.html#nilearn.maskers.NiftiMasker" title="nilearn.maskers.NiftiMasker"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.maskers.NiftiMasker</span></code></a>.
This object extracts <a class="reference internal" href="../glossary.html#term-voxel"><span class="xref std std-term">voxels</span></a> belonging to a given spatial mask and converts
their signal to a 2D data matrix with a shape (n_timepoints, n_voxels)
(see <a class="reference internal" href="../manipulating_images/manipulating_images.html#mask-4d-2-3d"><span class="std std-ref">Masking data: from 4D Nifti images to 2D data arrays</span></a> for a discussion on using</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Seemingly minor data preparation can matter a lot on the final score,
for instance standardizing the data.</p>
</div>
</section>
</section>
<section id="performing-a-simple-decoding-analysis">
<h2><a class="toc-backref" href="#id7"><span class="section-number">2.1.2. </span>Performing a simple decoding analysis</a><a class="headerlink" href="#performing-a-simple-decoding-analysis" title="Permalink to this headline">¶</a></h2>
<section id="a-few-definitions">
<h3><span class="section-number">2.1.2.1. </span>A few definitions<a class="headerlink" href="#a-few-definitions" title="Permalink to this headline">¶</a></h3>
<p>When doing predictive analysis you train an estimator to predict a variable of
interest to you. Or in other words to predict a condition label <strong>y</strong> given a
set <strong>X</strong> of imaging data.</p>
<p>This is always done in at least two steps:</p>
<ul class="simple">
<li><p>first a <cite>fit</cite> during which we “learn” the parameters of the model that make
good predictions. This is done on some “training data” or “training set”.</p></li>
<li><p>then a <cite>predict</cite> step where the “fitted” model is used to make prediction
on new data. Here, we just have to give the new set of images (as the target
should be unknown). These are called “test data” or “test set”.</p></li>
</ul>
<p>All objects used to make prediction in Nilearn will at least have functions for
these steps : a <cite>fit</cite> function and a <cite>predict</cite> function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Do not predict on data used by the fit: this would yield misleadingly
optimistic scores.</strong></p>
</div>
</section>
<section id="a-first-estimator">
<h3><span class="section-number">2.1.2.2. </span>A first estimator<a class="headerlink" href="#a-first-estimator" title="Permalink to this headline">¶</a></h3>
<p>To perform decoding, we need a model that can learn some relations
between <strong>X</strong> (the imaging data) and <strong>y</strong> the condition label. As a default,
Nilearn uses <a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html">Support Vector Classifier</a> (or SVC) with a
linear kernel. This is a simple yet performant choice that works in a wide
variety of problems.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html">The scikit-learn documentation on SVMs</a></p>
</div>
</section>
<section id="decoding-made-easy">
<h3><span class="section-number">2.1.2.3. </span>Decoding made easy<a class="headerlink" href="#decoding-made-easy" title="Permalink to this headline">¶</a></h3>
<p>Nilearn makes it easy to train a model with a principled pipeline using the
<a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> object. Using the mask we defined before
and an SVC estimator as we already introduced, we can create a pipeline in
two lines. The additional <cite>standardize=True</cite> argument adds a normalization
of images signal to a zero mean and unit variance, which will improve
performance of most estimators.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nilearn.decoding</span> <span class="kn">import</span> <span class="n">Decoder</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask_filename</span><span class="p">)</span> 
</pre></div>
</div>
<p>Then we can fit it on the images and the conditions we chose before.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">decoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">fmri_niimgs</span><span class="p">,</span> <span class="n">conditions</span><span class="p">)</span> 
</pre></div>
</div>
<p>This decoder can now be used to predict conditions for new images !
Be careful though, as we warned you, predicting on images that were used to
<cite>fit</cite> your model should never be done.</p>
</section>
<section id="measuring-prediction-performance">
<h3><span class="section-number">2.1.2.4. </span>Measuring prediction performance<a class="headerlink" href="#measuring-prediction-performance" title="Permalink to this headline">¶</a></h3>
<p>One of the most common interests of decoding is to measure how well we can learn
to predict various targets from our images to have a sense of which information
is really contained in a given region of the brain. To do this, we need ways to
measure the errors we make when we do prediction.</p>
<section id="cross-validation">
<h4><span class="section-number">2.1.2.4.1. </span>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h4>
<p>We cannot measure prediction error on the same set of data that we have
used to fit the estimator: it would be much easier than on new data, and
the result would be meaningless. We need to use a technique called
<em>cross-validation</em> to split the data into different sets, we can then <cite>fit</cite> our
estimator on some set and measure an unbiased error on another set.</p>
<p>The easiest way to do cross-validation is the <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation">K-Fold strategy</a>.
If you do 5-fold cross-validation manually, you split your data in 5 folds,
use 4 folds to <cite>fit</cite> your estimator, and 1 to <cite>predict</cite> and measure the errors
made by your estimators. You repeat this for every combination of folds, and get
5 prediction “scores”, one for each fold.</p>
<p>During the <cite>fit</cite>, <a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> object implicitly used a
cross-validation: Stratified K-fold by default. You can easily inspect
the prediction “score” it got in each fold.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">cv_scores_</span><span class="p">)</span> 
</pre></div>
</div>
</section>
<section id="choosing-a-good-cross-validation-strategy">
<h4><span class="section-number">2.1.2.4.2. </span>Choosing a good cross-validation strategy<a class="headerlink" href="#choosing-a-good-cross-validation-strategy" title="Permalink to this headline">¶</a></h4>
<p>There are many cross-validation strategies possible, including K-Fold or
leave-one-out. When choosing a strategy, keep in mind that the test set should
be as little correlated as possible with the train set and have enough samples
to enable a good measure the prediction error (at least 10-20% of the data as a
rule of thumb).</p>
<p>As a general advice :</p>
<ul class="simple">
<li><p>To train a decoder on one subject data, try to leave at least one session
out to have an independent test.</p></li>
<li><p>To train a decoder across different subject data, leaving some subjects data
out is often a good option.</p></li>
<li><p>In any case leaving only one image as test set (leave-one-out) is often
the worst option (see Varoquaux et al, <em>Assessing and tuning brain decoders:
cross-validation, caveats, and guidelines</em>, Neuroimage 2017).</p></li>
</ul>
<p>To improve our first pipeline for the Haxby example, we can leave one entire
session out. To do this, we can pass a <cite>LeaveOneGroupOut</cite> cross-validation
object from scikit-learn to our <cite>Decoder</cite>. Fitting it with the information of
groups=`session_labels` will use one session as test set.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Full code example can be found at :
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
</section>
<section id="choice-of-the-prediction-accuracy-measure">
<h4><span class="section-number">2.1.2.4.3. </span>Choice of the prediction accuracy measure<a class="headerlink" href="#choice-of-the-prediction-accuracy-measure" title="Permalink to this headline">¶</a></h4>
<p>Once you have a prediction about new data and its real label (the <em>ground truth</em>)
there are different ways to measure a <em>score</em> that summarizes its performance.</p>
<p>The default metric used for measuring errors is the accuracy score, i.e.
the number of total errors. It is not always a sensible metric,
especially in the case of very imbalanced classes, as in such situations
choosing the dominant class can achieve a low number of errors.</p>
<p>Other metrics, such as the <a class="reference internal" href="../glossary.html#term-AUC"><span class="xref std std-term">AUC</span></a> (Area Under the Curve, for the
<a class="reference internal" href="../glossary.html#term-ROC"><span class="xref std std-term">ROC</span></a>: the Receiver Operating Characteristic), can be used through the
<cite>scoring</cite> argument of <a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a>.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>the <a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values">list of scoring options</a></p>
</div>
</section>
<section id="prediction-accuracy-at-chance-using-simple-strategies">
<h4><span class="section-number">2.1.2.4.4. </span>Prediction accuracy at chance using simple strategies<a class="headerlink" href="#prediction-accuracy-at-chance-using-simple-strategies" title="Permalink to this headline">¶</a></h4>
<p>When performing decoding, prediction performance of a model can be checked against
null distributions or random predictions. For this, we guess a chance level score using
simple strategies while predicting condition <strong>y</strong> with <strong>X</strong> imaging data.</p>
<p>In Nilearn, we wrap
<a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators">Dummy estimators</a>
into the <a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> that
can be readily used to estimate this chance level score with the same model parameters
that was previously used for real predictions. This allows us to compare whether the
model is better than chance or not.</p>
<div class="topic">
<p class="topic-title"><strong>Putting it all together</strong></p>
<p>The <a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html#sphx-glr-auto-examples-02-decoding-plot-haxby-full-analysis-py"><span class="std std-ref">ROI-based decoding example</span></a>
does a decoding analysis per mask, giving the f1-score and chance score of
the prediction for each object.</p>
<p>It uses all the notions presented above, with <code class="docutils literal notranslate"><span class="pre">for</span></code> loop to iterate
over masks and categories and Python dictionaries to store the
scores.</p>
</div>
<figure class="align-left" id="id5">
<a class="reference external image-reference" href="../auto_examples/01_plotting/plot_haxby_masks.html"><img alt="../_images/sphx_glr_plot_haxby_masks_001.png" src="../_images/sphx_glr_plot_haxby_masks_001.png" style="width: 220.00000000000003px; height: 297.0px;" /></a>
<figcaption>
<p><span class="caption-text">Masks</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-left">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html"><img alt="../_images/sphx_glr_plot_haxby_full_analysis_001.png" src="../_images/sphx_glr_plot_haxby_full_analysis_001.png" style="width: 448.0px; height: 336.0px;" /></a>
</figure>
</section>
</section>
<section id="visualizing-the-decoder-s-weights">
<h3><span class="section-number">2.1.2.5. </span>Visualizing the decoder’s weights<a class="headerlink" href="#visualizing-the-decoder-s-weights" title="Permalink to this headline">¶</a></h3>
<p>During <cite>fit</cite> step, the <a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> object retains the
coefficients of best models for each class in <cite>decoder.coef_img_</cite>.</p>
<figure class="align-default">
<a class="reference external image-reference" href="../auto_examples/plot_decoding_tutorial.html"><img alt="../_images/sphx_glr_plot_haxby_anova_svm_001.png" src="../_images/sphx_glr_plot_haxby_anova_svm_001.png" style="width: 474.5px; height: 169.0px;" /></a>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Full code for the above can be found on
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../plotting/index.html#plotting"><span class="std std-ref">Plotting brain images</span></a></p></li>
</ul>
</div>
</section>
</section>
<section id="decoding-without-a-mask-anova-svm">
<h2><a class="toc-backref" href="#id8"><span class="section-number">2.1.3. </span>Decoding without a mask: Anova-SVM</a><a class="headerlink" href="#decoding-without-a-mask-anova-svm" title="Permalink to this headline">¶</a></h2>
<section id="dimension-reduction-with-feature-selection">
<h3><span class="section-number">2.1.3.1. </span>Dimension reduction with feature selection<a class="headerlink" href="#dimension-reduction-with-feature-selection" title="Permalink to this headline">¶</a></h3>
<p>If we do not start from a mask of the relevant regions, there is a very
large number of voxels and not all are useful for
face vs cat prediction. We thus add a <a class="reference external" href="http://scikit-learn.org/stable/modules/feature_selection.html">feature selection</a>
procedure. The idea is to select the <a class="reference external" href="https://en.wikipedia.org/wiki/Analysis_of_variance#The_F-test">k`% voxels most correlated to the
task through a simple F-score based feature selection (a.k.a.
`Anova</a>)</p>
<p>You can directly choose to keep only a certain percentage of voxels in the
<a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> object through the <cite>screening_percentile</cite>
argument. To keep the 10% most correlated voxels, just create us this parameter :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nilearn.decoding</span> <span class="kn">import</span> <span class="n">Decoder</span>
<span class="c1"># Here screening_percentile is set to 5 percent</span>
<span class="n">mask_img</span> <span class="o">=</span> <span class="n">haxby_dataset</span><span class="o">.</span><span class="n">mask</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask_img</span><span class="p">,</span> <span class="n">smoothing_fwhm</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                  <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">screening_percentile</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>

<span class="c1">#############################################################################</span>
<span class="c1"># Fit the decoder and predict</span>
<span class="c1"># ----------------------------</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">func_img</span><span class="p">,</span> <span class="n">conditions</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">func_img</span><span class="p">)</span>

<span class="c1">#############################################################################</span>
<span class="c1"># Obtain prediction scores via cross validation</span>
<span class="c1"># -----------------------------------------------</span>
<span class="c1"># Define the cross-validation scheme used for validation. Here we use a</span>
<span class="c1"># LeaveOneGroupOut cross-validation on the session group which corresponds to a</span>
<span class="c1"># leave a session out scheme, then pass the cross-validator object to the cv</span>
<span class="c1"># parameter of decoder.leave-one-session-out For more details please take a</span>
<span class="c1"># look at:</span>
<span class="c1"># &lt;https://nilearn.github.io/auto_examples/plot_decoding_tutorial.html#measuring-prediction-scores-using-cross-validation&gt;</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneGroupOut</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">LeaveOneGroupOut</span><span class="p">()</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask_img</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">screening_percentile</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="c1"># Compute the prediction accuracy for the different folds (i.e. session)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">func_img</span><span class="p">,</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">session_label</span><span class="p">)</span>

<span class="c1"># Print the CV scores</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">cv_scores_</span><span class="p">[</span><span class="s1">&#39;face&#39;</span><span class="p">])</span>

<span class="c1">#############################################################################</span>
</pre></div>
</div>
</section>
<section id="visualizing-the-results">
<h3><span class="section-number">2.1.3.2. </span>Visualizing the results<a class="headerlink" href="#visualizing-the-results" title="Permalink to this headline">¶</a></h3>
<p>To visualize the results, <a class="reference internal" href="../modules/generated/nilearn.decoding.Decoder.html#nilearn.decoding.Decoder" title="nilearn.decoding.Decoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.decoding.Decoder</span></code></a> handles two main steps for you :</p>
<ul class="simple">
<li><p>first get the support vectors of the SVC and inverse the feature selection mechanism</p></li>
<li><p>then, inverse the masking process to link weights to their spatial
position and plot</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ----------------------</span>
<span class="c1"># Look at the SVC&#39;s discriminating weights using</span>
<span class="c1"># :class:`nilearn.plotting.plot_stat_map`</span>
<span class="n">weight_img</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">coef_img_</span><span class="p">[</span><span class="s1">&#39;face&#39;</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_stat_map</span><span class="p">,</span> <span class="n">show</span>
<span class="n">plot_stat_map</span><span class="p">(</span><span class="n">weight_img</span><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><span class="n">haxby_dataset</span><span class="o">.</span><span class="n">anat</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;SVM weights&#39;</span><span class="p">)</span>

<span class="n">show</span><span class="p">()</span>
<span class="c1">#############################################################################</span>
<span class="c1"># Or we can plot the weights using :class:`nilearn.plotting.view_img` as a</span>
<span class="c1"># dynamic html viewer</span>
<span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">view_img</span>
<span class="n">view_img</span><span class="p">(</span><span class="n">weight_img</span><span class="p">,</span> <span class="n">bg_img</span><span class="o">=</span><span class="n">haxby_dataset</span><span class="o">.</span><span class="n">anat</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
         <span class="n">title</span><span class="o">=</span><span class="s2">&quot;SVM weights&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#############################################################################</span>
</pre></div>
</div>
<figure class="align-default">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html"><img alt="../_images/sphx_glr_plot_haxby_anova_svm_001.png" src="../_images/sphx_glr_plot_haxby_anova_svm_001.png" style="width: 474.5px; height: 169.0px;" /></a>
</figure>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../plotting/index.html#plotting"><span class="std std-ref">Plotting brain images</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title"><strong>Final script</strong></p>
<p>The complete script to do an SVM-Anova analysis can be found as
<a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html#sphx-glr-auto-examples-02-decoding-plot-haxby-anova-svm-py"><span class="std std-ref">an example</span></a>.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="frem.html#frem"><span class="std std-ref">FREM: fast ensembling of regularized models for robust decoding</span></a></p></li>
<li><p><a class="reference internal" href="space_net.html#space-net"><span class="std std-ref">SpaceNet: decoding with spatial structure for better maps</span></a></p></li>
<li><p><a class="reference internal" href="searchlight.html#searchlight"><span class="std std-ref">Searchlight : finding voxels containing information</span></a></p></li>
</ul>
</div>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">


<h4> Giving credit </h4>
  <ul class="simple">
    <li><p>Please consider <a href="../authors.html#citing">citing the
                    papers</a>.</p></li>
  </ul>

  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">2.1. An introduction to decoding</a><ul>
<li><a class="reference internal" href="#loading-and-preparing-the-data">2.1.1. Loading and preparing the data</a><ul>
<li><a class="reference internal" href="#the-haxby-2001-experiment">2.1.1.1. The Haxby 2001 experiment</a></li>
<li><a class="reference internal" href="#loading-the-data-into-nilearn">2.1.1.2. Loading the data into nilearn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performing-a-simple-decoding-analysis">2.1.2. Performing a simple decoding analysis</a><ul>
<li><a class="reference internal" href="#a-few-definitions">2.1.2.1. A few definitions</a></li>
<li><a class="reference internal" href="#a-first-estimator">2.1.2.2. A first estimator</a></li>
<li><a class="reference internal" href="#decoding-made-easy">2.1.2.3. Decoding made easy</a></li>
<li><a class="reference internal" href="#measuring-prediction-performance">2.1.2.4. Measuring prediction performance</a><ul>
<li><a class="reference internal" href="#cross-validation">2.1.2.4.1. Cross-validation</a></li>
<li><a class="reference internal" href="#choosing-a-good-cross-validation-strategy">2.1.2.4.2. Choosing a good cross-validation strategy</a></li>
<li><a class="reference internal" href="#choice-of-the-prediction-accuracy-measure">2.1.2.4.3. Choice of the prediction accuracy measure</a></li>
<li><a class="reference internal" href="#prediction-accuracy-at-chance-using-simple-strategies">2.1.2.4.4. Prediction accuracy at chance using simple strategies</a></li>
</ul>
</li>
<li><a class="reference internal" href="#visualizing-the-decoder-s-weights">2.1.2.5. Visualizing the decoder’s weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decoding-without-a-mask-anova-svm">2.1.3. Decoding without a mask: Anova-SVM</a><ul>
<li><a class="reference internal" href="#dimension-reduction-with-feature-selection">2.1.3.1. Dimension reduction with feature selection</a></li>
<li><a class="reference internal" href="#visualizing-the-results">2.1.3.2. Visualizing the results</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter"><span class="section-number">2. </span>Decoding and MVPA: predicting from brain images</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="estimator_choice.html"
                        title="next chapter"><span class="section-number">2.2. </span>Choosing the right predictive model for neuroimaging</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
            &copy; The nilearn developers 2010-2022.
          Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 4.2.0.
        <span style="padding-left: 5ex;">
          <a href="../_sources/decoding/decoding_intro.rst.txt"
        	 rel="nofollow">Show this page source</a>
        </span>
    </div>
  </body>
</html>