
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/04_glm_first_level/plot_localizer_surface_analysis.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py:


Example of surface-based first-level analysis
=============================================

A full step-by-step example of fitting a GLM to experimental data sampled on
the cortical surface and visualizing the results.

More specifically:

1. A sequence of fMRI volumes is loaded.
2. fMRI data are projected onto a reference cortical surface (the FreeSurfer
   template, fsaverage).
3. A design matrix describing all the effects related to the data is computed.
4. A GLM is applied to the dataset
   (effect/covariance, then contrast estimation).

The result of the analysis are statistical maps that are defined on the brain
mesh. We display them using Nilearn capabilities.

The projection of fMRI data onto a given brain mesh requires that both are
initially defined in the same space.

* The functional data should be coregistered to the anatomy from which the mesh
  was obtained.

* Another possibility, used here, is to project the normalized fMRI data to an
  MNI-coregistered mesh, such as fsaverage.

The advantage of this second approach is that it makes it easy to run
second-level analyses on the surface. On the other hand, it is obviously less
accurate than using a subject-tailored mesh.

.. GENERATED FROM PYTHON SOURCE LINES 36-39

Prepare data and analysis parameters
------------------------------------
Prepare the timing parameters.

.. GENERATED FROM PYTHON SOURCE LINES 39-42

.. code-block:: default

    t_r = 2.4
    slice_time_ref = 0.5








.. GENERATED FROM PYTHON SOURCE LINES 43-45

Prepare the data.
First, the volume-based fMRI data.

.. GENERATED FROM PYTHON SOURCE LINES 45-50

.. code-block:: default

    from nilearn.datasets import fetch_localizer_first_level

    data = fetch_localizer_first_level()
    fmri_img = data.epi_img








.. GENERATED FROM PYTHON SOURCE LINES 51-52

Second, the experimental paradigm.

.. GENERATED FROM PYTHON SOURCE LINES 52-57

.. code-block:: default

    import pandas as pd

    events_file = data.events
    events = pd.read_table(events_file)








.. GENERATED FROM PYTHON SOURCE LINES 58-64

Project the fMRI image to the surface
-------------------------------------

For this we need to get a mesh representing the geometry of the surface. We
could use an individual mesh, but we first resort to a standard mesh, the
so-called fsaverage5 template from the FreeSurfer software.

.. GENERATED FROM PYTHON SOURCE LINES 64-68

.. code-block:: default

    import nilearn

    fsaverage = nilearn.datasets.fetch_surf_fsaverage()








.. GENERATED FROM PYTHON SOURCE LINES 69-71

The projection function simply takes the fMRI data and the mesh.
Note that those correspond spatially, are they are both in MNI space.

.. GENERATED FROM PYTHON SOURCE LINES 71-75

.. code-block:: default

    from nilearn import surface

    texture = surface.vol_to_surf(fmri_img, fsaverage.pial_right)








.. GENERATED FROM PYTHON SOURCE LINES 76-81

Perform first level analysis
----------------------------

This involves computing the design matrix and fitting the model.
We start by specifying the timing of fMRI frames.

.. GENERATED FROM PYTHON SOURCE LINES 81-86

.. code-block:: default

    import numpy as np

    n_scans = texture.shape[1]
    frame_times = t_r * (np.arange(n_scans) + .5)








.. GENERATED FROM PYTHON SOURCE LINES 87-91

Create the design matrix.

We specify an hrf model containing the Glover model and its time derivative
The drift model is implicitly a cosine basis with a period cutoff at 128s.

.. GENERATED FROM PYTHON SOURCE LINES 91-98

.. code-block:: default

    from nilearn.glm.first_level import make_first_level_design_matrix

    design_matrix = make_first_level_design_matrix(frame_times,
                                                   events=events,
                                                   hrf_model='glover + derivative'
                                                   )








.. GENERATED FROM PYTHON SOURCE LINES 99-105

Setup and fit GLM.

Note that the output consists in 2 variables: `labels` and `fit`.
`labels` tags voxels according to noise autocorrelation.
`estimates` contains the parameter estimates.
We keep them for later contrast computation.

.. GENERATED FROM PYTHON SOURCE LINES 105-109

.. code-block:: default

    from nilearn.glm.first_level import run_glm

    labels, estimates = run_glm(texture.T, design_matrix.values)








.. GENERATED FROM PYTHON SOURCE LINES 110-116

Estimate contrasts
------------------
Specify the contrasts.

For practical purpose, we first generate an identity matrix whose size is
the number of columns of the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 116-118

.. code-block:: default

    contrast_matrix = np.eye(design_matrix.shape[1])








.. GENERATED FROM PYTHON SOURCE LINES 119-120

At first, we create basic contrasts.

.. GENERATED FROM PYTHON SOURCE LINES 120-123

.. code-block:: default

    basic_contrasts = dict([(column, contrast_matrix[i])
                            for i, column in enumerate(design_matrix.columns)])








.. GENERATED FROM PYTHON SOURCE LINES 124-126

Next, we add some intermediate contrasts and
one contrast adding all conditions with some auditory parts.

.. GENERATED FROM PYTHON SOURCE LINES 126-147

.. code-block:: default

    basic_contrasts['audio'] = (
        basic_contrasts['audio_left_hand_button_press']
        + basic_contrasts['audio_right_hand_button_press']
        + basic_contrasts['audio_computation']
        + basic_contrasts['sentence_listening'])

    # one contrast adding all conditions involving instructions reading
    basic_contrasts['visual'] = (
        basic_contrasts['visual_left_hand_button_press']
        + basic_contrasts['visual_right_hand_button_press']
        + basic_contrasts['visual_computation']
        + basic_contrasts['sentence_reading'])

    # one contrast adding all conditions involving computation
    basic_contrasts['computation'] = (basic_contrasts['visual_computation']
                                      + basic_contrasts['audio_computation'])

    # one contrast adding all conditions involving sentences
    basic_contrasts['sentences'] = (basic_contrasts['sentence_listening']
                                    + basic_contrasts['sentence_reading'])








.. GENERATED FROM PYTHON SOURCE LINES 148-160

Finally, we create a dictionary of more relevant contrasts

* 'left - right button press': probes motor activity
  in left versus right button presses.
* 'audio - visual': probes the difference of activity between listening
  to some content or reading the same type of content
  (instructions, stories).
* 'computation - sentences': looks at the activity
  when performing a mental computation task  versus simply reading sentences.

Of course, we could define other contrasts,
but we keep only 3 for simplicity.

.. GENERATED FROM PYTHON SOURCE LINES 160-175

.. code-block:: default


    contrasts = {
        'left - right button press': (
            basic_contrasts['audio_left_hand_button_press']
            - basic_contrasts['audio_right_hand_button_press']
            + basic_contrasts['visual_left_hand_button_press']
            - basic_contrasts['visual_right_hand_button_press']
        ),
        'audio - visual': basic_contrasts['audio'] - basic_contrasts['visual'],
        'computation - sentences': (
            basic_contrasts['computation']
            - basic_contrasts['sentences']
        )
    }








.. GENERATED FROM PYTHON SOURCE LINES 176-177

Let's estimate the contrasts by iterating over them.

.. GENERATED FROM PYTHON SOURCE LINES 177-196

.. code-block:: default

    from nilearn import plotting
    from nilearn.glm.contrasts import compute_contrast

    for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):
        print(f"  Contrast {index + 1:1} out of {len(contrasts)}: "
              f"{contrast_id}, right hemisphere")
        # compute contrast-related statistics
        contrast = compute_contrast(labels, estimates, contrast_val,
                                    contrast_type='t')
        # we present the Z-transform of the t map
        z_score = contrast.z_score()
        # we plot it on the surface, on the inflated fsaverage mesh,
        # together with a suitable background to give an impression
        # of the cortex folding.
        plotting.plot_surf_stat_map(
            fsaverage.infl_right, z_score, hemi='right',
            title=contrast_id, colorbar=True,
            threshold=3., bg_map=fsaverage.sulc_right)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_001.png
         :alt: left - right button press
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_002.png
         :alt: audio - visual
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_003.png
         :alt: computation - sentences
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_003.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      Contrast 1 out of 3: left - right button press, right hemisphere
      Contrast 2 out of 3: audio - visual, right hemisphere
      Contrast 3 out of 3: computation - sentences, right hemisphere




.. GENERATED FROM PYTHON SOURCE LINES 197-202

Analysing the left hemisphere
-----------------------------

Note that re-creating the above analysis for the left hemisphere requires
little additional code!

.. GENERATED FROM PYTHON SOURCE LINES 204-205

We project the fMRI data to the mesh.

.. GENERATED FROM PYTHON SOURCE LINES 205-207

.. code-block:: default

    texture = surface.vol_to_surf(fmri_img, fsaverage.pial_left)








.. GENERATED FROM PYTHON SOURCE LINES 208-209

Then we estimate the General Linear Model.

.. GENERATED FROM PYTHON SOURCE LINES 209-211

.. code-block:: default

    labels, estimates = run_glm(texture.T, design_matrix.values)








.. GENERATED FROM PYTHON SOURCE LINES 212-213

Finally, we create contrast-specific maps and plot them.

.. GENERATED FROM PYTHON SOURCE LINES 213-227

.. code-block:: default

    for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):
        print(f"  Contrast {index + 1:1} out of {len(contrasts)}: "
              f"{contrast_id}, left hemisphere")
        # compute contrasts
        contrast = compute_contrast(labels, estimates, contrast_val,
                                    contrast_type='t')
        z_score = contrast.z_score()
        # plot the result
        plotting.plot_surf_stat_map(
            fsaverage.infl_left, z_score, hemi='left',
            title=contrast_id, colorbar=True,
            threshold=3., bg_map=fsaverage.sulc_left)

    plotting.show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_004.png
         :alt: left - right button press
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_005.png
         :alt: audio - visual
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_006.png
         :alt: computation - sentences
         :srcset: /auto_examples/04_glm_first_level/images/sphx_glr_plot_localizer_surface_analysis_006.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      Contrast 1 out of 3: left - right button press, left hemisphere
      Contrast 2 out of 3: audio - visual, left hemisphere
      Contrast 3 out of 3: computation - sentences, left hemisphere





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 17.733 seconds)

**Estimated memory usage:**  209 MB


.. _sphx_glr_download_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/0.10.2?urlpath=lab/tree/notebooks/auto_examples/04_glm_first_level/plot_localizer_surface_analysis.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_localizer_surface_analysis.py <plot_localizer_surface_analysis.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_localizer_surface_analysis.ipynb <plot_localizer_surface_analysis.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
