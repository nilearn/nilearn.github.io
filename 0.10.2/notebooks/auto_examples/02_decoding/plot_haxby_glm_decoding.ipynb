{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decoding of a dataset after GLM fit for signal extraction\n\nFull step-by-step example of fitting a GLM to perform a decoding experiment.\nWe use the data from one subject of the Haxby dataset.\n\nMore specifically:\n\n1. Download the Haxby dataset.\n2. Extract the information to generate a glm\n   representing the blocks of stimuli.\n3. Analyze the decoding performance using a classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch example Haxby dataset\nWe download the Haxby dataset\nThis is a study of visual object category representation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# By default 2nd subject will be fetched\nimport numpy as np\nimport pandas as pd\n\nfrom nilearn import datasets\n\nhaxby_dataset = datasets.fetch_haxby()\n\n# repetition has to be known\nTR = 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the behavioral data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load target information as string and give a numerical identifier to each\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\nconditions = behavioral[\"labels\"].values\n\n# Record these as an array of sessions\nsessions = behavioral[\"chunks\"].values\nunique_sessions = behavioral[\"chunks\"].unique()\n\n# fMRI data: a unique file for each session\nfunc_filename = haxby_dataset.func[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a proper event structure for each session\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = {}\n# events will take  the form of a dictionary of Dataframes, one per session\nfor session in unique_sessions:\n    # get the condition label per session\n    conditions_session = conditions[sessions == session]\n    # get the number of scans per session, then the corresponding\n    # vector of frame times\n    n_scans = len(conditions_session)\n    frame_times = TR * np.arange(n_scans)\n    # each event last the full TR\n    duration = TR * np.ones(n_scans)\n    # Define the events object\n    events_ = pd.DataFrame(\n        {\n            \"onset\": frame_times,\n            \"trial_type\": conditions_session,\n            \"duration\": duration,\n        }\n    )\n    # remove the rest condition and insert into the dictionary\n    events[session] = events_[events_.trial_type != \"rest\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate and run FirstLevelModel\n\nWe generate a list of z-maps together with their session and condition index\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_maps = []\nconditions_label = []\nsession_label = []\n\n# Instantiate the glm\nfrom nilearn.glm.first_level import FirstLevelModel\n\nglm = FirstLevelModel(\n    t_r=TR,\n    mask_img=haxby_dataset.mask,\n    high_pass=0.008,\n    smoothing_fwhm=4,\n    memory=\"nilearn_cache\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the glm on data from each session\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events[session].trial_type.unique()\nfrom nilearn.image import index_img\n\nfor session in unique_sessions:\n    # grab the fmri data for that particular session\n    fmri_session = index_img(func_filename, sessions == session)\n\n    # fit the glm\n    glm.fit(fmri_session, events=events[session])\n\n    # set up contrasts: one per condition\n    conditions = events[session].trial_type.unique()\n    for condition_ in conditions:\n        z_maps.append(glm.compute_contrast(condition_))\n        conditions_label.append(condition_)\n        session_label.append(session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating a report\nSince we have already computed the FirstLevelModel\nand have the contrast, we can quickly create a summary report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import mean_img\nfrom nilearn.reporting import make_glm_report\n\nmean_img_ = mean_img(func_filename)\nreport = make_glm_report(\n    glm,\n    contrasts=conditions,\n    bg_img=mean_img_,\n)\n\nreport  # This report can be viewed in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a jupyter notebook, the report will be automatically inserted, as above.\nWe have several other ways to access the report:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# report.save_as_html('report.html')\n# report.open_in_browser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the decoding pipeline\nTo define the decoding pipeline we use Decoder object, we choose :\n\n    * a prediction model, here a Support Vector Classifier, with a linear\n      kernel\n\n    * the mask to use, here a ventral temporal ROI in the visual cortex\n\n    * although it usually helps to decode better, z-maps time series don't\n      need to be rescaled to a 0 mean, variance of 1 so we use\n      standardize=False.\n\n    * we use univariate feature selection to reduce the dimension of the\n      problem keeping only 5% of voxels which are most informative.\n\n    * a cross-validation scheme, here we use LeaveOneGroupOut\n      cross-validation on the sessions which corresponds to a\n      leave-one-session-out\n\nWe fit directly this pipeline on the Niimgs outputs of the GLM, with\ncorresponding conditions labels and session labels\n(for the cross validation).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\n\nfrom nilearn.decoding import Decoder\n\ndecoder = Decoder(\n    estimator=\"svc\",\n    mask=haxby_dataset.mask,\n    standardize=False,\n    screening_percentile=5,\n    cv=LeaveOneGroupOut(),\n)\ndecoder.fit(z_maps, conditions_label, groups=session_label)\n\n# Return the corresponding mean prediction accuracy compared to chance\n\nclassification_accuracy = np.mean(list(decoder.cv_scores_.values()))\nchance_level = 1.0 / len(np.unique(conditions))\nprint(\n    f\"Classification accuracy: {classification_accuracy:.4f} / \"\n    f\"Chance level: {chance_level}\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
