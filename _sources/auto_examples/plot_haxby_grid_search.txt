

.. _example_plot_haxby_grid_search.py:


Setting a parameter by cross-validation
=======================================================

Here we set the number of features selected in an Anova-SVC approach to
maximize the cross-validation score.

After separating 2 sessions for validation, we vary that parameter and
measure the cross-validation score. We also measure the prediction score
on the left-out validation data. As we can see, the two scores vary by a
significant amount: this is due to sampling noise in cross validation,
and choosing the parameter k to maximize the cross-validation score,
might not maximize the score on left-out data.

Thus using data to maximize a cross-validation score computed on that
same data is likely to optimistic and lead to an overfit.

The proper appraoch is known as a "nested cross-validation". It consists
in doing cross-validation loops to set the model parameters inside the
cross-validation loop used to judge the prediction performance: the
parameters are set separately on each fold, never using the data used to
measure performance.

In scikit-learn, this can be done using the GridSearchCV object, that
will automatically select the best parameters of an estimator from a
grid of parameter values.

One difficulty here is that we are working with a composite estimator: a
pipeline of feature selection followed by SVC. Thus to give the name
of the parameter that we want to tune we need to give the name of the
step in the pipeline, followed by the name of the parameter, with '__' as
a separator.




.. image:: images/plot_haxby_grid_search_1.png
    :align: center


**Script output**:

.. rst-class:: max_height

 ::

    CV score 0.6
  score validation 0.5
  CV score 0.611111111111
  score validation 0.555555555556
  CV score 0.616666666667
  score validation 0.5
  CV score 0.661111111111
  score validation 0.722222222222
  CV score 0.605555555556
  score validation 0.722222222222
  CV score 0.677777777778
  score validation 0.777777777778
  CV score 0.644444444444
  score validation 0.611111111111
  CV score 0.622222222222
  score validation 0.555555555556
  CV score 0.661111111111
  score validation 0.555555555556
  CV score 0.611111111111
  score validation 0.5
  CV score 0.605555555556
  score validation 0.555555555556
  Fitting 3 folds for each of 11 candidates, totalling 33 fits
  Fitting 3 folds for each of 11 candidates, totalling 33 fits
  Fitting 3 folds for each of 11 candidates, totalling 33 fits



**Python source code:** :download:`plot_haxby_grid_search.py <plot_haxby_grid_search.py>`

.. literalinclude:: plot_haxby_grid_search.py
    :lines: 35-

**Total running time of the example:**  38.00 seconds 
( 0 minutes  38.00 seconds)
    